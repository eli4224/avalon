{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import easyrl.models.diag_gaussian_policy as DiagGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# set random seed\n",
    "seed = 0\n",
    "set_random_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "# Set default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getDefaultParams():\n",
    "    # here goes the default parameters for the agent\n",
    "    config = dict(\n",
    "        # env=env, the agent does not need to ahve access to the env because there is an engine\n",
    "        learning_rate=0.00025,\n",
    "        gamma=0.99,\n",
    "        memory_size=200000,\n",
    "        initial_epsilon=1.0,\n",
    "        min_epsilon=0.1,\n",
    "        max_epsilon_decay_steps=150000,\n",
    "        warmup_steps=500,\n",
    "        target_update_freq=2000,\n",
    "        batch_size=32,\n",
    "        device=None,\n",
    "        disable_target_net=False,\n",
    "        enable_double_q=False,\n",
    "        use_gae=False,\n",
    "        train_ac_iters=5,\n",
    "        target_kl=0.01,\n",
    "        clip_ratio=0.2,\n",
    "        entropy_coef=0.01,\n",
    "        use_critic=True\n",
    "    )\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "# Create categorical policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Stolen from easyrl Categorical policy policy with some modifications\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self,\n",
    "                 body_net,\n",
    "                 action_dim,\n",
    "                 in_features=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.body = body_net\n",
    "        if in_features is None:\n",
    "            for i in reversed(range(len(self.body.fcs))):\n",
    "                layer = self.body.fcs[i]\n",
    "                if hasattr(layer, 'out_features'):\n",
    "                    in_features = layer.out_features\n",
    "                    break\n",
    "\n",
    "        self.head = nn.Sequential(nn.Linear(in_features, action_dim), nn.Softmax())\n",
    "\n",
    "    def forward(self, x=None, body_x=None, **kwargs):\n",
    "        if x is None and body_x is None:\n",
    "            raise ValueError('One of [x, body_x] should be provided!')\n",
    "        if body_x is None:\n",
    "            body_x = self.body(x, **kwargs)\n",
    "        if isinstance(body_x, tuple):\n",
    "            pi = self.head(body_x[0])\n",
    "        else:\n",
    "            pi = self.head(body_x)\n",
    "        action_dist = Categorical(probs=pi)\n",
    "        return action_dist, body_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "# Create backbone NN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def init_params(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class WhoNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super().__init__()\n",
    "        #### A simple network that takes\n",
    "        #### as input the history, and outputs the \n",
    "        #### distribution parameters.\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out_layer = nn.Sequential(nn.Linear(32, action_dim), nn.Sigmoid())\n",
    "            \n",
    "    def forward(self, ob):\n",
    "        x = self.fcs(ob)\n",
    "        preds = self.out_layer(x)\n",
    "        return preds\n",
    "            \n",
    "class NNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, final_activation=None, use_critic=False):\n",
    "        self.final_activation = final_activation\n",
    "        self.use_critic = use_critic\n",
    "        super().__init__()\n",
    "        #### A simple network that takes\n",
    "        #### as input the history, and outputs the \n",
    "        #### distribution parameters.\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU())\n",
    "        self.out_layer = nn.Sequential(nn.Linear(64, action_dim))\n",
    "        \n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(input_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 32),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "        \n",
    "        # Set initial weights and biases\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, ob):\n",
    "        mid_logits = self.fcs(ob)\n",
    "        logits = self.out_layer(mid_logits)\n",
    "        if self.final_activation is not None:\n",
    "            logits = self.final_activation(logits, dim=-1)\n",
    "        if self.use_critic:\n",
    "            return logits, self.critic(ob)\n",
    "        else:\n",
    "            return logits, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "# Define game parameters and NN initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RED_TEAM_ID = 1\n",
    "BLUE_TEAM_ID = 0\n",
    "NUM_PLAYERS = 5\n",
    "RED_PLAYERS = 2\n",
    "BLUE_PLAYERS = 3\n",
    "HIST_SHAPE = 2 * 25 * (3 * NUM_PLAYERS + 5)\n",
    "SELF_SHAPE = torch.tensor([5])\n",
    "COMM_SHAPE = 32  # Change freely\n",
    "WHO_SHAPE = NUM_PLAYERS\n",
    "VOTE_SHAPE =torch.tensor([NUM_PLAYERS])\n",
    "BADNESS_SHAPE = WHO_SHAPE\n",
    "\n",
    "assert RED_PLAYERS + BLUE_PLAYERS == NUM_PLAYERS\n",
    "\n",
    "def get_who():\n",
    "    return WhoNetwork(SELF_SHAPE + HIST_SHAPE + BADNESS_SHAPE + NUM_PLAYERS*COMM_SHAPE, WHO_SHAPE)\n",
    "\n",
    "def get_comm(use_critic=False):\n",
    "    return DiagGaussian.DiagGaussianPolicy(NNetwork(SELF_SHAPE + HIST_SHAPE + WHO_SHAPE, torch.tensor([64]), use_critic=use_critic), COMM_SHAPE, in_features=torch.tensor([64]))\n",
    "\n",
    "def get_miss(mission_shapes = (10,10), use_critic=False):\n",
    "    model = NNetwork(SELF_SHAPE + WHO_SHAPE + HIST_SHAPE, 64, use_critic=use_critic)\n",
    "    return [CategoricalPolicy(model, mish, 64) for mish in mission_shapes]\n",
    "\n",
    "def get_vote(use_critic=False):\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + SELF_SHAPE + WHO_SHAPE, 128, use_critic=use_critic), 2, 128)\n",
    "\n",
    "def get_succ(use_critic=False):\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + SELF_SHAPE + WHO_SHAPE, 128,use_critic=use_critic), 2, 128)\n",
    "\n",
    "@torch.no_grad()\n",
    "def miss_players(miss):\n",
    "    # 0: 0,1\n",
    "    # 1: 0,2\n",
    "    # 2: 0,3\n",
    "    # 3: 0,4\n",
    "    # 4: 1,2\n",
    "    # 5: 1,3\n",
    "    # 6: 1,4\n",
    "    # 7: 2,3\n",
    "    # 8: 2,4\n",
    "    # 9: 3,4\n",
    "    miss_cat = torch.zeros(NUM_PLAYERS)\n",
    "    if miss<4:\n",
    "        miss_cat[0] = 1\n",
    "        miss_cat[miss+1] = 1\n",
    "    elif miss<7:\n",
    "        miss_cat[1] = 1\n",
    "        miss_cat[miss-2] = 1\n",
    "    elif miss<9:\n",
    "        miss_cat[2] = 1\n",
    "        miss_cat[miss-4] = 1\n",
    "    else:\n",
    "        miss_cat[3] = 1\n",
    "        miss_cat[miss-5] = 1\n",
    "    return miss_cat\n",
    "\n",
    "def model_lookup(train_model, red_agent, blue_agent):\n",
    "    table = {\"comm_red\": (red_agent.COMM, red_agent.COMM_opt),\n",
    "        \"miss_red_2\" : (red_agent.MISS, red_agent.MISS_opt2),\n",
    "        \"miss_red_3\" : (red_agent.MISS, red_agent.MISS_opt3),\n",
    "        \"vote_red\" : (red_agent.VOTE, red_agent.VOTE_opt), \n",
    "        \"succ_red\" : (red_agent.SUCC, red_agent.SUCC_opt),\n",
    "        \"who_blue\" : (blue_agent.WHO, blue_agent.WHO_opt),\n",
    "        \"comm_blue\" : (blue_agent.COMM, blue_agent.COMM_opt),\n",
    "        \"miss_blue_2\" : (blue_agent.MISS, blue_agent.MISS_opt2),\n",
    "        \"miss_blue_3\" : (blue_agent.MISS, blue_agent.MISS_opt3),\n",
    "        \"vote_blue\" : (blue_agent.VOTE, blue_agent.VOTE_opt)}\n",
    "    return table[train_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def comm():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def who():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def miss():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def vote():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def succ():\n",
    "        pass\n",
    "\n",
    "class RedAgent():\n",
    "\n",
    "    def __init__(self, lrs, use_critic):\n",
    "        self.lrs = lrs\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        self.COMM = get_comm(use_critic = use_critic)\n",
    "#         self.who = (lambda *args : every)\n",
    "        mission_models = get_miss(use_critic = use_critic)\n",
    "        self.MISS = (lambda args : [mission_models[i](args) for i in range(2)])\n",
    "        self.VOTE = get_vote(use_critic = use_critic)\n",
    "        self.SUCC = get_succ(use_critic = use_critic)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.COMM_opt = torch.optim.Adam(self.COMM.parameters(), lr=lrs['COMM'])\n",
    "        self.MISS_opt2 = torch.optim.Adam(mission_models[0].parameters(), lr=lrs['MISS2'])\n",
    "        self.MISS_opt3 = torch.optim.Adam(mission_models[1].parameters(), lr=lrs['MISS3'])\n",
    "        self.VOTE_opt = torch.optim.Adam(self.VOTE.parameters(), lr=lrs['VOTE'])\n",
    "        self.SUCC_opt = torch.optim.Adam(self.SUCC.parameters(), lr=lrs['SUCC'])\n",
    "\n",
    "class BlueAgent():\n",
    "    \n",
    "    def __init__(self, lrs, use_critic):\n",
    "        self.lrs = lrs\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        self.COMM = get_comm(use_critic=use_critic)\n",
    "        self.WHO = get_who()\n",
    "        mission_models = get_miss(use_critic=use_critic)\n",
    "        self.MISS = (lambda args : [mission_models[i](args) for i in range(2)])\n",
    "        self.VOTE = get_vote(use_critic=use_critic)\n",
    "#         self.succ = (lambda *args : torch.distributions.bernoulli.Bernoulli(1))\n",
    "        \n",
    "        # Optimizers\n",
    "        self.COMM_opt = torch.optim.Adam(self.COMM.parameters(), lr=lrs['COMM'])\n",
    "        self.WHO_opt = torch.optim.Adam(self.WHO.parameters(), lr=lrs['WHO'])\n",
    "        self.MISS_opt2 = torch.optim.Adam(mission_models[0].parameters(), lr=lrs['MISS2'])\n",
    "        self.MISS_opt3 = torch.optim.Adam(mission_models[1].parameters(), lr=lrs['MISS3'])\n",
    "        self.VOTE_opt = torch.optim.Adam(self.VOTE.parameters(), lr=lrs['VOTE'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline agents\n",
    "class ConstantDistribution():\n",
    "#   A \"distribution\" that always returns the same result when sampled\n",
    "\n",
    "    def __init__(self, values):\n",
    "#       Values must support element-wise comparison with ==\n",
    "        self.values = values\n",
    "    \n",
    "    def sample(self):\n",
    "        return self.values\n",
    "    \n",
    "    def log_prob(self, actions):\n",
    "        return self.values == actions\n",
    "\n",
    "class RedAgentBaseline():\n",
    "    # this red agent takes actions according to the following rules:\n",
    "    # COMM - outputs a random vector\n",
    "    # MISS - always randomly choose 1 red player and 1~2 blue players (according to how many players go on mission)\n",
    "    # VOTE - always vote yes if there are one or more red players on the mission, else no\n",
    "    # SUCC - always fail the mission\n",
    "\n",
    "    def __init__(self, use_critic, lr = 0.01):\n",
    "#         self.COMM = (lambda args: (ConstantDistribution(torch.rand((COMM_SHAPE))), None))\n",
    "        self.COMM = get_comm(use_critic = use_critic)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.COMM_opt = None\n",
    "        self.MISS_opt2 = None\n",
    "        self.MISS_opt3 = None\n",
    "        self.VOTE_opt = None\n",
    "        self.SUCC_opt = None\n",
    "\n",
    "    def SUCC(self, obs):\n",
    "        if len(obs.shape) == 1:\n",
    "            return Categorical(probs=torch.Tensor([1])), None\n",
    "        else:\n",
    "            return Categorical(probs=torch.ones((obs.shape[0],1))), None\n",
    "    \n",
    "    def VOTE(self, obs):\n",
    "        if len(obs.shape) == 1:\n",
    "            # obs = SELF_SHAPE + EVERY_SHAPE + (HIST_SHAPE) <- who goes on mission is here\n",
    "            # env.hist = torch.zeros((2, 25, 20))\n",
    "            every = obs[SELF_SHAPE:SELF_SHAPE+WHO_SHAPE]\n",
    "            hist = obs[SELF_SHAPE+WHO_SHAPE:]\n",
    "            hist = hist.reshape((2, 25, len(hist) // 50))\n",
    "            \n",
    "            # find the LAST VALID mission combination\n",
    "            miss = hist[0, hist[1, :, 5].sum().long()-1, 5:10]\n",
    "            probs = torch.Tensor([0, 1]) if (miss[every == RED_TEAM_ID].sum()).sum() > 0 else torch.Tensor([1, 0])\n",
    "\n",
    "            return Categorical(probs=probs), None\n",
    "        else:\n",
    "            # obs = EPISODES, SELF_SHAPE + EVERY_SHAPE + (HIST_SHAPE) <- who goes on mission is here\n",
    "            \n",
    "            probs_multi = []\n",
    "            for ob in obs:            \n",
    "                # env.hist = torch.zeros((2, 25, 20))\n",
    "                every = ob[SELF_SHAPE:SELF_SHAPE+WHO_SHAPE]\n",
    "                hist = ob[SELF_SHAPE+WHO_SHAPE:]\n",
    "                hist = hist.reshape((2, 25, len(hist) // 50))\n",
    "\n",
    "                # find the LAST VALID mission combination\n",
    "                miss = hist[0, hist[1, :, 5].sum().long()-1, 5:10]\n",
    "                probs = torch.Tensor([0, 1]) if (miss[every == RED_TEAM_ID].sum()).sum() > 0 else torch.Tensor([1, 0])\n",
    "\n",
    "                probs_multi.append(probs)\n",
    "            \n",
    "            return Categorical(torch.stack(probs_multi,axis=0)), None\n",
    "                \n",
    "#                 return Categorical(probs=probs), None\n",
    "            \n",
    "\n",
    "    def MISS(self, obs):\n",
    "        if len(obs.shape) == 1:\n",
    "            # obs = SELF_SHAPE + EVERY_SHAPE + HIST_SHAPE\n",
    "            every = obs[SELF_SHAPE:SELF_SHAPE+WHO_SHAPE]\n",
    "            \n",
    "            misses = []\n",
    "            for num_pl_miss in [2, 3]:\n",
    "                seq = np.arange(math.comb(NUM_PLAYERS, num_pl_miss))\n",
    "                np.random.shuffle(seq)\n",
    "                for comb in seq:\n",
    "                    miss = miss_players(comb) if num_pl_miss == 2 else 1 - miss_players(comb)\n",
    "                    if (every[miss.numpy().astype(bool)] == RED_TEAM_ID).sum() == 1:\n",
    "                        break\n",
    "                assert (every[miss.numpy().astype(bool)] == RED_TEAM_ID).sum() == 1\n",
    "                misses.append(torch.Tensor([comb]))\n",
    "\n",
    "            return (ConstantDistribution(misses[0]), None), \\\n",
    "                    (ConstantDistribution(misses[1]), None)\n",
    "        else:\n",
    "            # obs = EPISODES, SELF_SHAPE + EVERY_SHAPE + HIST_SHAPE\n",
    "            misses_multi = [[],[]]\n",
    "            \n",
    "            for ob in obs:\n",
    "                # obs = SELF_SHAPE + EVERY_SHAPE + HIST_SHAPE\n",
    "                every = ob[SELF_SHAPE:SELF_SHAPE+WHO_SHAPE]\n",
    "\n",
    "                for num_pl_miss in [2, 3]:\n",
    "                    seq = np.arange(math.comb(NUM_PLAYERS, num_pl_miss))\n",
    "                    np.random.shuffle(seq)\n",
    "                    for comb in seq:\n",
    "                        miss = miss_players(comb) if num_pl_miss == 2 else 1 - miss_players(comb)\n",
    "                        if (every[miss.numpy().astype(bool)] == RED_TEAM_ID).sum() == 1:\n",
    "                            break\n",
    "                    assert (every[miss.numpy().astype(bool)] == RED_TEAM_ID).sum() == 1\n",
    "                    \n",
    "                    misses_multi[num_pl_miss-2].append(torch.Tensor([comb]))\n",
    "            \n",
    "            misses_multi[0] = ConstantDistribution(torch.concat(misses_multi[0]))\n",
    "            misses_multi[1] = ConstantDistribution(torch.concat(misses_multi[1]))\n",
    "                    \n",
    "            return (misses_multi[0], None), (misses_multi[1], None)\n",
    "            \n",
    "\n",
    "def badness_factor(obs):\n",
    "    # Takes in input to who model.\n",
    "    self_v = obs[0:SELF_SHAPE]\n",
    "    _comm_v = obs[SELF_SHAPE:COMM_SHAPE*NUM_PLAYERS]\n",
    "    hist = obs[SELF_SHAPE+COMM_SHAPE*NUM_PLAYERS:]\n",
    "    hist = hist.reshape((2, 25, len(hist) // 50))\n",
    "\n",
    "    # Create badness score\n",
    "    previous_mission_players = hist[0, :, 5:10]\n",
    "#         previous_mission_successes = np.argmax(hist[1, :, 15:18], axis=-1)\n",
    "    # 0 -> no mission\n",
    "    # 1 -> success\n",
    "    # 2 -> failure\n",
    "    mission_no_result = hist[0, :, 15] == 1\n",
    "    mission_failures = hist[0, :, 17] == 1\n",
    "\n",
    "    # Number of missions each player was on minus the missions with no result\n",
    "    total_previous_mission_players = torch.sum(previous_mission_players, dim=0) - \\\n",
    "                    torch.sum(previous_mission_players * mission_no_result.reshape(25,1), dim=0)\n",
    "    # Number of mission each player was on that failed\n",
    "    previous_mission_failures_players = torch.sum(previous_mission_players * mission_failures.reshape(25,1), dim=0)\n",
    "\n",
    "    # Badness = Number_of_failed_missions_you_were_on / number_of_missions_you_were_on\n",
    "    badness_score = previous_mission_failures_players / total_previous_mission_players\n",
    "    # In the case where someone has never been on a mission, this will result in a nan\n",
    "    # We will fill nans with 0.5.\n",
    "    badness_score = torch.nan_to_num(badness_score, nan=0.5)\n",
    "\n",
    "    # We know that I am blue\n",
    "    badness_score[np.argmax(self_v)] = 0\n",
    "\n",
    "#     self.who_v = badness_score\n",
    "\n",
    "    return badness_score    \n",
    "\n",
    "class BlueAgentBaseline():\n",
    "    # this blue agent has a PRETRAINED WHO NETWORK and takes actions according to the following rules:\n",
    "    # COMM - outputs a random vector\n",
    "    # MISS - always choose itself and 1~2 other players PREDICTED TO BE BLUE THE MOST\n",
    "    # VOTE - according to the 3 MOST POSSIBLE BLUE PLAYER,\n",
    "    #        vote yes if the mission consist of only blue players, else no\n",
    "\n",
    "    def __init__(self):\n",
    "#         self.COMM = lambda args: (ConstantDistribution(torch.rand((COMM_SHAPE))), None)\n",
    "        self.COMM = get_comm(use_critic = use_critic)\n",
    "        \n",
    "        self.COMM_opt = None\n",
    "        self.WHO_opt = None\n",
    "        self.MISS_opt2 = None\n",
    "        self.MISS_opt3 = None\n",
    "        self.VOTE_opt = None\n",
    "        \n",
    "        self.who_v = torch.zeros(NUM_PLAYERS)\n",
    "    \n",
    "    def VOTE(self, obs):\n",
    "        # obs = SELF_SHAPE + WHO_SHAPE + (HIST_SHAPE) <- who goes on mission is here\n",
    "        # env.hist = torch.zeros((2, 25, 20))\n",
    "        hist = obs[SELF_SHAPE+WHO_SHAPE:]\n",
    "        hist = hist.reshape((2, 25, len(hist) // 50))\n",
    "        # find the LAST VALID mission combination\n",
    "        # -- hist[1, si, 5:10]\n",
    "        miss = hist[0, hist[1, :, 5].sum().long()-1, 5:10]\n",
    "        \n",
    "        reds = np.argsort(self.who_v)[-RED_PLAYERS:]\n",
    "        \n",
    "        yes = int(miss[reds].sum()==0)\n",
    "        \n",
    "        return Categorical(probs=torch.Tensor([1-yes, yes])), None\n",
    "\n",
    "    def MISS(self, obs):        \n",
    "        blues = np.argsort(self.who_v)[:BLUE_PLAYERS]\n",
    "        \n",
    "        miss2_v = torch.zeros(NUM_PLAYERS)\n",
    "        miss3_v = torch.zeros(NUM_PLAYERS)\n",
    "        miss2_v[blues[:2]] = 1\n",
    "        miss3_v[blues[:3]] = 1\n",
    "        miss2 = torch.Tensor([-1])\n",
    "        miss3 = torch.Tensor([-1])\n",
    "        \n",
    "        for i in range(math.comb(NUM_PLAYERS, 2)):            \n",
    "            if (miss_players(i) == miss2_v).sum() == NUM_PLAYERS:\n",
    "                miss2 = torch.Tensor([i])\n",
    "            if (1.0 - miss_players(i) == miss3_v).sum() == NUM_PLAYERS:\n",
    "                miss3 = torch.Tensor([i])\n",
    "                \n",
    "        assert miss2 != -1 and miss3 != -1\n",
    "        \n",
    "        return (ConstantDistribution(miss2), None), (ConstantDistribution(miss3), None)\n",
    "    \n",
    "    def WHO(self, obs):\n",
    "#         obs = obs.numpy()\n",
    "        return obs[len(obs)-5:]\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "class BlueAgentBaselineWHO():\n",
    "    # this blue agent has a PRETRAINED WHO NETWORK and takes actions according to the following rules:\n",
    "    # COMM - outputs a random vector\n",
    "    # MISS - always choose itself and 1~2 other players PREDICTED TO BE BLUE THE MOST\n",
    "    # VOTE - according to the 3 MOST POSSIBLE BLUE PLAYER,\n",
    "    #        vote yes if the mission consist of only blue players, else no\n",
    "\n",
    "    def __init__(self, lrs, use_critic):\n",
    "#         self.COMM = lambda args: (ConstantDistribution(torch.rand((COMM_SHAPE))), None)\n",
    "        self.COMM = get_comm(use_critic = use_critic)\n",
    "        \n",
    "        self.WHO = get_who()\n",
    "        self.COMM_opt = None\n",
    "        self.WHO_opt = torch.optim.Adam(self.WHO.parameters(), lr=lrs['WHO'])\n",
    "        self.MISS_opt2 = None\n",
    "        self.MISS_opt3 = None\n",
    "        self.VOTE_opt = None\n",
    "        \n",
    "    \n",
    "    def VOTE(self, obs):\n",
    "        # obs = SELF_SHAPE + WHO_SHAPE + (HIST_SHAPE) <- who goes on mission is here\n",
    "        # env.hist = torch.zeros((2, 25, 20))\n",
    "        hist = obs[SELF_SHAPE+WHO_SHAPE:]\n",
    "        hist = hist.reshape((2, 25, len(hist) // 50))\n",
    "        # find the LAST VALID mission combination\n",
    "        # -- hist[1, si, 5:10]\n",
    "        miss = hist[0, hist[1, :, 5].sum().long()-1, 5:10]\n",
    "        who_v = obs[SELF_SHAPE:SELF_SHAPE+WHO_SHAPE].detach().numpy()\n",
    "        reds = np.argsort(who_v)[-RED_PLAYERS:]\n",
    "        \n",
    "        yes = int(miss[reds].sum()==0)\n",
    "        return Categorical(probs=torch.Tensor([1-yes, yes])), None\n",
    "\n",
    "    def MISS(self, obs):        \n",
    "        who_v = obs[SELF_SHAPE:SELF_SHAPE+WHO_SHAPE].detach().numpy()\n",
    "        blues = np.argsort(who_v)[:BLUE_PLAYERS]\n",
    "        \n",
    "        miss2_v = torch.zeros(NUM_PLAYERS)\n",
    "        miss3_v = torch.zeros(NUM_PLAYERS)\n",
    "        miss2_v[blues[:2]] = 1\n",
    "        miss3_v[blues[:3]] = 1\n",
    "        miss2 = torch.Tensor([-1])\n",
    "        miss3 = torch.Tensor([-1])\n",
    "        \n",
    "        for i in range(math.comb(NUM_PLAYERS, 2)):            \n",
    "            if (miss_players(i) == miss2_v).sum() == NUM_PLAYERS:\n",
    "                miss2 = torch.Tensor([i])\n",
    "            if (1.0 - miss_players(i) == miss3_v).sum() == NUM_PLAYERS:\n",
    "                miss3 = torch.Tensor([i])\n",
    "                \n",
    "        assert miss2 != -1 and miss3 != -1\n",
    "        \n",
    "        return (ConstantDistribution(miss2), None), (ConstantDistribution(miss3), None)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "# Game environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AvalonEnv():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nm = [2, 3, 2, 3, 3, 0]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.mi = 0\n",
    "        self.ri = 0\n",
    "        self.si = 0\n",
    "        self.li = 0\n",
    "        self.hist = torch.zeros((2, 25, 20))\n",
    "        every = [0, 0, 0, 1, 1]\n",
    "        random.shuffle(every)\n",
    "        self.every = torch.Tensor(every.copy())        \n",
    "        self.who = torch.normal(0.5, 0.1, (NUM_PLAYERS, NUM_PLAYERS))\n",
    "        self.done = False\n",
    "        self.winning_team = None\n",
    "        \n",
    "        # the initial observation of \"hist\" is all zeros\n",
    "        # EXCEPT a one at the leader id (self.li) location of the zero-th step\n",
    "        self.hist[0, 0, self.li] = 1\n",
    "        self.hist[1, 0, :5] = 1\n",
    "        \n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self):         \n",
    "        return self.hist, self.every, self.who, self.li, self.si, self.mi, self.nm[self.mi], self.done\n",
    "    \n",
    "    def update_who(self, who_m):\n",
    "        self.who = who_m\n",
    "        \n",
    "    def update_miss(self, miss):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 5:10] = miss.detach()\n",
    "        self.hist[1, self.si, 5:10] = 1\n",
    "        return self.hist\n",
    "    \n",
    "    def update_vote(self, vote):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 10:15] = vote\n",
    "        self.hist[1, self.si, 10:15] = 1\n",
    "        \n",
    "        # check if there are more yeses than noes\n",
    "        if (vote >= 0.5).sum() > 2:\n",
    "            # set relevance of only the no mission flag\n",
    "            self.hist[1, self.si, 15] = 1\n",
    "        else:\n",
    "            # set the no mission flag\n",
    "            self.hist[0, self.si, 15] = 1\n",
    "\n",
    "            # set the current round\n",
    "            self.hist[0, self.si, 19] = self.ri\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] if self.si else 0\n",
    "            \n",
    "            # set relevance\n",
    "            self.hist[1, self.si, 15:] = 1\n",
    "            \n",
    "            if self.ri == 4:\n",
    "                # game is over, red team wins\n",
    "                self.winning_team = RED_TEAM_ID\n",
    "                self.done = True\n",
    "                \n",
    "            # update step id\n",
    "            self.si += 1\n",
    "                \n",
    "            # update leader\n",
    "            self.li = (self.li + 1) % 5\n",
    "            self.hist[0, self.si, self.li] = 1\n",
    "            self.hist[1, self.si, :5] = 1\n",
    "            \n",
    "            # update round id\n",
    "            self.ri = (self.ri+1) % 5\n",
    "            \n",
    "            \n",
    "        return self.hist\n",
    "        \n",
    "    def update_succ(self, succ):\n",
    "        # set the current round\n",
    "        self.hist[0, self.si, 19] = self.ri\n",
    "        \n",
    "        # set relevance\n",
    "        self.hist[1, self.si, 16:] = 1\n",
    "        \n",
    "        if (succ < 0.5).sum():\n",
    "            # set the mission failure flag\n",
    "            self.hist[0, self.si, 17] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] + 1 if self.si else 1\n",
    "        else:\n",
    "            # set the mission success flag\n",
    "            self.hist[0, self.si, 16] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] if self.si else 0\n",
    "        \n",
    "        # check if game is over\n",
    "        if self.hist[0, self.si, 18]  == 3:\n",
    "            # game is over, red team wins\n",
    "            self.winning_team = RED_TEAM_ID\n",
    "            self.done = True\n",
    "        elif self.mi == 2 + self.hist[0, self.si, 18]:\n",
    "            # game is over, blue team wins\n",
    "            self.winning_team = BLUE_TEAM_ID\n",
    "            self.done = True\n",
    "            \n",
    "        # update mission id\n",
    "        self.mi += 1\n",
    "\n",
    "        # update round id\n",
    "        self.ri = 0\n",
    "\n",
    "        # update step id\n",
    "        self.si += 1\n",
    "\n",
    "        # update leader\n",
    "        self.li = (self.li + 1) % 5\n",
    "        self.hist[0, self.si, self.li] = 1\n",
    "        self.hist[1, self.si, :5] = 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Game status checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def stat_check(hist):\n",
    "    if torch.is_tensor(hist):\n",
    "        H_flat = hist.flatten().cpu().detach().numpy()\n",
    "    else:\n",
    "        H_flat = hist.flatten().copy()\n",
    "    \n",
    "    H = H_flat.reshape((2,25,20))\n",
    "    num_steps = int(H[1,:,0].sum())\n",
    "    \n",
    "    try:\n",
    "        # assert that the indicators in H are non-decreasing\n",
    "        assert ((H_flat[500:-1] - H_flat[501:]) == -1).sum() == 0\n",
    "    \n",
    "        # assert for every 0 in the indicator, there cannot be non-zero in the values\n",
    "        assert ((H[1] == 0) * (H[0] > 0)).sum() == 0\n",
    "        \n",
    "        # assert there is exactly one 1 in the leaders\n",
    "        assert (H[0,:num_steps,:5].sum(axis=-1) != 1).sum() == 0\n",
    "        \n",
    "        # assert the number of players on mission is 2 or 3\n",
    "        if num_steps > 1:\n",
    "            miss_n = H[0,:num_steps-1,5:10].sum(axis=-1)\n",
    "            assert (miss_n == 2).sum() + (miss_n == 3).sum() == num_steps - 1\n",
    "        \n",
    "        # if majority vote yes then no_miss must be 0, else if majority vote no then fail/succ must be 0\n",
    "        vote_res = H[0,:num_steps,10:15].sum(axis=-1) > 2\n",
    "        no_miss = H[0,:num_steps,15]\n",
    "        succ = H[0,:num_steps,16]\n",
    "        fail = H[0,:num_steps,17]\n",
    "        assert (vote_res * no_miss).sum() == 0\n",
    "        assert (~vote_res * succ).sum() == 0\n",
    "        assert (~vote_res * fail).sum() == 0\n",
    "        \n",
    "        # assert number of fails don't exceed 3\n",
    "        assert (H[0,:num_steps,18] > 3).sum() == 0\n",
    "        \n",
    "        # assert the round ids don't exceed 4\n",
    "        assert (H[0,:num_steps,19] > 4).sum() == 0\n",
    "        \n",
    "    except:\n",
    "        print(H)\n",
    "        raise\n",
    "    \n",
    "#     H = H.reshape(2,25,20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Game engine and runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experiences(env: AvalonEnv, R, B, episodes, is_training, train_model):\n",
    "    self_m = torch.eye(5)\n",
    "    wins = []\n",
    "    hists = []\n",
    "    everys = []\n",
    "    everys_per_step = []\n",
    "\n",
    "    # trajectory buffer\n",
    "    if is_training:\n",
    "        obss = []\n",
    "        actionss = []\n",
    "        log_probss = []\n",
    "        stepidss = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        hist, every, who, li, si, mi, nm, done = env.reset()\n",
    "\n",
    "        everys.append(every)\n",
    "        if is_training:\n",
    "            obs = []\n",
    "            actions = []\n",
    "            log_probs = []\n",
    "            stepids = []\n",
    "\n",
    "        while not done:                        \n",
    "            ''' Communication '''\n",
    "            # Initialize communication matrix\n",
    "            comm_m = []\n",
    "            # Loop over every agent\n",
    "            for i in range(5):\n",
    "                # create input vector for network                            \n",
    "                comm_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "\n",
    "                # Call the COMM network of the current agent and return \n",
    "                # the communication vector (comm_v)\n",
    "                comm_dist, _ = B.COMM(comm_in) if every[i]==BLUE_TEAM_ID else R.COMM(comm_in)\n",
    "                comm_v = comm_dist.sample()\n",
    "\n",
    "                # Append it to the communication matrix (comm)\n",
    "                comm_m.append(comm_v)\n",
    "                # If we are currently training on the COMM network, save it to experience buffer\n",
    "                if is_training and ((train_model == 'comm_red' and every[i] == RED_TEAM_ID) or \\\n",
    "                        (train_model == 'comm_blue' and every[i] == BLUE_TEAM_ID)):\n",
    "                    obs.append(comm_in)\n",
    "                    actions.append(comm_v)\n",
    "                    log_probs.append(comm_dist.log_prob(comm_v))\n",
    "                    stepids.append(si)\n",
    "            # Make the torch.Tensor communication matrix\n",
    "            comm_m = torch.cat(comm_m)\n",
    "\n",
    "            ''' Predicting Who '''\n",
    "            # Loop over every agent\n",
    "            for i in range(NUM_PLAYERS):\n",
    "                # continue if the agent is on the red team\n",
    "                if every[i] == RED_TEAM_ID:\n",
    "                    continue\n",
    "                # create input vector for network\n",
    "                who_in = torch.cat((self_m[i], comm_m, hist.flatten()))\n",
    "                badness = badness_factor(who_in)\n",
    "                who_in = torch.cat((who_in, badness))\n",
    "\n",
    "                # Call the WHO network of the current agent (on the blue team)\n",
    "                # and return the who vector (who)\n",
    "                who_v = B.WHO(who_in)\n",
    "\n",
    "                # Update \"who_v\" into \"who\"\n",
    "                who[i] = who_v\n",
    "                # If we are currently training on the WHO network, save it to experience buffer\n",
    "                if is_training and train_model == 'who_blue':\n",
    "                    obs.append(who_in)\n",
    "            # update the who matrix to the environment\n",
    "            env.update_who(who)\n",
    "\n",
    "            ''' Deciding candidates to go on mission '''\n",
    "            # create input vector for \"miss\" network\n",
    "            miss_in = torch.cat((self_m[li], every if every[li] else who[li], hist.flatten()))\n",
    "\n",
    "            # Only call the leader\n",
    "            miss_dist = B.MISS(miss_in) if every[li]==BLUE_TEAM_ID else R.MISS(miss_in)\n",
    "            miss = miss_dist[nm - 2][0].sample()\n",
    "\n",
    "            # If we are currently training on the MISS network, save it to the experience buffer\n",
    "            if is_training and (('miss_red' in train_model and every[li] == RED_TEAM_ID) or \n",
    "                                ('miss_blue' in train_model and every[li] == BLUE_TEAM_ID)):\n",
    "                if str(nm) in train_model:\n",
    "                    obs.append(miss_in)\n",
    "                    actions.append(miss)\n",
    "                    log_probs.append(miss_dist)\n",
    "                    stepids.append(si)\n",
    "            # Update the \"miss\" vector to the environment\n",
    "            miss = miss_players(miss.long()) if nm==2 else 1-miss_players(miss.long())\n",
    "            hist = env.update_miss(miss)\n",
    "\n",
    "            ''' Voting YES/NO for the mission candidates '''\n",
    "            # Initialize vote vector\n",
    "            vote = []\n",
    "            # Loop over every agent\n",
    "            for i in range(5):\n",
    "                # create input vector for network\n",
    "                vote_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "\n",
    "                # Call the VOTE network of the current agent and return vote_pi                            \n",
    "                vote_dist, _ = B.VOTE(vote_in) if every[i]==BLUE_TEAM_ID else R.VOTE(vote_in)\n",
    "                vote_pi = vote_dist.sample()\n",
    "\n",
    "                # Append the voting results to \"vote\"\n",
    "                vote.append(vote_pi)\n",
    "                # If we are currently training on the VOTE network, save it to experience buffer\n",
    "                if is_training and ((train_model == 'vote_red' and every[i] == RED_TEAM_ID) or \n",
    "                                    (train_model == 'vote_blue' and every[i] == BLUE_TEAM_ID)):\n",
    "                    obs.append(vote_in)\n",
    "                    actions.append(vote_pi)\n",
    "                    log_probs.append(vote_dist)\n",
    "                    stepids.append(si)\n",
    "            # Make the torch.Tensor vote vector\n",
    "            vote = torch.Tensor(vote)\n",
    "\n",
    "            # Update the \"vote\" vector to the environment\n",
    "            hist = env.update_vote(vote)\n",
    "\n",
    "            ''' Success/Failure for the mission '''\n",
    "            # check if there are more yeses than noes\n",
    "            if (vote >= 0.5).sum() > 2:\n",
    "                # Initialize succ vector\n",
    "                succ = []\n",
    "                # Loop over every agent   \n",
    "                for i in range(5):\n",
    "                    if not miss[i]:\n",
    "                        continue\n",
    "                    # create input vector for network\n",
    "                    succ_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "\n",
    "                    # Call the SUCCESS network of the current agent and return succ_i\n",
    "                    if every[i] == BLUE_TEAM_ID:\n",
    "                        succ_i = torch.Tensor([1.0])\n",
    "                    else:\n",
    "                        succ_dist, _ = R.SUCC(succ_in)\n",
    "                        succ_i = succ_dist.sample()\n",
    "\n",
    "                    # Append the voting results to \"vote\"\n",
    "                    succ.append(succ_i)\n",
    "\n",
    "                    # If we are currently training on the SUCCESS network, \n",
    "                    # save it to experience buffer\n",
    "                    if is_training and train_model == 'succ_red' and every[i] == RED_TEAM_ID:\n",
    "                        obs.append(succ_in)\n",
    "                        actions.append(succ_i)\n",
    "                        log_probs.append(succ_dist)\n",
    "                        stepids.append(si)\n",
    "\n",
    "                # Make the torch.Tensor succ vector\n",
    "                succ = torch.Tensor(succ)\n",
    "\n",
    "                # Update the \"succ\" vector to the environment\n",
    "                env.update_succ(succ)\n",
    "\n",
    "            hist, every, who, li, si, mi, nm, done = env.get_observation()\n",
    "\n",
    "            # Check hist\n",
    "            stat_check(hist)\n",
    "\n",
    "        # append the result of each episode into the epoch buffer\n",
    "        wins.append(env.winning_team)\n",
    "        hists.append(hist)\n",
    "        if is_training:\n",
    "            obss.append(obs)\n",
    "            actionss.append(actions)\n",
    "            log_probss.append(log_probs)\n",
    "            stepidss.append(stepids)\n",
    "            last_step = si\n",
    "            everys_per_step += [every] * si * (1 if train_model != 'who_blue' else BLUE_PLAYERS)\n",
    "                \n",
    "    if is_training:\n",
    "        return hists, wins, everys, obss, actionss, log_probss, stepidss, everys_per_step, last_step\n",
    "    else:\n",
    "        return hists, wins, everys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experiences_multi(R, B, episodes, is_training, train_model):\n",
    "    # Here, we initialize multiple environments, have them output the observations,\n",
    "    # concatenate them, and forward through the models all at once!\n",
    "    \n",
    "    # initialize\n",
    "    self_m = torch.eye(5)\n",
    "    envs = []\n",
    "    for epi in range(episodes):\n",
    "        envs.append(AvalonEnv())\n",
    "    done = [None] * episodes\n",
    "    hist = [None] * episodes\n",
    "    every = [None] * episodes\n",
    "    who = [None] * episodes\n",
    "    li = [None] * episodes\n",
    "    si = [None] * episodes\n",
    "    mi = [None] * episodes\n",
    "    nm = [None] * episodes\n",
    "    if is_training:\n",
    "        obss = [[] for _ in range(episodes)]\n",
    "        actionss = [[] for _ in range(episodes)]\n",
    "        log_probss = [[] for _ in range(episodes)]\n",
    "        stepidss = [[] for _ in range(episodes)]\n",
    "    \n",
    "    # reset all environments\n",
    "    for i in range(episodes):\n",
    "        hist[i], every[i], who[i], li[i], si[i], mi[i], nm[i], done[i] = envs[i].reset()\n",
    "    \n",
    "    # loop until all environments are finished\n",
    "    while False in done:\n",
    "        ''' Communication '''\n",
    "        # communication input\n",
    "        comm_in_r = []\n",
    "        comm_in_b = []\n",
    "        # loop over every undone environment\n",
    "        for i in range(episodes):\n",
    "            if done[i]:\n",
    "                continue\n",
    "            # loop over every agent\n",
    "            for j in range(NUM_PLAYERS):\n",
    "                # construct and append input\n",
    "                if every[i][j] == RED_TEAM_ID:\n",
    "                    comm_in_r.append(torch.cat((self_m[j], every[i], hist[i].flatten())))\n",
    "                else:\n",
    "                    comm_in_b.append(torch.cat((self_m[j], who[i][j], hist[i].flatten())))\n",
    "        \n",
    "        # Now, comm_in_r should be the input for the RED agent in all episodes,\n",
    "        # and comm_in_b should be the input for the BLUE agent in all episodes\n",
    "        \n",
    "        assert len(comm_in_r) // RED_PLAYERS + np.sum(done) == episodes\n",
    "        assert len(comm_in_b) // BLUE_PLAYERS + np.sum(done) == episodes\n",
    "        \n",
    "        # the sequence of the players is not fixed, but for each episode, there are exactly 2\n",
    "        # vectors in comm_in_r and exactly 3 vectors in comm_in_b\n",
    "        # we can find the sequence of the players using 'every'\n",
    "                    \n",
    "        # Forward through the COMM network and sample all\n",
    "        comm_dist_r, _ = R.COMM(torch.stack(comm_in_r,axis=0))\n",
    "        comm_dist_b, _ = B.COMM(torch.stack(comm_in_b,axis=0))\n",
    "        comm_v_r = comm_dist_r.sample()\n",
    "        comm_v_b = comm_dist_b.sample()\n",
    "        \n",
    "        # save into replay buffer if training on COMM network\n",
    "        if is_training and 'comm_red' == train_model:\n",
    "            log_probs = comm_dist_r.log_prob(comm_v_r)\n",
    "            r_cnt = 0\n",
    "            for i in range(episodes):\n",
    "                if done[i]:\n",
    "                    continue\n",
    "                obss[i] += comm_in_r[r_cnt:r_cnt+RED_PLAYERS]\n",
    "                actionss[i] += comm_v_r[r_cnt:r_cnt+RED_PLAYERS]\n",
    "                log_probss[i] += log_probs[r_cnt:r_cnt+RED_PLAYERS]\n",
    "                stepidss[i] += [si[i]] * RED_PLAYERS\n",
    "                r_cnt += RED_PLAYERS\n",
    "        elif is_training and 'comm_blue' == train_model:\n",
    "            log_probs = comm_dist_b.log_prob(comm_v_b)\n",
    "            b_cnt = 0\n",
    "            for i in range(episodes):\n",
    "                if done[i]:\n",
    "                    continue\n",
    "                obss[i] += comm_in_b[b_cnt:b_cnt+BLUE_PLAYERS]\n",
    "                actionss[i] += comm_v_b[b_cnt:b_cnt+BLUE_PLAYERS]\n",
    "                log_probss[i] += log_probs[b_cnt:b_cnt+BLUE_PLAYERS]\n",
    "                stepidss[i] += [si[i]] * BLUE_PLAYERS\n",
    "                b_cnt += BLUE_PLAYERS\n",
    "        \n",
    "        ''' Predicting Who '''\n",
    "        # for counting the current id of communication\n",
    "        r_cnt = 0\n",
    "        b_cnt = 0\n",
    "        # who input\n",
    "        who_in = []\n",
    "        # loop over every undone environment\n",
    "        for i in range(episodes):\n",
    "            if done[i]:\n",
    "                continue\n",
    "            # construct the communication matrix for this episode\n",
    "            comm_m = []\n",
    "            for j in range(NUM_PLAYERS):\n",
    "                if every[i][j] == RED_TEAM_ID:\n",
    "                    comm_m.append(comm_v_r[r_cnt])\n",
    "                    r_cnt += 1\n",
    "                else:\n",
    "                    comm_m.append(comm_v_b[b_cnt])\n",
    "                    b_cnt += 1\n",
    "            # construct the communication matrix (actually, this becomes a vector because it is flattened)\n",
    "            comm_m = torch.cat(comm_m)\n",
    "                    \n",
    "            # construct who_in\n",
    "            for j in range(NUM_PLAYERS):\n",
    "                if every[i][j] == RED_TEAM_ID:\n",
    "                    continue                    \n",
    "                who_in_raw = torch.cat((self_m[j],comm_m,hist[i].flatten()))\n",
    "                who_in.append(torch.cat((who_in_raw, badness_factor(who_in_raw))))\n",
    "            \n",
    "        # forward through the WHO network\n",
    "        who_v = B.WHO(torch.stack(who_in,axis=0))\n",
    "        \n",
    "        # the who_v is a tensor with shape (N * 3, 5), with N being the episodes not done\n",
    "        assert who_v.shape[0] == (episodes - np.sum(done)) * BLUE_PLAYERS\n",
    "        assert who_v.shape[1] == NUM_PLAYERS\n",
    "        \n",
    "        # construct an expanded who with Nones for the red team and for already done episodes\n",
    "        # and update to each environment\n",
    "        who = []\n",
    "        who_cnt = 0\n",
    "        for i in range(episodes):\n",
    "            if done[i]:\n",
    "                who.append(None)\n",
    "                continue\n",
    "            who_single = []\n",
    "            for j in range(NUM_PLAYERS):\n",
    "                if every[i][j] == RED_TEAM_ID:\n",
    "                    who_single.append(torch.zeros(NUM_PLAYERS))\n",
    "                else:\n",
    "                    who_single.append(who_v[who_cnt])\n",
    "                    who_cnt += 1\n",
    "            who.append(torch.stack(who_single,axis=0))\n",
    "            envs[i].update_who(who[-1])\n",
    "        \n",
    "        assert len(who) == episodes\n",
    "        \n",
    "        # save to replay buffer if training on WHO network\n",
    "        if is_training and train_model == 'who_blue':\n",
    "            who_cnt = 0\n",
    "            for i in range(episodes):\n",
    "                if done[i]:\n",
    "                    continue\n",
    "                obss[i] += who_in[who_cnt:who_cnt+BLUE_PLAYERS]\n",
    "                who_cnt += BLUE_PLAYERS\n",
    "            \n",
    "        ''' Deciding candidates to go on mission '''\n",
    "        miss_in_r = []\n",
    "        miss_in_b = []\n",
    "        for i in range(episodes):\n",
    "            if done[i]:\n",
    "                continue\n",
    "            # construct the input for mission network\n",
    "            if every[i][li[i]]:\n",
    "                miss_in_r.append(torch.cat((self_m[li[i]], every[i], hist[i].flatten())))\n",
    "            else:\n",
    "                miss_in_b.append(torch.cat((self_m[li[i]], who[i][li[i]], hist[i].flatten())))\n",
    "        \n",
    "        # the length of miss_dist_r should be the same as comm_v_r\n",
    "        assert_n = 0\n",
    "        \n",
    "        # forward through the miss network\n",
    "        if len(miss_in_r) > 0:\n",
    "            miss_dist_r = R.MISS(torch.stack(miss_in_r,axis=0))\n",
    "            assert_n += len(miss_dist_r[0][0].sample())\n",
    "        if len(miss_in_b) > 0:\n",
    "            miss_dist_b = B.MISS(torch.stack(miss_in_b,axis=0))\n",
    "            assert_n += len(miss_dist_b[0][0].sample())\n",
    "        \n",
    "        assert assert_n + np.sum(done) == episodes\n",
    "        \n",
    "        # sample one mission combination for each episode\n",
    "        miss = []\n",
    "        miss_vs = []\n",
    "        miss_v_r = [miss_dist_r[0][0].sample(), miss_dist_r[1][0].sample()]\n",
    "        miss_v_b = [miss_dist_b[0][0].sample(), miss_dist_b[1][0].sample()]\n",
    "        r_cnt = 0\n",
    "        b_cnt = 0\n",
    "        for i in range(episodes):\n",
    "            if done[i]:\n",
    "                miss_vs.append(None)\n",
    "                continue\n",
    "            if every[i][li[i]] == RED_TEAM_ID:\n",
    "                miss.append(miss_v_r[nm[i]-2][r_cnt])\n",
    "                r_cnt += 1\n",
    "            else:\n",
    "                miss.append(miss_v_b[nm[i]-2][b_cnt])\n",
    "                b_cnt += 1\n",
    "        \n",
    "            # Update the miss vectors to each environment\n",
    "            miss_v = miss_players(miss[-1].long()) if nm[i]==2 else 1-miss_players(miss[-1].long())\n",
    "            hist[i] = envs[i].update_miss(miss_v)\n",
    "            miss_vs.append(miss_v)\n",
    "        \n",
    "        assert len(miss) + np.sum(done) == episodes\n",
    "\n",
    "        # If we are currently training on the MISS network, save it to the experience buffer\n",
    "        if is_training and 'miss_red' in train_model:\n",
    "            r_cnt = 0\n",
    "            for i in range(episodes):\n",
    "                if done[i] or every[i][li[i]] == BLUE_TEAM_ID or str(nm[i]) not in train_model:\n",
    "                    continue\n",
    "                obss[i] += [miss_in_r[r_cnt]]\n",
    "                actionss[i] += miss_v_r[nm[i]-2][r_cnt]\n",
    "                log_probss[i] += [((miss_dist_r[0][0].log_prob(miss_v_r[0])[r_cnt], None), \n",
    "                                   (miss_dist_r[1][0].log_prob(miss_v_r[1])[r_cnt], None))]\n",
    "                stepidss[i].append(si[i])\n",
    "                r_cnt += 1\n",
    "        elif is_training and 'miss_blue' in train_model:\n",
    "            b_cnt = 0\n",
    "            for i in range(episodes):\n",
    "                if done[i] or every[i][li[i]] == RED_TEAM_ID or str(nm[i]) not in train_model:\n",
    "                    continue\n",
    "                obss[i] += [miss_in_b[b_cnt]]\n",
    "                \n",
    "                actionss[i] += [miss_v_b[nm[i]-2][b_cnt]]\n",
    "                log_probss[i] += [((miss_dist_b[0][0].log_prob(miss_v_b[0])[b_cnt], None), \n",
    "                                   (miss_dist_b[1][0].log_prob(miss_v_b[1])[b_cnt], None))]\n",
    "                stepidss[i].append(si[i])\n",
    "                b_cnt += 1\n",
    "            \n",
    "            \n",
    "#                         miss_dist = B.MISS(miss_in) if every[li]==BLUE_TEAM_ID else R.MISS(miss_in)\n",
    "#             miss = miss_dist[nm - 2][0].sample()\n",
    "\n",
    "#             # If we are currently training on the MISS network, save it to the experience buffer\n",
    "#             if is_training and (('miss_red' in train_model and every[li] == RED_TEAM_ID) or \n",
    "#                                 ('miss_blue' in train_model and every[li] == BLUE_TEAM_ID)):\n",
    "#                 if str(nm) in train_model:\n",
    "#                     obs.append(miss_in)\n",
    "#                     actions.append(miss)\n",
    "#                     log_probs.append(miss_dist)\n",
    "#                     stepids.append(si)\n",
    "            \n",
    "            \n",
    "            \n",
    "        ''' Voting YES/NO for the mission candidates '''\n",
    "        vote_in_r = []\n",
    "        vote_in_b = []\n",
    "        for i in range(episodes):\n",
    "            if done[i]:\n",
    "                continue\n",
    "            for j in range(NUM_PLAYERS):\n",
    "                if every[i][j] == RED_TEAM_ID:\n",
    "                    vote_in_r.append(torch.cat((self_m[j], every[i], hist[i].flatten())))\n",
    "                else:\n",
    "                    vote_in_b.append(torch.cat((self_m[j], who[i][j], hist[i].flatten())))\n",
    "        \n",
    "        # the length of vote_in_r should be the same as miss_dist_r\n",
    "        assert len(vote_in_r) == len(comm_v_r)\n",
    "        assert len(vote_in_b) == len(comm_v_b)\n",
    "        \n",
    "        # Forward through the VOTE network and sample all\n",
    "        vote_dist_r, _ = R.VOTE(torch.stack(vote_in_r,axis=0))\n",
    "        vote_dist_b, _ = B.VOTE(torch.stack(vote_in_b,axis=0))\n",
    "        vote_v_r = vote_dist_r.sample()\n",
    "        vote_v_b = vote_dist_b.sample()\n",
    "        \n",
    "        # if currently training on the VOTE network, then add to replay buffer\n",
    "        if is_training and 'vote_red' == train_model:\n",
    "            r_cnt = 0\n",
    "            for i in range(episodes):\n",
    "                if done[i]:\n",
    "                    continue\n",
    "                obss[i] += vote_in_r[r_cnt:r_cnt+RED_PLAYERS]\n",
    "                actionss[i] += vote_v_r[r_cnt:r_cnt+RED_PLAYERS]\n",
    "                log_probss[i] += vote_dist_r.log_prob(vote_v_r)[r_cnt:r_cnt+RED_PLAYERS]\n",
    "                stepidss[i] += [si[i]] * RED_PLAYERS\n",
    "                r_cnt += RED_PLAYERS\n",
    "        elif is_training and 'vote_blue' == train_model:\n",
    "            b_cnt = 0\n",
    "            for i in range(episodes):\n",
    "                if done[i]:\n",
    "                    continue\n",
    "                obss[i] += vote_in_b[b_cnt:b_cnt+BLUE_PLAYERS]\n",
    "                actionss[i] += vote_v_b[b_cnt:b_cnt+BLUE_PLAYERS]\n",
    "                log_probss[i] += vote_dist_b.log_prob(vote_v_b)[b_cnt:b_cnt+BLUE_PLAYERS]\n",
    "                stepidss[i] += [si[i]] * BLUE_PLAYERS\n",
    "                b_cnt += BLUE_PLAYERS\n",
    "        \n",
    "        r_cnt = 0\n",
    "        b_cnt = 0\n",
    "        go_on_miss = []\n",
    "        succ_in = []\n",
    "        for i in range(episodes):\n",
    "            if done[i]:\n",
    "                go_on_miss.append(None)\n",
    "                continue\n",
    "            vote = []\n",
    "            for j in range(NUM_PLAYERS):\n",
    "                if every[i][j] == RED_TEAM_ID:\n",
    "                    vote.append(vote_v_r[r_cnt])\n",
    "                    r_cnt += 1\n",
    "                else:\n",
    "                    vote.append(vote_v_b[b_cnt])\n",
    "                    b_cnt += 1\n",
    "            # Update the \"vote\" vector to the environment\n",
    "            vote = torch.stack(vote,axis=0)\n",
    "            hist[i] = envs[i].update_vote(vote)\n",
    "        \n",
    "            # check if there are more yeses than noes, continue to next episode if majority votes no\n",
    "            go_on_miss.append((vote >= 0.5).sum() > 2)\n",
    "            if not go_on_miss[-1]:\n",
    "                continue\n",
    "            # create input vector for succ network (red agent)\n",
    "            for j in range(NUM_PLAYERS):\n",
    "                if every[i][j] == RED_TEAM_ID and miss_vs[i][j] == 1.0:\n",
    "                    succ_in.append(torch.cat((self_m[j], every[i], hist[i].flatten())))\n",
    "        \n",
    "        # forward the network\n",
    "        if len(succ_in) > 0:\n",
    "            succ_dist, _ = R.SUCC(torch.stack(succ_in, axis=0))\n",
    "            succ_v = succ_dist.sample()\n",
    "                    \n",
    "            # if currently training on the SUCC network, then add to replay buffer\n",
    "            if is_training and 'succ_red' == train_model:\n",
    "                r_cnt = 0\n",
    "                for i in range(episodes):\n",
    "                    if done[i] or not go_on_miss[i]:\n",
    "                        continue\n",
    "                    for j in range(NUM_PLAYERS):\n",
    "                        if every[i][j] == RED_TEAM_ID and miss_vs[i][j] == 1.0:\n",
    "                            obss[i] += [succ_in[r_cnt]]\n",
    "                            actionss[i] += [succ_v[r_cnt]]\n",
    "                            log_probss[i] += [succ_dist[r_cnt]]\n",
    "                            stepidss[i] += [si[i]]\n",
    "                            r_cnt += 1\n",
    "        \n",
    "        # update succ_v into each environment\n",
    "        succ_cnt = 0\n",
    "        for i in range(episodes):\n",
    "            if go_on_miss[i]:\n",
    "                # we need to find who is on the mission of episode i\n",
    "                succ_res = []\n",
    "                for j in range(NUM_PLAYERS):\n",
    "                    if miss_vs[i][j].item() == 1.0:\n",
    "                        if every[i][j] == RED_TEAM_ID:\n",
    "                            succ_res.append(succ_v[succ_cnt].unsqueeze(0))\n",
    "                            succ_cnt += 1\n",
    "                        else:\n",
    "                            succ_res.append(torch.Tensor([1.0]))                \n",
    "                # update succ_res\n",
    "                \n",
    "                envs[i].update_succ(torch.cat(succ_res))\n",
    "        \n",
    "        # get new observations\n",
    "        for i in range(episodes):\n",
    "            if not done[i]:\n",
    "                hist[i], every[i], who[i], li[i], si[i], mi[i], nm[i], done[i] = envs[i].get_observation()\n",
    "                # Check hist\n",
    "                stat_check(hist[i])\n",
    "                \n",
    "    # end of collecting experiences, calculate return values\n",
    "    win = [envs[i].winning_team for i in range(episodes)]\n",
    "    if is_training:\n",
    "        everys_per_step = []\n",
    "        for i in range(episodes):\n",
    "            everys_per_step += [every[i]] * envs[i].si * (1 if train_model != 'who_blue' else BLUE_PLAYERS)\n",
    "        last_step = envs[-1].si\n",
    "        \n",
    "        return hist, win, every, obss, actionss, log_probss, stepidss, everys_per_step, last_step\n",
    "    else:\n",
    "        return hist, win, every\n",
    "                \n",
    "#     if is_training:\n",
    "#         return hists, wins, everys, obss, actionss, log_probss, stepidss, everys_per_step, last_step\n",
    "#     else:\n",
    "#         return hists, wins, everys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = getDefaultParams()\n",
    "@dataclass\n",
    "class AvalonEngine:\n",
    "    env: AvalonEnv\n",
    "    train_episodes: int\n",
    "    max_epoch: int\n",
    "    blue: BlueAgent\n",
    "    red: RedAgent\n",
    "    trainable_models: list\n",
    "    gamma: float\n",
    "    gae_lambda: float\n",
    "    \n",
    "    def run(self, is_training=True, plot=True):\n",
    "        # Useful for constructing the self vector (self_v)\n",
    "        self_m = torch.eye(5)\n",
    "        hist_log = []\n",
    "        win_log = []\n",
    "        every_log = []\n",
    "        \n",
    "        for epoch in tqdm(range(self.max_epoch), desc='epoch', leave=False):\n",
    "            for train_model in self.trainable_models:\n",
    "                \n",
    "                exps = collect_experiences_multi(R=red, B=blue, episodes=self.train_episodes, \n",
    "                                          is_training=is_training, train_model=train_model)\n",
    "                \n",
    "                # collect experiences for 'self.train_episodes' episodes\n",
    "#                 exps = collect_experiences(self.env, red, blue, self.train_episodes, is_training, train_model)\n",
    "                if is_training:\n",
    "                    (hist_ep, win_ep, every_ep, obs_ep, actions_ep, log_probs_ep, stepid_ep, \\\n",
    "                         every_replay, last_step) = exps\n",
    "                else:\n",
    "                    (hist_ep, win_ep, every_ep) = exps\n",
    "                        \n",
    "                # extend to log\n",
    "                win_log.extend(win_ep)\n",
    "                every_log.extend(every_ep)\n",
    "                hist_log.extend(hist_ep)\n",
    "                \n",
    "                if is_training and train_model == 'who_blue':\n",
    "                    \n",
    "                    # concatenate all the observations and true identities in each episode\n",
    "                    # and transfer to torch.Tensor\n",
    "                    obs_all = []\n",
    "                    for epi in range(self.train_episodes):\n",
    "                        obs_all += obs_ep[epi]\n",
    "                    every_replay = torch.stack(every_replay, axis=0)\n",
    "                    obs_all_t = torch.from_numpy(np.stack(obs_all, axis=0))\n",
    "\n",
    "                    # retrieve the model and optimizer\n",
    "                    acmodel, optimizer = model_lookup(train_model, red, blue)\n",
    "\n",
    "                    # get predictions and loss\n",
    "                    preds = acmodel(obs_all_t)\n",
    "                    loss = torch.nn.functional.binary_cross_entropy(preds, every_replay)\n",
    "                    \n",
    "                    # update parameters\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # check loss at last step of the last episode\n",
    "                    randi = len(every_replay) - 1\n",
    "                    pred_l = acmodel(obs_all_t[randi:randi+1])\n",
    "                    real_l = every_replay[randi:randi+1]\n",
    "                    last_loss = torch.nn.functional.binary_cross_entropy(pred_l, real_l)\n",
    "                    print(f'epoch {epoch} - loss = {loss.item()}, last - {last_loss.item()}')\n",
    "\n",
    "                elif is_training:                    \n",
    "                    # concatenate all the observations, actions, log_probs, stepids, and discounted R\n",
    "                    obs_all, actions_all, log_probs_all, stepid_all, discounted_rewards = [], [], [], [], []\n",
    "                    for epi in range(self.train_episodes):\n",
    "                        if len(stepid_ep[epi]) == 0:\n",
    "                            continue\n",
    "                        obs_all += obs_ep[epi]\n",
    "                        actions_all += actions_ep[epi]\n",
    "                        log_probs_all += log_probs_ep[epi]\n",
    "                        stepid_all += stepid_ep[epi]\n",
    "                        discounted_rewards += compute_discounted_rewards(stepid_ep[epi], last_step, win_ep[epi], \n",
    "                                                                         train_model, self.gamma)\n",
    "                            \n",
    "                    value_function = (lambda x : 0)\n",
    "                    sb = preprocess(obs_all, actions_all, stepid_all, discounted_rewards, value_function)\n",
    "\n",
    "                    acmodel, optimizer = model_lookup(train_model, red, blue)\n",
    "                    update_parameters_ppo(optimizer, acmodel, sb, args, train_model)\n",
    "            \n",
    "            # plot result every 50 epochs\n",
    "            if plot and epoch % 50 == 0:\n",
    "                evaluate(hist_log, win_log, every_log)\n",
    "        \n",
    "        return hist_log, win_log, every_log\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "# PPO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# args = getDefaultParams()\n",
    "def compute_discounted_rewards(stepid_replay, last_step, winning_team, train_model, gamma):\n",
    "\n",
    "    # last_step = 10\n",
    "    # stepid_replay = [0, 0, 1, 3, 5, 5, 6, 6, 6, 8]\n",
    "\n",
    "    # Calculate the discounted rewards\n",
    "    win_team_color = 'red' if winning_team == RED_TEAM_ID else 'blue'\n",
    "    R = 1 if win_team_color in train_model else -1\n",
    "    discounted_rewards = [R * (gamma ** (last_step - 1 - stepid_replay[-1]))]\n",
    "    curr_step = stepid_replay[-1]\n",
    "\n",
    "    for si in range(len(stepid_replay) - 2, -1, -1):\n",
    "        discounted_rewards += [discounted_rewards[-1] * (gamma ** (curr_step - stepid_replay[si]))]\n",
    "        curr_step = stepid_replay[si]\n",
    "\n",
    "    # stepN = len(stepid_replay)\n",
    "    # win_team_color = 'red' if winning_team[-1] == RED_TEAM_ID else 'blue'\n",
    "    # discounted_rewards = [1 if win_team_color in train_model else -1]\n",
    "\n",
    "    # for i in range(stepN - 2, -1, -1):\n",
    "    #     if stepid_replay[i] == stepid_replay[i+1]:\n",
    "    #         # the current step of the i-th index is the same as the step of the i+1-th index\n",
    "    #         discounted_rewards.append(discounted_rewards[-1])\n",
    "    #     elif stepid_replay[i] != 0 and stepid_replay[i+1] == 0:\n",
    "    #         # the current step of the i-th index is the last step of a new episode\n",
    "    #         win_team_color = 'red' if winning_team[i] == RED_TEAM_ID else 'blue'\n",
    "    #         discounted_rewards.append(1 if win_team_color in train_model else -1)\n",
    "    #     else:\n",
    "    #         # the current step of the i-th index is the step of the i+1-th index minus 1\n",
    "    #         discounted_rewards.append(discounted_rewards[-1] * gamma)\n",
    "\n",
    "    discounted_rewards.reverse()\n",
    "    return discounted_rewards\n",
    "\n",
    "\n",
    "def preprocess(obs_replay, actions_replay, stepid_replay, discounted_rewards, value_function):\n",
    "    # Example:\n",
    "    # preprocess(*full_replay_buffer, range(10), (lambda x: 0))\n",
    "\n",
    "    assert len(obs_replay) == len(actions_replay)\n",
    "    assert len(obs_replay) == len(stepid_replay)\n",
    "    assert len(discounted_rewards) == len(obs_replay)\n",
    "    \n",
    "    discounted_rewards = torch.Tensor(discounted_rewards)\n",
    "    obs_replay = torch.stack(obs_replay)\n",
    "    actions_replay = torch.stack(actions_replay)\n",
    "    \n",
    "    sb = {\n",
    "        \"obs\" : obs_replay,\n",
    "        \"action\" : actions_replay,\n",
    "        \"advantage\" : discounted_rewards - value_function(obs_replay),\n",
    "        \"discounted_reward\" : discounted_rewards,\n",
    "    }\n",
    "    return sb\n",
    "\n",
    "\n",
    "\n",
    "def update_parameters_ppo(optimizer, acmodel, sb, args, train_model):\n",
    "    def _compute_policy_loss_ppo(logp, old_logp, entropy, advantages):\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        ### TODO: implement PPO policy loss computation (30 pts).  #######\n",
    "        logr = logp - old_logp\n",
    "        ratios = torch.exp(logr)\n",
    "        \n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1-args['clip_ratio'], 1+args['clip_ratio']) * advantages\n",
    "        \n",
    "        policy_loss = (-torch.min(surr1, surr2) -args['entropy_coef']*entropy).mean()\n",
    "        \n",
    "        # approx_kl = torch.sum(torch.exp(logp) * logr)\n",
    "        #approx_kl = torch.nn.functional.kl_div(logp, old_logp)\n",
    "\n",
    "        # approx_kl = ((logr.exp() - 1) - logr).sum()\n",
    "        approx_kl = ((logr ** 2) / 2).sum()\n",
    "        \n",
    "        ##################################################################\n",
    "        \n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(values, returns):\n",
    "        ### TODO: implement PPO value loss computation (10 pts) ##########\n",
    "        value_loss = F.mse_loss(values.squeeze(-1), returns).mean() #(values - returns).pow(2).mean()\n",
    "        ##################################################################\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    result = acmodel(sb['obs'])\n",
    "    if 'miss' in train_model:\n",
    "        if \"2\" in train_model:\n",
    "            result = result[0]\n",
    "        else:\n",
    "            result = result[1]\n",
    "\n",
    "    dist, (_, values) = result\n",
    "    \n",
    "    old_logp = dist.log_prob(sb['action']).detach()\n",
    "    logp = dist.log_prob(sb['action'])\n",
    "    dist_entropy = dist.entropy()\n",
    "    \n",
    "    advantage = sb['advantage_gae'] if args['use_gae'] else sb['advantage']\n",
    "    \n",
    "    policy_loss, _ = _compute_policy_loss_ppo(logp, old_logp, dist_entropy, advantage)\n",
    "\n",
    "    if not args[\"use_critic\"]:\n",
    "        values = torch.zeros_like(sb['discounted_reward'])\n",
    "\n",
    "    value_loss = _compute_value_loss(values, sb['discounted_reward'])\n",
    "\n",
    "    for i in range(args['train_ac_iters']):\n",
    "        result = acmodel(sb['obs'])\n",
    "        if 'miss' in train_model:\n",
    "            if \"2\" in train_model:\n",
    "                result = result[0]\n",
    "            else:\n",
    "                result = result[1]\n",
    "        dists, (_, values) = result\n",
    "            \n",
    "        logp = dists.log_prob(sb['action'])\n",
    "        dist_entropy = dists.entropy()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_pi, approx_kl = _compute_policy_loss_ppo(logp, old_logp, dist_entropy, advantage)\n",
    "\n",
    "        if not args[\"use_critic\"]:\n",
    "            values = torch.zeros_like(sb['discounted_reward'])\n",
    "            \n",
    "        loss_v = _compute_value_loss(values, sb['discounted_reward'])\n",
    "\n",
    "        loss = loss_v + loss_pi\n",
    "        if approx_kl > 1.5 * args['target_kl']:\n",
    "            break\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    \n",
    "    update_policy_loss = policy_loss.item()\n",
    "    update_value_loss = value_loss.item()\n",
    "\n",
    "    logs = {\n",
    "        \"policy_loss\": update_policy_loss,\n",
    "        \"value_loss\": update_value_loss,\n",
    "    }\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rounds_in_hist(hist):\n",
    "    # 5 is arbitrary\n",
    "    return hist[1, :, 5].sum().long()\n",
    "\n",
    "def count_votes(hist, every):\n",
    "    \n",
    "    player_red_indices = torch.nonzero(every).flatten()\n",
    "    player_blue_indices = torch.nonzero(~(every.bool())).flatten()\n",
    "    \n",
    "    rounds = rounds_in_hist(hist)\n",
    "    \n",
    "    red_votes = hist[0, :rounds, player_red_indices + 10]\n",
    "    blue_votes = hist[0, :rounds, player_blue_indices + 10]\n",
    "    \n",
    "    red_yay_percent = red_votes.sum() / (len(player_red_indices) * rounds)\n",
    "    blue_yay_percent = blue_votes.sum() / (len(player_blue_indices) * rounds)\n",
    "    return red_yay_percent, blue_yay_percent\n",
    "\n",
    "def count_rounds_per_mission(hist, every):\n",
    "    rounds = rounds_in_hist(hist)\n",
    "    \n",
    "    return torch.nonzero(hist[0, :rounds, 15]).sum() / rounds\n",
    "\n",
    "def count_round_stats(hist, every):\n",
    "    non_zero_rounds = np.count_nonzero(hist[0, :, 15])\n",
    "    mission_successes = np.count_nonzero(hist[0, :, 16])\n",
    "    mission_failures = np.count_nonzero(hist[0, :, 17])\n",
    "    \n",
    "    total_rounds = non_zero_rounds + mission_successes + mission_failures\n",
    "    non_zero_rounds /= total_rounds\n",
    "    mission_successes /= total_rounds\n",
    "    mission_failures /= total_rounds\n",
    "    return non_zero_rounds, mission_successes, mission_failures\n",
    "\n",
    "def game_success_state(hist, every):\n",
    "    non_zero_rounds = hist[0, :, 15]\n",
    "    mission_timout = False\n",
    "    ones = torch.ones(5)\n",
    "    for i in range(len(non_zero_rounds)-4):\n",
    "        if (non_zero_rounds[i:i+5] == ones).all():\n",
    "            mission_timout = True\n",
    "            break\n",
    "    mission_successes = np.count_nonzero(hist[0, :, 16])\n",
    "    mission_failures = np.count_nonzero(hist[0, :, 17])\n",
    "    \n",
    "    if mission_successes == 3 and mission_failures < 3 and not mission_timout:\n",
    "        return 0, 1, 0\n",
    "    elif mission_successes < 3 and mission_failures == 3 and not mission_timout:\n",
    "        return 0, 0, 1\n",
    "    elif mission_successes < 3 and mission_failures < 3 and mission_timout:\n",
    "        return 1, 0, 0\n",
    "    else:\n",
    "        print(mission_successes, mission_failures, mission_timout)\n",
    "        import IPython; IPython.embed()\n",
    "        raise Exception(\"Hist had multiple success conditions met.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluate(hist_log, winning_log, every_log):\n",
    "    def moving_average(x, w):\n",
    "        return np.convolve(x, np.ones(w), 'valid') / w\n",
    "    plt.plot(winning_log)\n",
    "    plt.title(\"Success Rate over time\")\n",
    "    plt.ylabel(\"Success Rate\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(moving_average(winning_log, 50))\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Success Rate\")\n",
    "    plt.title(\"Smoothed Success Rate over Time\")\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.show()\n",
    "    \n",
    "    non_rounds, suc_rounds, fail_rounds = zip(*[count_round_stats(hist, every) for (hist, every) in zip(hist_log, every_log)])\n",
    "    plt.plot(moving_average(non_rounds, 100), label=\"non-rounds\")\n",
    "    plt.plot(moving_average(suc_rounds, 100), label=\"suc-rounds\")\n",
    "    plt.plot(moving_average(fail_rounds, 100), label=\"fail-rounds\")\n",
    "    plt.legend([\"non-rounds\", \"suc-rounds\", \"fail-rounds\"], loc=0, frameon=True)\n",
    "    plt.ylabel(\"Percent of Rounds Won Each Game\")\n",
    "    plt.title(\"rounds occurences\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.show()\n",
    "    \n",
    "    fail_5_rounds, blue_win, red_wind = zip(*[game_success_state(hist, every) for (hist, every) in zip(hist_log, every_log)])\n",
    "    plt.plot(moving_average(blue_win, 100), label=\"blue_wind\")\n",
    "    plt.plot(moving_average(red_wind, 100), label=\"red_wind\")\n",
    "    plt.plot(moving_average(fail_5_rounds, 100), label=\"5-round-timeout\")\n",
    "    plt.legend([\"Blue-win\", \"Red-win\", \"5-round-timeout\"], loc=0, frameon=True)\n",
    "    plt.title(\"Win conditions per step\")\n",
    "    plt.ylabel(\"Percent of games won\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.show()\n",
    "    \n",
    "    red_yays, blue_yays = zip(*[count_votes(hist, every) for (hist, every) in zip(hist_log, every_log)])\n",
    "    plt.plot(moving_average(blue_yays, 100), label=\"blue\")\n",
    "    plt.plot(moving_average(red_yays, 100), label=\"red\")\n",
    "    plt.legend([\"Blue\", \"Red\"], loc=0, frameon=True)\n",
    "    plt.title(\"Percent of votes that are 'Yes' by agent\")\n",
    "    plt.ylabel(\"Percent of votes\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# eval_single(hist_log, winning_log, every_log)\n",
    "def eval_single(hist_log, win_log, every_log, episode):\n",
    "    assert len(hist_log) == len(win_log)\n",
    "    assert len(win_log) == len(every_log)\n",
    "    hist = hist_log[episode].detach().numpy().astype(int).reshape((2,25,20))\n",
    "    win = win_log[episode]\n",
    "    every = every_log[episode].detach().numpy()\n",
    "    \n",
    "    num_steps = hist[1,:,0].sum()\n",
    "    num_no_miss = hist[0,:num_steps,15].sum()\n",
    "    num_succ = hist[0,:num_steps,16].sum()\n",
    "    num_fail = hist[0,:num_steps,17].sum()\n",
    "    print(num_steps, num_no_miss, num_succ, num_fail)\n",
    "    \n",
    "    log = ''\n",
    "    for si in range(num_steps):\n",
    "        log += f'lead = {hist[0,si,:5]}\\n'\n",
    "        \n",
    "    print(log)\n",
    "    \n",
    "# eval_single(hist_log, winning_log, every_log, episode=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11a58f5b7d34510bae4c704dd91cf39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - loss = 0.6876224875450134, last - 0.6830006837844849\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcA0lEQVR4nO3de5wcZZ3v8c+XhOBqQgIkYEhCEiS6G9QXlyHgUZHbYpJdCd7JHhdhWbK+jtxWFzcsF1mU9Qgu7EGjGA8s9wQExFkNAiu3I3KbQLgkARwDmIRIJlyDXELgd/6oZ6BoumdqSKp7JvV9v179mqqnnq769VM99at6nu5qRQRmZlZdm7U6ADMzay0nAjOzinMiMDOrOCcCM7OKcyIwM6s4JwIzs4pzIrCNQtJhkn6zkdY1QVJIGrwx1mcDj6RzJZ3c6jiqwolggJH0MUm/lfScpKcl3SZpjybH0NIDdX9og41J0s2SXpb0gqQ1kq6WNLrgc/eRtKLsGDc2Sdem1/uCpFclrcvNnxsRX4mIb7U6zqpwIhhAJG0J/AL4PrA1MAb4V+CVVsbVTJtwGxwVEUOBnYChwPdaHM9GVXvSEBHTImJoes2XAmd0z0fEV1oTZXU5EQws7weIiHkR8VpEvBQR10fE/fBG98xtks6W9KykZZL+RypfLmm1pC93r0zScEkXSeqS9LikkyRtlpZtluYfT8+7SNLw9NRb099n0xncR3Lr/J6kZyQ9KmlazbbOk7RK0kpJ35Y0KC0blJ63RtIy4K82oA1OlXRJbrtvuXqRtLWk/5T0RIrzmlzdGZIWSXpe0u8lTS0Q+06SbklXJ2skXZ7KlfbD6rS+ByR9sLcdHBHPAtcAu+TiOlzSUklr0z79h1T+HuBaYPvc2fT2ad/NTq/hKUlXSNq60TYlHSmpM11dtUvaPpX/SNL3aur+XNLX0vT2kq5K759HJR2Tq3eqpCslXSLpeeCw3l57zXYukPTtNL2PpBWSvpHac5WkgyVNl/RIivtfcs/t0+s3J4KB5hHgNUkXSpomaas6dfYE7ge2AS4D5gN7kJ1pfgn4gaShqe73geHAjsAngEOBw9Oyw9Jj37R8KPCDtGzv9HdEOoO7Pbfth4GRwBnAeZKUll0ArE9x7AocCPx9WnYk8NepvA343Aa2QU8uBt4N7AxsC5wNIGkKcBFwPDAivcbHCsT+LeB6YCtgLFmbkursTZa4hgNfAJ7qLThJ2wCfATpzxavJ2mdLsv1ztqTdIuJPwDTgidzZ9BPA0cDBZPt0e+AZYE6D7e0HfCfFNxp4nOw9AzAP+GL3PkxtfSAwX9kJw38B95Fdle0PHCfpk7nVzwCuJGvPS3t77b14L/CutK1TgJ+QvZ93Bz4OnCxpYqpb+PVbEhF+DKAH8BdkB6YVZAendmC7tOww4He5uh8Cont5KnuK7GxzELAOmJxb9g/AzWn618D/yi37APAqMBiYkNY7OLf8MKAzN//uVOe9wHZkXTd/lls+E7gpTd8IfCW37MDa9fehDU4FLsnVfSNWsgPd68BWddb5Y+DsOuW9xX4RMBcYW/O8/ciS1l7AZr3s05uBF4HnUqyLgB16qH8NcGya3gdYUbN8KbB/bn50976rs67zyLpluueHproTAAF/APZOy44EbkzTewJ/qFnXCcB/5vbDrQXf0xcA325Ull7jS8CgND8stdOeufoLgYP7+vr9yB6+IhhgImJpRBwWEWOBD5Kd8fxHrsqTuemX0nNqy4aSnbVvTnYG2O1xsjMu0nprlw0mOzA28sdcnC+myaHA+LStVcq6rJ4lO/Bum9vW8pptNVSgDRoZBzwdEc80WPb7OuW9xf4NsgPmXZIWS/q7FOONZFdQc4DVkuYqG99o5JiIGA58mDevLgBIVz53pC6QZ4HpZPuvkfHAz3LxLgVeo/6+e8t+jogXyE4WxkR2FJ1PlvgA/oY3z+zHk3VJPZvbzr/UbCO/TzfUUxHxWpp+Kf2t977ujq3o6zfcNTSgRcRDZGdOvfY917GG7CxpfK5sB2Blmn6izrL1ZP98fb1l7XKys+qRETEiPbaMiJ3T8lVkB+L8tgqp0wZ/Irsa6fbemji2ljSiQYzv62vsEfHHiDgyIrYnu6L6oaSd0rJzImJ3YDJZF9HxBV7PA8C3gTlpnGEL4CqywePtImIEsIAs+UD9fbEcmJaLd0REvCsiVtap+5b9nMYdtuHN98E84HOSxpNdBVyV28ajNdsYFhHT8y+nt9dbkr68fsOJYECR9OeSvi5pbJofR3a2dkdf15XOrq4ATpc0LP2jfw3oHmidB/yjpIlpTOHfgMsjYj3QRdbFsmPBba0i60f/d0lbpsG890n6RKpyBXCMpLGpH3p2o3UVaINFwN6SdlA2uH1CTRzXkh2st5K0uaTu8Y7zgMMl7Z/iGyPpz3uLXdLnu2Mh64sO4HVJe0jaU9LmZMnp5dRmRVxIdvZ6EDAE2IKszdcrG4A/MFf3SWAbvTmQD3Au2X4dn2IcJWlGg23NS697l5R0/g24MyIeS212L9lJw/8FrotsMBvgLmCtpH+W9GfKBvw/qP7xMd6+vH7DiWCgWUt2VnanpD+RHfweBL7+Dtd3NNlBahnwG7LB5fPTsvPJBlZvBR4lO5AdDW90+5wO3JYuv/cqsK1DyQ5qS8gOmFeS9d1CNvB3HdnA4z3A1T2sp8c2iIgbgMvJBswXkn3UNO9vya6EHiIbhD0uPe8u0kAsWV/9Lbx5ptxT7HukWF4gG6s4NiKWkQ3s/iTVf5ysu+XMXtqIFMs64P8AJ0fEWuAYsmT5DFn3THuu7kNkB/NlaV9sn57bDlwvaW1qoz0bbOu/gZPJzvRXkV0VHVJT7TLggPS3+3mvkQ1g70L2/uhOFsNpvcKv3zLKugHNzKyqfEVgZlZxTgRmZhXnRGBmVnFOBGZmFTfgbvM7cuTImDBhQqvDMDMbUBYuXLgmIkbVWzbgEsGECRPo6OhodRhmZgOKpIbf2HfXkJlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxZWWCCSdL2m1pAcbLJekcyR1Srpf0m5lxWJmZo2VeUVwATC1h+XTgEnpMQv4UYmxmJlZA6X9HkFE3CppQg9VZgAXRUQAd0gaIWl0RKwqI55//a/FLHni+TJWbWbWFJO335Jvfmrnjb7eVo4RjAGW5+ZXpLK3kTRLUoekjq6urqYEZ2ZWFQPiF8oiYi4wF6CtrS3eyTrKyKJmZpuCVl4RrATG5ebHpjIzM2uiViaCduDQ9OmhvYDnyhofMDOzxkrrGpI0D9gHGClpBfBNYHOAiDgXWABMBzqBF4HDy4rFzMwaK/NTQzN7WR7AV8vavpmZFeNvFpuZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgZlZxTkRmJlVnBOBmVnFlZoIJE2V9LCkTkmz6yzfQdJNku6VdL+k6WXGY2Zmb1daIpA0CJgDTAMmAzMlTa6pdhJwRUTsChwC/LCseMzMrL4yrwimAJ0RsSwi1gHzgRk1dQLYMk0PB54oMR4zM6ujzEQwBliem1+RyvJOBb4kaQWwADi63ookzZLUIamjq6urjFjNzCqr1YPFM4ELImIsMB24WNLbYoqIuRHRFhFto0aNanqQZmabsjITwUpgXG5+bCrLOwK4AiAibgfeBYwsMSYzM6tRZiK4G5gkaaKkIWSDwe01df4A7A8g6S/IEoH7fszMmqi0RBAR64GjgOuApWSfDlos6TRJB6VqXweOlHQfMA84LCKirJjMzOztBpe58ohYQDYInC87JTe9BPhomTGYmVnPWj1YbGZmLeZEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXXayJQ5kuSTknzO0iaUn5oZmbWDEWuCH4IfASYmebXAnNKi8jMzJqqyG8W7xkRu0m6FyAinpE0pOS4zMysSYpcEbwqaRAQAJJGAa+XGpWZmTVNkURwDvAzYFtJpwO/Ab5TalRmZtY0vXYNRcSlkhYC+wMCDo6IpaVHZmZmTdFrIpB0cUT8LfBQnTIzMxvginQN7ZyfSeMFu5cTjpmZNVvDRCDpBElrgQ9Lel7S2jS/Gvh50yI0M7NSNUwEEfGdiBgGnBkRW0bEsPTYJiJOaGKMZmZWol67hiLiBElbSZoiae/uR5GVS5oq6WFJnZJmN6jzBUlLJC2WdFlfX4CZmW2YIoPFfw8cC4wFFgF7AbcD+/XyvEFk30D+S2AFcLek9ohYkqszCTgB+Gj6otq27/B1mJnZO1RksPhYYA/g8YjYF9gVeLbA86YAnRGxLCLWAfOBGTV1jgTmRMQzABGxumjgZma2cRRJBC9HxMsAkraIiIeADxR43hhgeW5+RSrLez/wfkm3SbpD0tR6K5I0S1KHpI6urq4CmzYzs6KK3GtohaQRwDXADZKeAR7fiNufBOxD1vV0q6QPRcSz+UoRMReYC9DW1hYbadtmZkaxbxZ/Ok2eKukmYDhwbYF1rwTG5ebHprK8FcCdEfEq8KikR8gSw90F1m9mZhtBn36YJiJuAV4GFhSofjcwSdLEdLfSQ4D2mjrXkF0NIGkkWVfRsr7EZGZmG6anL5TtJ+kRSS9IukTShyR1kN1w7ke9rTgi1gNHAdcBS4ErImKxpNMkHZSqXQc8JWkJcBNwfEQ8taEvyszMilNE/S739PsD/0j2UdFpwCXA7Ij4QfPCe7u2trbo6OhoZQhmZgOOpIUR0VZvWU9jBBERN6fpayStbHUSMDOzja+nRDBC0mfydfPzEXF1eWGZmVmz9JQIbgE+lZu/NTcfgBOBmdkmoGEiiIjDmxmImZm1Rp8+PmpmZpseJwIzs4pzIjAzq7heE4Gkz0salqZPknS1pN3KD83MzJqhyBXByRGxVtLHgAOA8yjwzWIzMxsYiiSC19LfvwLmRsQvgSHlhWRmZs1UJBGslPRj4IvAAklbFHyemZkNAEUO6F8guzncJ9PvBGwNHF9mUGZm1jxFfphmNPDLiHhF0j7Ah4GLygzKzMyap8gVwVXAa5J2IvuVsHHAZaVGZWZmTVMkEbyeflvgM8D3I+J4sqsEMzPbBBRJBK9KmgkcCvwilW1eXkhmZtZMRRLB4cBHgNMj4lFJE4GLyw3LzMyapciP1y+R9M/ADmn+UeC7ZQdmZmbNUeQWE58CFgG/SvO7SKr9EXozMxuginQNnQpMAZ4FiIhFwI6lRWRmZk1VaLA4Ip6rKXu9jGDMzKz5inyhbLGkvwEGSZoEHAP8ttywzMysWYpcERwN7Ay8QvZFsueA40qMyczMmqjIp4ZeBE5MDzMz28QU+dTQDZJG5Oa3knRdqVGZmVnTFOkaGpnuOgpARDwDbFtaRGZm1lSF7jUkaYfuGUnjgSgvJDMza6YiieBE4DeSLpZ0CXArcEKRlUuaKulhSZ2SZvdQ77OSQlJbsbDNzGxjKTJY/Kv0Y/V7paLjImJNb8+TNAiYA/wlsAK4W1J7RCypqTcMOBa4s6/Bm5nZhisyWPxpsi+V/SIifgGsl3RwgXVPATojYllErAPmAzPq1PsW2b2LXi4etpmZbSxFuoa+mf9mcRo4/maB540BlufmV6SyN6QrjXER8cueViRplqQOSR1dXV0FNm1mZkUVSQT16hT5RnKPJG0GnAV8vbe6ETE3Itoiom3UqFEbumkzM8spkgg6JJ0l6X3pcRawsMDzVpL9rGW3sams2zDgg8DNkh4jG4No94CxmVlzFb3FxDrg8vR4BfhqgefdDUySNFHSEOAQ4I3bV0fEcxExMiImRMQE4A7goIjo6ONrMDOzDVDkU0N/Ahp+9LOH562XdBRwHTAIOD8iFks6DeiICP+mgZlZP9BrIpB0E3W+QBYR+/X23IhYACyoKTulQd19elufmZltfEUGff8pN/0u4LPA+nLCMTOzZivSNVQ7MHybpLtKisfMzJqsSNfQ1rnZzYDdgeGlRWRmZk1VpGtoIdkYgci6hB4FjigzKDMza54iXUMTmxGImZm1RsPvEUjaQ9J7c/OHSvq5pHNquovMzGwA6+kLZT8m+yIZkvYG/jdwEdlvFs8tPzQzM2uGnrqGBkXE02n6i8DciLgKuErSotIjMzOzpujpimCQpO5EsT9wY27ZBt90zszM+oeeDujzgFskrQFeAv4fgKSdyLqHzMxsE9AwEUTE6ZJ+DYwGro+I7ttMbEZ2IzozM9sE9NjFExF31Cl7pLxwzMys2YrchtrMzDZhTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnGlJgJJUyU9LKlT0uw6y78maYmk+yX9WtL4MuMxM7O3Ky0RSBoEzAGmAZOBmZIm11S7F2iLiA8DVwJnlBWPmZnVV+YVwRSgMyKWRcQ6YD4wI18hIm6KiBfT7B3A2BLjMTOzOspMBGOA5bn5FamskSOAa+stkDRLUoekjq6uro0YopmZ9YvBYklfAtqAM+stj4i5EdEWEW2jRo1qbnBmZpu4Hn+8fgOtBMbl5semsreQdABwIvCJiHilxHjMzKyOMq8I7gYmSZooaQhwCNCeryBpV+DHwEERsbrEWMzMrIHSEkFErAeOAq4DlgJXRMRiSadJOihVOxMYCvxU0iJJ7Q1WZ2ZmJSmza4iIWAAsqCk7JTd9QJnbNzOz3vWLwWIzM2sdJwIzs4pzIjAzqzgnAjOzinMiMDOrOCcCM7OKcyIwM6s4JwIzs4pzIjAzqzgnAjOzinMiMDOrOCcCM7OKcyIwM6s4JwIzs4pzIjAzqzgnAjOzinMiMDOrOCcCM7OKcyIwM6s4JwIzs4pzIjAzqzgnAjOzinMiMDOrOCcCM7OKcyIwM6s4JwIzs4pzIjAzqzgnAjOziis1EUiaKulhSZ2SZtdZvoWky9PyOyVNKDMeMzN7u9ISgaRBwBxgGjAZmClpck21I4BnImIn4Gzgu2XFY2Zm9ZV5RTAF6IyIZRGxDpgPzKipMwO4ME1fCewvSSXGZGZmNcpMBGOA5bn5Famsbp2IWA88B2xTuyJJsyR1SOro6uoqKVwzs2oaEIPFETE3Itoiom3UqFGtDsfMbJNSZiJYCYzLzY9NZXXrSBoMDAeeKjEmMzOrUWYiuBuYJGmipCHAIUB7TZ124Mtp+nPAjRERJcZkZmY1Bpe14ohYL+ko4DpgEHB+RCyWdBrQERHtwHnAxZI6gafJkoWZmTVRaYkAICIWAAtqyk7JTb8MfL7MGMzMrGcDYrDYzMzK40RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcRpod32W1AU8/g6fPhJYsxHD2VgcV984rr7rr7E5rr7ZkLjGR0TdX/YacIlgQ0jqiIi2VsdRy3H1jePqu/4am+Pqm7LicteQmVnFORGYmVVc1RLB3FYH0IDj6hvH1Xf9NTbH1TelxFWpMQIzM3u7ql0RmJlZDScCM7OKq0wikDRV0sOSOiXNbmEc4yTdJGmJpMWSjk3lp0paKWlRekxvQWyPSXogbb8jlW0t6QZJv0t/t2pyTB/ItckiSc9LOq4V7SXpfEmrJT2YK6vbPsqck95v90varclxnSnpobTtn0kakconSHop127nNjmuhvtN0gmpvR6W9Mkmx3V5LqbHJC1K5c1sr0bHhvLfYxGxyT+AQcDvgR2BIcB9wOQWxTIa2C1NDwMeASYDpwL/1OJ2egwYWVN2BjA7Tc8Gvtvi/fhHYHwr2gvYG9gNeLC39gGmA9cCAvYC7mxyXAcCg9P0d3NxTcjXa0F71d1v6X/gPmALYGL6fx3UrLhqlv87cEoL2qvRsaH091hVrgimAJ0RsSwi1gHzgRmtCCQiVkXEPWl6LbAUGNOKWAqaAVyYpi8EDm5dKOwP/D4i3uk3yzdIRNwKPF1T3Kh9ZgAXReYOYISk0c2KKyKuj4j1afYOYGwZ2+5rXD2YAcyPiFci4lGgk+z/tqlxSRLwBWBeGdvuSQ/HhtLfY1VJBGOA5bn5FfSDg6+kCcCuwJ2p6Kh0iXd+s7tgkgCul7RQ0qxUtl1ErErTfwS2a0Fc3Q7hrf+grW4vaNw+/ek993dkZ47dJkq6V9Itkj7egnjq7bf+0l4fB56MiN/lypreXjXHhtLfY1VJBP2OpKHAVcBxEfE88CPgfcAuwCqyy9Nm+1hE7AZMA74qae/8wsiuR1vyeWNJQ4CDgJ+mov7QXm/RyvZpRNKJwHrg0lS0CtghInYFvgZcJmnLJobU7/ZbjZm89WSj6e1V59jwhrLeY1VJBCuBcbn5samsJSRtTrajL42IqwEi4smIeC0iXgd+QkmXxT2JiJXp72rgZymGJ7svN9Pf1c2OK5kG3BMRT6YYW95eSaP2afl7TtJhwF8D/zMdQEhdL0+l6YVkffHvb1ZMPey3/tBeg4HPAJd3lzW7veodG2jCe6wqieBuYJKkienM8hCgvRWBpD7I84ClEXFWrjzft/dp4MHa55Yc13skDeueJhtsfJCsnb6cqn0Z+Hkz48p5y5laq9srp1H7tAOHpk927AU8l7u8L52kqcA3gIMi4sVc+ShJg9L0jsAkYFkT42q039qBQyRtIWliiuuuZsWVHAA8FBErugua2V6Njg004z3WjNHw/vAgG2F/hCyjn9jCOD5Gdml3P7AoPaYDFwMPpPJ2YHST49qR7FMb9wGLu9sI2Ab4NfA74L+BrVvQZu8BngKG58qa3l5kiWgV8CpZf+wRjdqH7JMcc9L77QGgrclxdZL1H3e/x85NdT+b9u8i4B7gU02Oq+F+A05M7fUwMK2ZcaXyC4Cv1NRtZns1OjaU/h7zLSbMzCquKl1DZmbWgBOBmVnFORGYmVWcE4GZWcU5EZiZVZwTgVkBkk5Md4S8P92Fck9ld0F9d6tjM9tQ/vioWS8kfQQ4C9gnIl6RNJLsLra/Jfvs9pqWBmi2gXxFYNa70cCaiHgFIB34PwdsD9wk6SYASQdKul3SPZJ+mu4Z0/07D2co+62HuyTt1KoXYlaPE4FZ764Hxkl6RNIPJX0iIs4BngD2jYh901XCScABkd24r4PsJmXdnouIDwE/AP6jyfGb9WhwqwMw6+8i4gVJu5Pdonhf4HK9/Vfu9iL7EZHbslvGMAS4Pbd8Xu7v2eVGbNY3TgRmBUTEa8DNwM2SHuDNm4B1E3BDRMxstIoG02Yt564hs14o+93kSbmiXYDHgbVkPykI2a+AfbS7/z/dzTV/u+Iv5v7mrxTMWs5XBGa9Gwp8X9kPwK8nu7PnLLJbY/9K0hNpnOAwYJ6kLdLzTiK74y3AVpLuB15JzzPrN/zxUbOSSXoMf8zU+jF3DZmZVZyvCMzMKs5XBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhX3/wGSmwaGuGbGnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 - loss = 0.680044949054718, last - 0.6620932221412659\n",
      "epoch 2 - loss = 0.6807552576065063, last - 0.6692114472389221\n",
      "epoch 3 - loss = 0.6725737452507019, last - 0.6715962290763855\n",
      "epoch 4 - loss = 0.6657921075820923, last - 0.6514276266098022\n",
      "epoch 5 - loss = 0.6713419556617737, last - 0.6545042991638184\n",
      "epoch 6 - loss = 0.6599489450454712, last - 0.6690789461135864\n",
      "epoch 7 - loss = 0.6458629965782166, last - 0.6104288101196289\n",
      "epoch 8 - loss = 0.652531087398529, last - 0.5373992323875427\n",
      "epoch 9 - loss = 0.6535983681678772, last - 0.5848621129989624\n",
      "epoch 10 - loss = 0.6437795758247375, last - 0.7258771657943726\n",
      "epoch 11 - loss = 0.6351888179779053, last - 0.6260153651237488\n",
      "epoch 12 - loss = 0.6317532658576965, last - 0.5911102294921875\n",
      "epoch 13 - loss = 0.6169924139976501, last - 0.5572603344917297\n",
      "epoch 14 - loss = 0.6256584525108337, last - 0.5459590554237366\n",
      "epoch 15 - loss = 0.6064280867576599, last - 0.4623495042324066\n",
      "epoch 16 - loss = 0.589237630367279, last - 0.6132950186729431\n",
      "epoch 17 - loss = 0.6117523312568665, last - 0.7615705728530884\n",
      "epoch 18 - loss = 0.5760005712509155, last - 0.40183544158935547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [471]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m my_env \u001b[38;5;241m=\u001b[39m AvalonEnv()\n\u001b[1;32m     21\u001b[0m engine \u001b[38;5;241m=\u001b[39m AvalonEngine(env\u001b[38;5;241m=\u001b[39mmy_env, blue\u001b[38;5;241m=\u001b[39mblue, red\u001b[38;5;241m=\u001b[39mred, train_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, max_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     22\u001b[0m                       trainable_models\u001b[38;5;241m=\u001b[39mtrainable_models, gamma\u001b[38;5;241m=\u001b[39mgamma, gae_lambda\u001b[38;5;241m=\u001b[39mgae_lambda)\n\u001b[0;32m---> 23\u001b[0m hist_log, winning_log, every_log \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m evaluate(hist_log, winning_log, every_log)\n",
      "Input \u001b[0;32mIn [470]\u001b[0m, in \u001b[0;36mAvalonEngine.run\u001b[0;34m(self, is_training, plot)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epoch), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m train_model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_models:\n\u001b[0;32m---> 23\u001b[0m                 exps \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_experiences_multi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m                 \u001b[38;5;66;03m# collect experiences for 'self.train_episodes' episodes\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#                 exps = collect_experiences(self.env, red, blue, self.train_episodes, is_training, train_model)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m is_training:\n",
      "Input \u001b[0;32mIn [469]\u001b[0m, in \u001b[0;36mcollect_experiences_multi\u001b[0;34m(R, B, episodes, is_training, train_model)\u001b[0m\n\u001b[1;32m    135\u001b[0m             who_single\u001b[38;5;241m.\u001b[39mappend(who_v[who_cnt])\n\u001b[1;32m    136\u001b[0m             who_cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 137\u001b[0m     who\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwho_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    138\u001b[0m     envs[i]\u001b[38;5;241m.\u001b[39mupdate_who(who[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(who) \u001b[38;5;241m==\u001b[39m episodes\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trainable_models=['comm_red', 'comm_blue', 'who_blue', 'miss_red_2', 'miss_red_3', 'miss_blue_2', 'miss_blue_3', 'vote_red', 'vote_blue', 'succ_red']\n",
    "# trainable_models=['miss_red_2', 'miss_red_3', 'comm_red', 'vote_red', 'succ_red']\n",
    "trainable_models=['comm_blue', 'who_blue', 'miss_blue_2', 'miss_blue_3', 'vote_blue',]\n",
    "\n",
    "# trainable_models=['who_blue']\n",
    "# trainable_models=['comm_blue']\n",
    "\n",
    "set_random_seed(2)\n",
    "\n",
    "lrs_blue = {'COMM':0.00025, 'WHO':0.003, 'MISS2':0.00025, 'MISS3':0.00025, 'VOTE':0.00025}\n",
    "lrs_red = {'COMM':0.00025, 'SUCC':0.00025, 'MISS2':0.00025, 'MISS3':0.00025, 'VOTE':0.00025}\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "\n",
    "blue = BlueAgent(lrs=lrs_blue, use_critic=args[\"use_critic\"])\n",
    "# blue = BlueAgentBaselineWHO(lrs=lrs_blue, use_critic=args[\"use_critic\"]) # BlueAgentBaseline()\n",
    "# red = RedAgent(lrs=lrs_red, use_critic=args[\"use_critic\"])\n",
    "red = RedAgentBaseline(use_critic=args[\"use_critic\"])\n",
    "\n",
    "my_env = AvalonEnv()\n",
    "engine = AvalonEngine(env=my_env, blue=blue, red=red, train_episodes=50, max_epoch=1000,\n",
    "                      trainable_models=trainable_models, gamma=gamma, gae_lambda=gae_lambda)\n",
    "hist_log, winning_log, every_log = engine.run(is_training=True, plot=True)\n",
    "evaluate(hist_log, winning_log, every_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "evaluate(hist_log, winning_log, every_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot detailed events in single round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save the model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump red and blue agent to file\n",
    "import os, dill\n",
    "model_path = 'pretrained_models.pkl'\n",
    "if os.path.exists(model_path):\n",
    "    with open(model_path, 'rb') as file:\n",
    "        pretrained_models = dill.load(file)\n",
    "else:\n",
    "    pretrained_models = {}\n",
    "    \n",
    "# choose the model name and the corresponding model to save\n",
    "# pretrained_models[ /// name of the pretrained model /// ] = copy.deepcopy( /// the model class /// )\n",
    "\n",
    "with open('pretrained_models.pkl', 'wb') as file:\n",
    "    dill.dump(pretrained_models, file)\n",
    "\n",
    "pretrained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = pretrained_models['blue_against_red_1']\n",
    "red = pretrained_models['red_against_blue_bl_WHO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title\n",
    "# def compute_advantage_gae(values, discounted_rewards, stepid_replay, gae_lambda, gamma, train_model):\n",
    "#     rewards = [x if abs(x) == 1 else 0 for x in discounted_rewards]\n",
    "#     # print(rewards)\n",
    "#     # print(discounted_rewards)\n",
    "\n",
    "#     reps = 0\n",
    "#     if stepid_replay[0] != 0:\n",
    "#         print(train_model)\n",
    "#         print(stepid_replay)\n",
    "#     while stepid_replay[reps] == 0:\n",
    "#         reps += 1\n",
    "\n",
    "#     rewards_single = torch.Tensor(rewards[::reps])\n",
    "#     values_single = torch.Tensor(values[::reps])\n",
    "\n",
    "#     end_of_epi = np.arange(len(rewards_single))[rewards_single != 0].astype(int)\n",
    "    \n",
    "#     # print(end_of_epi)\n",
    "#     # print(values_single)\n",
    "\n",
    "#     last_end = -1\n",
    "#     total_advs = []\n",
    "#     for end in end_of_epi:\n",
    "#         # calculate advantage_gae from index last_end+1 ~ end\\\n",
    "#         theta = rewards_single[last_end+1:end+1] - values_single[last_end+1:end+1]\n",
    "#         theta[:-1] += gamma * values_single[last_end+2:end+1]\n",
    "#         gl = gamma * gae_lambda\n",
    "#         advantages = torch.tensor([\n",
    "#             (theta[i:] * (gl ** torch.arange(end - last_end -i))).sum() for i in range(end - last_end)\n",
    "#         ])\n",
    "\n",
    "#         # print(f'last_end: {last_end} ~ end: {end} => {len(advantages)}')\n",
    "#         # print(advantages)\n",
    "#         total_advs.append(advantages)\n",
    "#         last_end = end\n",
    "#     ############################################################################\n",
    "    \n",
    "#     total_advs = torch.concat(total_advs)\n",
    "#     torch.repeat_interleave(total_advs, 2)\n",
    "\n",
    "#     # print(total_advs)\n",
    "#     # print(torch.repeat_interleave(total_advs, 2))\n",
    "#     # assert 0\n",
    "\n",
    "#     return total_advs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
