{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import easyrl.models.diag_gaussian_policy as DiagGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stolen from easyrl Categorical policy policy with some modifications\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self,\n",
    "                 body_net,\n",
    "                 action_dim,\n",
    "                 in_features=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.body = body_net\n",
    "        if in_features is None:\n",
    "            for i in reversed(range(len(self.body.fcs))):\n",
    "                layer = self.body.fcs[i]\n",
    "                if hasattr(layer, 'out_features'):\n",
    "                    in_features = layer.out_features\n",
    "                    break\n",
    "\n",
    "        self.head = nn.Sequential(nn.Linear(in_features, action_dim), nn.Softmax())\n",
    "\n",
    "    def forward(self, x=None, body_x=None, **kwargs):\n",
    "        if x is None and body_x is None:\n",
    "            raise ValueError('One of [x, body_x] should be provided!')\n",
    "        if body_x is None:\n",
    "            body_x = self.body(x, **kwargs)\n",
    "        if isinstance(body_x, tuple):\n",
    "            pi = self.head(body_x[0])\n",
    "        else:\n",
    "            pi = self.head(body_x)\n",
    "        action_dist = Categorical(probs=pi)\n",
    "        return action_dist, body_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# set random seed\n",
    "seed = 0\n",
    "set_random_seed(seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, final_activation=None):\n",
    "        self.final_activation = final_activation\n",
    "        super().__init__()\n",
    "        #### A simple network that takes\n",
    "        #### as input the history, and outputs the \n",
    "        #### distribution parameters.\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU())\n",
    "        self.out_layer = nn.Sequential(nn.Linear(64, action_dim))\n",
    "\n",
    "    def forward(self, ob):\n",
    "        mid_logits = self.fcs(ob)\n",
    "        logits = self.out_layer(mid_logits)\n",
    "        if self.final_activation is not None:\n",
    "            logits = self.final_activation(logits, dim=-1)\n",
    "        return logits\n",
    "\n",
    "# class MishNet(NNetwork):\n",
    "#     def __init__(*args, mission_shapes, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.final_layers = [nn.Sequential(nn.Linear(64, s)) for s in mission_shapes]\n",
    "        \n",
    "#     def forward(self, ob):\n",
    "#         mid_logits = self.fcs(ob)\n",
    "#         logits = [fl(mid_logits) for fl in self.final_layers]\n",
    "#         if self.final_activation is not None:\n",
    "#             logits = [self.final_activation(logit) for logit in logits]\n",
    "#         return logits\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED_TEAM_ID = 1\n",
    "BLUE_TEAM_ID = 0\n",
    "NUM_PLAYERS = 5\n",
    "RED_PLAYERS = 2\n",
    "BLUE_PLAYERS = 3\n",
    "HIST_SHAPE = 2 * 25 * (3 * NUM_PLAYERS + 5)\n",
    "SELF_SHAPE = torch.tensor([5])\n",
    "COMM_SHAPE = 32  # Change freely\n",
    "WHO_SHAPE = torch.tensor([NUM_PLAYERS])\n",
    "VOTE_SHAPE =torch.tensor([NUM_PLAYERS])\n",
    "\n",
    "assert RED_PLAYERS + BLUE_PLAYERS == NUM_PLAYERS\n",
    "\n",
    "def get_who():\n",
    "    return NNetwork(SELF_SHAPE + HIST_SHAPE + NUM_PLAYERS*COMM_SHAPE, WHO_SHAPE, nn.functional.softmax)\n",
    "\n",
    "def get_comm():\n",
    "    return DiagGaussian.DiagGaussianPolicy(NNetwork(SELF_SHAPE + HIST_SHAPE + WHO_SHAPE, torch.tensor([64])), COMM_SHAPE, in_features=torch.tensor([64]))\n",
    "\n",
    "def get_miss(mission_shapes = (10,10)):\n",
    "    model = NNetwork(SELF_SHAPE + WHO_SHAPE + HIST_SHAPE, 64)\n",
    "    return [CategoricalPolicy(model, mish, 64) for mish in mission_shapes]\n",
    "\n",
    "def get_vote():\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + SELF_SHAPE + WHO_SHAPE, 128), 2, 128)\n",
    "\n",
    "def get_succ():\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + SELF_SHAPE + WHO_SHAPE, 128), 2, 128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def comm():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def who():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def miss():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def vote():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def succ():\n",
    "        pass\n",
    "\n",
    "class RedAgent():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.COMM = get_comm()\n",
    "#         self.who = (lambda *args : every)\n",
    "        mission_models = get_miss()\n",
    "        self.MISS = (lambda args : [mission_models[i](args) for i in range(2)])\n",
    "        self.VOTE = get_vote()\n",
    "        self.SUCC = get_succ()\n",
    "\n",
    "class BlueAgent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.COMM = get_comm()\n",
    "        self.WHO = get_who()\n",
    "        mission_models = get_miss()\n",
    "        self.MISS = (lambda args : [mission_models[i](args) for i in range(2)])\n",
    "        self.VOTE = get_vote()\n",
    "#         self.succ = (lambda *args : torch.distributions.bernoulli.Bernoulli(1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AvalonEnv():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nm = [2, 3, 2, 3, 3]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.mi = 0\n",
    "        self.ri = 0\n",
    "        self.si = 0\n",
    "        self.li = 0\n",
    "        self.hist = torch.zeros((2, 25, 20))\n",
    "        every = [0, 0, 0, 1, 1]\n",
    "        random.shuffle(every)\n",
    "        self.every = torch.Tensor(every.copy())        \n",
    "        self.who = torch.normal(0.5, 0.1, (NUM_PLAYERS, NUM_PLAYERS))\n",
    "        self.done = False\n",
    "        self.winning_team = None\n",
    "        \n",
    "        # the initial observation of \"hist\" is all zeros\n",
    "        # EXCEPT a one at the leader id (self.li) location of the zero-th step\n",
    "        self.hist[0, 0, self.li] = 1\n",
    "        self.hist[1, 0, :5] = 1\n",
    "        \n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self):         \n",
    "        return self.hist, self.every, self.who, self.li, self.si, self.mi, self.nm[self.mi], self.done\n",
    "    \n",
    "    def update_who(self, who_m):\n",
    "        self.who = who_m\n",
    "        \n",
    "    def update_miss(self, miss):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 5:10] = miss.detach()\n",
    "        self.hist[1, self.si, 5:10] = 1\n",
    "        return self.hist\n",
    "    \n",
    "    def update_vote(self, vote):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 10:15] = vote\n",
    "        self.hist[1, self.si, 10:15] = 1\n",
    "        \n",
    "        # check if there are more yeses than noes\n",
    "        if (vote >= 0.5).sum() > 2:\n",
    "            # set relevance of only the no mission flag\n",
    "            self.hist[1, self.si, 15] = 1\n",
    "        else:\n",
    "            # set the no mission flag\n",
    "            self.hist[0, self.si, 15] = 1\n",
    "\n",
    "            # set the current round\n",
    "            self.hist[0, self.si, 19] = self.ri\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] if self.si else 0\n",
    "            \n",
    "            # set relevance\n",
    "            self.hist[1, self.si, 15:] = 1\n",
    "            \n",
    "            if self.ri == 4:\n",
    "                # game is over, red team wins\n",
    "                self.winning_team = RED_TEAM_ID\n",
    "                self.done = True\n",
    "                \n",
    "            # update leader\n",
    "            self.li = (self.li + 1) % 5\n",
    "            self.hist[0, self.si, self.li] = 1\n",
    "            self.hist[1, self.si, :5] = 1\n",
    "            \n",
    "            # update round id\n",
    "            self.ri = (self.ri+1) % 5\n",
    "            \n",
    "            # update step id\n",
    "            self.si += 1\n",
    "        return self.hist\n",
    "        \n",
    "    def update_succ(self, succ):\n",
    "        # set the current round\n",
    "        self.hist[0, self.si, 19] = self.ri\n",
    "        \n",
    "        # set relevance\n",
    "        self.hist[1, self.si, 16:] = 1\n",
    "        \n",
    "        if (succ < 0.5).sum():\n",
    "            # set the mission failure flag\n",
    "            self.hist[0, self.si, 17] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] + 1 if self.si else 1\n",
    "        else:\n",
    "            # set the mission success flag\n",
    "            self.hist[0, self.si, 16] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] if self.si else 0\n",
    "        \n",
    "        # check if game is over\n",
    "        if self.hist[0, self.si, 18]  == 3:\n",
    "            # game is over, red team wins\n",
    "            self.winning_team = RED_TEAM_ID\n",
    "            self.done = True\n",
    "        elif self.mi == 2 + self.hist[0, self.si, 18]:\n",
    "            # game is over, blue team wins\n",
    "            self.winning_team = BLUE_TEAM_ID\n",
    "            self.done = True\n",
    "            \n",
    "        # update mission id\n",
    "        self.mi += 1\n",
    "\n",
    "        # update round id\n",
    "        self.ri = 0\n",
    "\n",
    "        # update step id\n",
    "        self.si += 1\n",
    "\n",
    "        # update leader\n",
    "        self.li = (self.li + 1) % 5\n",
    "        self.hist[0, self.si, self.li] = 1\n",
    "        self.hist[1, self.si, :5] = 1\n",
    "            \n",
    "        return self.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> tensor([1., 1., 0., 0., 0.])\n",
      "1 -> tensor([1., 0., 1., 0., 0.])\n",
      "2 -> tensor([1., 0., 0., 1., 0.])\n",
      "3 -> tensor([1., 0., 0., 0., 1.])\n",
      "4 -> tensor([0., 1., 1., 0., 0.])\n",
      "5 -> tensor([0., 1., 0., 1., 0.])\n",
      "6 -> tensor([0., 1., 0., 0., 1.])\n",
      "7 -> tensor([0., 0., 1., 1., 0.])\n",
      "8 -> tensor([0., 0., 1., 0., 1.])\n",
      "9 -> tensor([0., 0., 0., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def miss_players(miss):\n",
    "    # 0: 0,1\n",
    "    # 1: 0,2\n",
    "    # 2: 0,3\n",
    "    # 3: 0,4\n",
    "    # 4: 1,2\n",
    "    # 5: 1,3\n",
    "    # 6: 1,4\n",
    "    # 7: 2,3\n",
    "    # 8: 2,4\n",
    "    # 9: 3,4\n",
    "    miss_cat = torch.zeros(NUM_PLAYERS)\n",
    "    if miss<4:\n",
    "        miss_cat[0] = 1\n",
    "        miss_cat[miss+1] = 1\n",
    "    elif miss<7:\n",
    "        miss_cat[1] = 1\n",
    "        miss_cat[miss-2] = 1\n",
    "    elif miss<9:\n",
    "        miss_cat[2] = 1\n",
    "        miss_cat[miss-4] = 1\n",
    "    else:\n",
    "        miss_cat[3] = 1\n",
    "        miss_cat[miss-5] = 1\n",
    "    return miss_cat\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'{i} -> {miss_players(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AvalonEngine:\n",
    "    env: AvalonEnv\n",
    "    train_episodes: int\n",
    "    max_epoch: int\n",
    "    blue: BlueAgent\n",
    "    red: RedAgent\n",
    "    trainable_models: list\n",
    "    \n",
    "    def run(self):\n",
    "        # Useful for constructing the self vector (self_v)\n",
    "        self_m = torch.eye(5)\n",
    "        \n",
    "        for epoch in tqdm(range(self.max_epoch), desc='epoch'):\n",
    "            for train_model in tqdm(self.trainable_models, desc='train_model'):\n",
    "                # trajectory buffer\n",
    "                obs_replay = []\n",
    "                actions_replay = []\n",
    "                log_probs = []\n",
    "                stepid_replay = []\n",
    "                winning_team = []\n",
    "\n",
    "                for episode in range(self.train_episodes):\n",
    "                    hist, every, who, li, si, mi, nm, done = self.env.reset()\n",
    "                    while not done:\n",
    "                        # Flow: communication -> predict who -> decide miss -> voting -> succ/fail -> next round\n",
    "                        #                                                             -> next round\n",
    "\n",
    "#                         print(f'Mission {mi} Round {self.env.ri} Step {si}')\n",
    "                        \n",
    "                        ''' Communication '''\n",
    "                        # Initialize communication matrix\n",
    "                        comm_m = []\n",
    "                        # Loop over every agent\n",
    "                        for i in range(5):\n",
    "                            # create input vector for network                            \n",
    "                            comm_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "                            \n",
    "#                             if agents == []:\n",
    "#                                 comm_v = torch.rand(COMM_SHAPE)\n",
    "#                             else:\n",
    "                            \n",
    "                            # Call the COMM network of the current agent and return the communication vector (comm_v)\n",
    "                            comm_dist, _ = self.blue.COMM(comm_in) if every[i]==BLUE_TEAM_ID else self.red.COMM(comm_in)\n",
    "                            comm_v = comm_dist.sample()\n",
    "                \n",
    "                            # Append it to the communication matrix (comm)\n",
    "                            comm_m.append(comm_v)\n",
    "                            # If we are currently training on the COMM network, save it to experience buffer\n",
    "                            if (train_model == 'comm_red' and every[i] == RED_TEAM_ID) or \\\n",
    "                                    (train_model == 'comm_blue' and every[i] == BLUE_TEAM_ID):\n",
    "                                obs_replay.append(comm_in)\n",
    "                                actions_replay.append(comm_v)\n",
    "                                log_probs.append(comm_dist.log_prob(comm_v))\n",
    "                                stepid_replay.append(si)\n",
    "                        # Make the torch.Tensor communication matrix\n",
    "                        comm_m = torch.cat(comm_m)\n",
    "                        \n",
    "#                         print(f'  All the players communicated')\n",
    "\n",
    "                        ''' Predicting Who '''\n",
    "                        # Loop over every agent\n",
    "                        for i in range(NUM_PLAYERS):\n",
    "                            # continue if the agent is on the red team\n",
    "                            if every[i] == RED_TEAM_ID:\n",
    "                                continue\n",
    "                            # create input vector for network\n",
    "                            who_in = torch.cat((self_m[i], comm_m, hist.flatten()))\n",
    "                            \n",
    "#                             if agents == []:\n",
    "#                                 who_v = torch.rand(NUM_PLAYERS)\n",
    "#                             else:\n",
    "\n",
    "                            # Call the WHO network of the current agent (on the blue team)\n",
    "                            # and return the who vector (who)\n",
    "                            who_v = self.blue.WHO(who_in)\n",
    "                            \n",
    "                            # Update \"who_v\" into \"who\"\n",
    "                            who[i] = who_v\n",
    "                            # If we are currently training on the WHO network, save it to experience buffer\n",
    "                            if train_model == 'who_blue':\n",
    "                                obs_replay.append(who_in)\n",
    "                                actions_replay.append(who_v)\n",
    "                                stepid_replay.append(si)\n",
    "                        # update the who matrix to the environment\n",
    "                        self.env.update_who(who)\n",
    "                        \n",
    "#                         print(f'  All blue players predicted who')\n",
    "#                         for i in range(NUM_PLAYERS):\n",
    "#                             if every[i] == RED_TEAM_ID:\n",
    "#                                 continue\n",
    "#                             print(f'    {i} -> {who[i]}')\n",
    "\n",
    "                        ''' Deciding candidates to go on mission '''\n",
    "                        # create input vector for \"miss\" network\n",
    "                        miss_in = torch.cat((self_m[li], every if every[li] else who[li], hist.flatten()))\n",
    "\n",
    "                        # Only call the leader\n",
    "                        miss_dist = self.blue.MISS(miss_in) if every[li]==BLUE_TEAM_ID else self.red.MISS(miss_in)\n",
    "                        miss = miss_dist[nm - 2][0].sample()\n",
    "            \n",
    "                        # If we are currently training on the MISS network, save it tp experience buffer\n",
    "                        if (train_model == 'miss_red' and every[i] == RED_TEAM_ID) or \\\n",
    "                                (train_model == 'miss_blue' and every[i] == BLUE_TEAM_ID):\n",
    "                            obs_replay.append(miss_in)\n",
    "                            actions_replay.append(miss)\n",
    "                            log_probs.append(miss_dist)\n",
    "                            stepid_replay.append(si)\n",
    "                        # Update the \"miss\" vector to the environment\n",
    "                        miss = miss_players(miss) if nm==2 else 1-miss_players(miss)\n",
    "                        hist = self.env.update_miss(miss)\n",
    "                        \n",
    "#                         print(f'  Leader {li} decides {miss} go on mission')\n",
    "                            \n",
    "                        ''' Voting YES/NO for the mission candidates '''\n",
    "                        # Initialize vote vector\n",
    "                        vote = []\n",
    "                        # Loop over every agent   \n",
    "                        for i in range(5):\n",
    "                            # create input vector for network\n",
    "                            vote_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "\n",
    "                            # Call the VOTE network of the current agent and return vote_pi\n",
    "                            vote_dist, _ = self.blue.VOTE(vote_in) if every[i]==BLUE_TEAM_ID else self.red.VOTE(vote_in)\n",
    "                            vote_pi = vote_dist.sample()\n",
    "            \n",
    "                            # Append the voting results to \"vote\"\n",
    "                            vote.append(vote_pi)\n",
    "                            # If we are currently training on the VOTE network, save it to experience buffer\n",
    "                            if (train_model == 'vote_red' and every[i] == RED_TEAM_ID) or \\\n",
    "                                    (train_model == 'vote_blue' and every[i] == BLUE_TEAM_ID):\n",
    "                                obs_replay.append(vote_in)\n",
    "                                actions_replay.append(vote_pi)\n",
    "                                log_probs.append(vote_dist)\n",
    "                                stepid_replay.append(si)\n",
    "                        # Make the torch.Tensor vote vector\n",
    "                        vote = torch.Tensor(vote)\n",
    "                        \n",
    "                        # Update the \"vote\" vector to the environment\n",
    "                        hist = self.env.update_vote(vote)\n",
    "                        \n",
    "#                         print(f'  Voting results {vote} -> {(vote >= 0.5).sum() > 2}')\n",
    "                        \n",
    "    \n",
    "                        ''' Success/Failure for the mission '''\n",
    "                        # check if there are more yeses than noes\n",
    "                        if (vote >= 0.5).sum() > 2:\n",
    "                            # Initialize succ vector\n",
    "                            succ = []\n",
    "                            # Loop over every agent   \n",
    "                            for i in range(5):\n",
    "                                if not miss[i]:\n",
    "                                    continue\n",
    "                                # create input vector for network\n",
    "                                succ_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "                                \n",
    "#                                 if agents == []:\n",
    "#                                     succ_i = torch.rand(1)\n",
    "#                                 else:\n",
    "\n",
    "                                # Call the SUCCESS network of the current agent and return succ_i\n",
    "                                if every[i] == BLUE_TEAM_ID:\n",
    "                                    succ_i = torch.Tensor(1)\n",
    "                                else:\n",
    "                                    succ_dist, _ = self.red.SUCC(succ_in)\n",
    "                                    succ_i = succ_dist.sample()\n",
    "            \n",
    "                                # Append the voting results to \"vote\"\n",
    "                                succ.append(succ_i)\n",
    "                                # If we are currently training on the SUCCESS network, save it to experience buffer\n",
    "                                if train_model == 'succ_red' and every[i] == RED_TEAM_ID:\n",
    "                                    obs_replay.append(succ_in)\n",
    "                                    actions_replay.append(succ_i)\n",
    "                                    log_probs.append(succ_dist)\n",
    "                                    stepid_replay.append(si)\n",
    "                            # Make the torch.Tensor succ vector\n",
    "                            succ = torch.Tensor(succ)\n",
    "                            \n",
    "#                             print(f'  Succ = {succ}')\n",
    "                            \n",
    "                            # Update the \"succ\" vector to the environment\n",
    "                            hist = self.env.update_succ(succ)\n",
    "                            \n",
    "#                         print(f'  Mission: {hist[0,self.env.si-1,15:18]} - {hist[0,self.env.si-1,18]} Fails {hist[0,self.env.si-1,19]} Round')\n",
    "                        \n",
    "                        hist, every, who, li, si, mi, nm, done = self.env.get_observation()\n",
    "                        \n",
    "                    # check who won\n",
    "                    winning_team += [self.env.winning_team] * si\n",
    "                \n",
    "                # end of self.train_episodes episodes\n",
    "                # gather full_replay_buffer\n",
    "                global full_replay_buffer\n",
    "                full_replay_buffer = zip(obs_replay, actions_replay, stepid_replay, winning_team)\n",
    "                assert False\n",
    "                # sb['obs', 'action', 'advantage_gae' or 'advantage', 'discounted_reward']\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.winning_team == 1:\n",
    "    team_color = \"red\"\n",
    "else:\n",
    "    team_color = \"blue\"\n",
    "\n",
    "reward = 0\n",
    "if team_color in train_model:\n",
    "    reward += 1\n",
    "\n",
    "discounted_rewards = compute_discounted_return(rewards, replay_buffer, env.si)\n",
    "\n",
    "model_lookup = {\"comm_red\": agents[np.argmax(every)].comm,\n",
    " \"miss_red\" : agents[np.argmax(every)].miss,\n",
    " \"vote_red\" : agents[np.argmax(every)].vote, \n",
    " \"succ_red\" : agents[np.argmax(every)].succ,\n",
    " \"who_blue\" : agents[np.argmin(every)].who,\n",
    " \"comm_blue\" : agents[np.argmin(every)].comm,\n",
    " \"miss_blue\" : agents[np.argmin(every)].miss,\n",
    " \"vote_blue\" : agents[np.argmin(every)].vote}\n",
    "\n",
    "acmodel = model_lookup[train_model]\n",
    "\n",
    "dist, _ = acmodel(sb['obs'])            \n",
    "old_logp = dist.log_prob(sb['action']).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1827466900.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [36]\u001b[0;36m\u001b[0m\n\u001b[0;31m    if i+1 == stepN or\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# trainable_models=['comm_red', 'comm_blue', 'who_blue', 'miss_red', 'miss_blue', 'vote_red', 'vote_blue', 'succ_red']\n",
    "\n",
    "gamma = 0.9\n",
    "obs_replay, actions_replay, stepid_replay, winning_team = zip(*full_replay_buffer)\n",
    "\n",
    "stepN = len(stepid_replay)\n",
    "team_color = 'red' if winning_team[-1] == RED_TEAM_ID else 'blue'\n",
    "discounted_rewards = [1 if team_color in train_model else -1]\n",
    "\n",
    "for i in range(stepN - 2, -1, -1):\n",
    "    if stepid_replay[i] != 0 and stepid_replay[i+1] == 0:\n",
    "        team_color = 'red' if winning_team[i] == RED_TEAM_ID else 'blue'\n",
    "        discounted_rewards.append(1 if team_color in train_model else -1)\n",
    "    else:\n",
    "        discounted_rewards(discounted_rewards[-1] * gamma)\n",
    "\n",
    "discounted_rewards.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs_replay, actions_replay, stepid_replay, discounted_rewards, value_function):\n",
    "    # Example:\n",
    "    # preprocess(*full_replay_buffer, range(10), (lambda x: 0))\n",
    "    assert len(obs_replay) == len(actions_replay)\n",
    "    assert len(obs_replay) == len(stepid_replay)\n",
    "    assert len(discounted_rewards) == len(obs_replay)\n",
    "    sbs = []\n",
    "    for i in range(len(obs_replay)):\n",
    "        sb = {\n",
    "            \"obs\" : obs_replay[i],\n",
    "            \"actions\" : actions_replay[i],\n",
    "            \"advantage\" : discounted_rewards[i] - value_function(obs_replay[i]),\n",
    "            \"discounted_reward\" : discounted_rewards[i],\n",
    "        }\n",
    "        sbs.append(sb)\n",
    "    return sbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2deecb6e3c143358bc580225251d978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24eee0ee2fb54bffa71f5c5dd5521920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/andrewlai/OneDrive - Massachusetts Institute of Technology/MIT/2022 Spring/6.484 Sensorimotor Learning/Final project/main/avalon/main.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewlai/OneDrive%20-%20Massachusetts%20Institute%20of%20Technology/MIT/2022%20Spring/6.484%20Sensorimotor%20Learning/Final%20project/main/avalon/main.ipynb#ch0000013?line=7'>8</a>\u001b[0m my_env \u001b[39m=\u001b[39m AvalonEnv()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andrewlai/OneDrive%20-%20Massachusetts%20Institute%20of%20Technology/MIT/2022%20Spring/6.484%20Sensorimotor%20Learning/Final%20project/main/avalon/main.ipynb#ch0000013?line=8'>9</a>\u001b[0m engine \u001b[39m=\u001b[39m AvalonEngine(env\u001b[39m=\u001b[39mmy_env, blue\u001b[39m=\u001b[39mblue, red\u001b[39m=\u001b[39mred, train_episodes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, max_epoch\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andrewlai/OneDrive%20-%20Massachusetts%20Institute%20of%20Technology/MIT/2022%20Spring/6.484%20Sensorimotor%20Learning/Final%20project/main/avalon/main.ipynb#ch0000013?line=9'>10</a>\u001b[0m                       trainable_models\u001b[39m=\u001b[39mtrainable_models)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andrewlai/OneDrive%20-%20Massachusetts%20Institute%20of%20Technology/MIT/2022%20Spring/6.484%20Sensorimotor%20Learning/Final%20project/main/avalon/main.ipynb#ch0000013?line=10'>11</a>\u001b[0m engine\u001b[39m.\u001b[39;49mrun()\n",
      "\u001b[1;32m/Users/andrewlai/OneDrive - Massachusetts Institute of Technology/MIT/2022 Spring/6.484 Sensorimotor Learning/Final project/main/avalon/main.ipynb Cell 10'\u001b[0m in \u001b[0;36mAvalonEngine.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/andrewlai/OneDrive%20-%20Massachusetts%20Institute%20of%20Technology/MIT/2022%20Spring/6.484%20Sensorimotor%20Learning/Final%20project/main/avalon/main.ipynb#ch0000011?line=192'>193</a>\u001b[0m \u001b[39mglobal\u001b[39;00m full_replay_buffer\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/andrewlai/OneDrive%20-%20Massachusetts%20Institute%20of%20Technology/MIT/2022%20Spring/6.484%20Sensorimotor%20Learning/Final%20project/main/avalon/main.ipynb#ch0000011?line=193'>194</a>\u001b[0m full_replay_buffer \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(obs_replay, actions_replay, stepid_replay, winning_team)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/andrewlai/OneDrive%20-%20Massachusetts%20Institute%20of%20Technology/MIT/2022%20Spring/6.484%20Sensorimotor%20Learning/Final%20project/main/avalon/main.ipynb#ch0000011?line=194'>195</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainable_models=['comm_red', 'comm_blue', 'who_blue', 'miss_red', 'miss_blue', 'vote_red', 'vote_blue', 'succ_red']\n",
    "\n",
    "set_random_seed(0)\n",
    "\n",
    "blue = BlueAgent()\n",
    "red = RedAgent()\n",
    "\n",
    "my_env = AvalonEnv()\n",
    "engine = AvalonEngine(env=my_env, blue=blue, red=red, train_episodes=10, max_epoch=10,\n",
    "                      trainable_models=trainable_models)\n",
    "engine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_config():\n",
    "    # here goes the default parameters for the agent\n",
    "    config = dict(\n",
    "        # env=env, the agent does not need to ahve access to the env because there is an engine\n",
    "        learning_rate=0.00025,\n",
    "        gamma=0.99,\n",
    "        memory_size=200000,\n",
    "        initial_epsilon=1.0,\n",
    "        min_epsilon=0.1,\n",
    "        max_epsilon_decay_steps=150000,\n",
    "        warmup_steps=500,\n",
    "        target_update_freq=2000,\n",
    "        batch_size=32,\n",
    "        device=None,\n",
    "        disable_target_net=False,\n",
    "        enable_double_q=False\n",
    "    )\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def compute_advantage_gae(values, rewards, T, gae_lambda, discount):\n",
    "    #advantages = torch.zeros_like(values)\n",
    "\n",
    "    #### TODO: populate GAE in advantages over T timesteps (10 pts) ############\n",
    "    theta = rewards - values\n",
    "    theta[:-1] += discount * values[1:]\n",
    "    gl = discount * gae_lambda\n",
    "    advantages = torch.tensor([\n",
    "        (theta[i:] * (gl ** torch.arange(len(rewards)-i))).sum()\n",
    "        for i in range(len(rewards))\n",
    "    ])\n",
    "    ############################################################################\n",
    "    \n",
    "    return advantages[:T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title \n",
    "def compute_discounted_return(rewards, discount, device=None):\n",
    "    returns = torch.zeros(*rewards.shape, device=device)\n",
    "    \n",
    "    #### TODO: populate discounted reward trajectory (10 pts) ############\n",
    "    returns = torch.tensor([\n",
    "            torch.sum(rewards[i:] * (discount ** torch.arange(0, len(rewards)-i)))\n",
    "            for i in range(len(rewards))\n",
    "        ])\n",
    "    ######################################################################\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'comm_red', 'comm_blue'\n",
    "# 'miss_red', 'miss_blue', 'vote_red', 'vote_blue', 'succ_red'\n",
    "\n",
    "def update_parameters_ppo(optimizer, acmodel, sb, args):\n",
    "    def _compute_policy_loss_ppo(logp, old_logp, entropy, advantages):\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        ### TODO: implement PPO policy loss computation (30 pts).  #######\n",
    "        logr = logp - old_logp\n",
    "        ratios = torch.exp(logr)\n",
    "        \n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1-args.clip_ratio, 1+args.clip_ratio) * advantages\n",
    "        \n",
    "        policy_loss = (-torch.min(surr1, surr2) -args.entropy_coef*entropy).mean()\n",
    "        \n",
    "        # approx_kl = torch.sum(torch.exp(logp) * logr)\n",
    "        #approx_kl = torch.nn.functional.kl_div(logp, old_logp)\n",
    "\n",
    "        # approx_kl = ((logr.exp() - 1) - logr).sum()\n",
    "        approx_kl = ((logr ** 2) / 2).sum()\n",
    "        \n",
    "        ##################################################################\n",
    "        \n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(values, returns):\n",
    "        ### TODO: implement PPO value loss computation (10 pts) ##########\n",
    "        value_loss = F.mse_loss(values.squeeze(-1), returns).mean() #(values - returns).pow(2).mean()\n",
    "        ##################################################################\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    dist, values = acmodel(sb['obs'])\n",
    "    \n",
    "    print(dist)\n",
    "    \n",
    "    old_logp = dist.log_prob(sb['action']).detach()\n",
    "    logp = dist.log_prob(sb['action'])\n",
    "    dist_entropy = dist.entropy()\n",
    "    \n",
    "    advantage = sb['advantage_gae'] if args.use_gae else sb['advantage']\n",
    "    \n",
    "    policy_loss, _ = _compute_policy_loss_ppo(logp, old_logp, dist_entropy, advantage)\n",
    "    value_loss = _compute_value_loss(values, sb['discounted_reward'])\n",
    "\n",
    "    for i in range(args.train_ac_iters):\n",
    "        dists, values = acmodel(sb['obs'])\n",
    "        logp = dists.log_prob(sb['action'])\n",
    "        dist_entropy = dists.entropy()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_pi, approx_kl = _compute_policy_loss_ppo(logp, old_logp, dist_entropy, advantage)\n",
    "        loss_v = _compute_value_loss(values, sb['discounted_reward'])\n",
    "\n",
    "        loss = loss_v + loss_pi\n",
    "        if approx_kl > 1.5 * args.target_kl:\n",
    "            break\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    \n",
    "    update_policy_loss = policy_loss.item()\n",
    "    update_value_loss = value_loss.item()\n",
    "\n",
    "    logs = {\n",
    "        \"policy_loss\": update_policy_loss,\n",
    "        \"value_loss\": update_value_loss,\n",
    "    }\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
