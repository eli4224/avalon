{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import easyrl.models.diag_gaussian_policy as DiagGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# set random seed\n",
    "seed = 0\n",
    "set_random_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDefaultParams():\n",
    "    # here goes the default parameters for the agent\n",
    "    config = dict(\n",
    "        # env=env, the agent does not need to ahve access to the env because there is an engine\n",
    "        learning_rate=0.00025,\n",
    "        gamma=0.99,\n",
    "        memory_size=200000,\n",
    "        initial_epsilon=1.0,\n",
    "        min_epsilon=0.1,\n",
    "        max_epsilon_decay_steps=150000,\n",
    "        warmup_steps=500,\n",
    "        target_update_freq=2000,\n",
    "        batch_size=32,\n",
    "        device=None,\n",
    "        disable_target_net=False,\n",
    "        enable_double_q=False,\n",
    "        use_gae=False,\n",
    "        train_ac_iters=5,\n",
    "        target_kl=0.01,\n",
    "        clip_ratio=0.2,\n",
    "        entropy_coef=0.01,\n",
    "        use_critic=True\n",
    "    )\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create categorical policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from easyrl Categorical policy policy with some modifications\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self,\n",
    "                 body_net,\n",
    "                 action_dim,\n",
    "                 in_features=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.body = body_net\n",
    "        if in_features is None:\n",
    "            for i in reversed(range(len(self.body.fcs))):\n",
    "                layer = self.body.fcs[i]\n",
    "                if hasattr(layer, 'out_features'):\n",
    "                    in_features = layer.out_features\n",
    "                    break\n",
    "\n",
    "        self.head = nn.Sequential(nn.Linear(in_features, action_dim), nn.Softmax())\n",
    "\n",
    "    def forward(self, x=None, body_x=None, **kwargs):\n",
    "        if x is None and body_x is None:\n",
    "            raise ValueError('One of [x, body_x] should be provided!')\n",
    "        if body_x is None:\n",
    "            body_x = self.body(x, **kwargs)\n",
    "        if isinstance(body_x, tuple):\n",
    "            pi = self.head(body_x[0])\n",
    "        else:\n",
    "            pi = self.head(body_x)\n",
    "        action_dist = Categorical(probs=pi)\n",
    "        return action_dist, body_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create backbone NN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class NNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, final_activation=None, use_critic=False):\n",
    "        self.final_activation = final_activation\n",
    "        self.use_critic = use_critic\n",
    "        super().__init__()\n",
    "        #### A simple network that takes\n",
    "        #### as input the history, and outputs the \n",
    "        #### distribution parameters.\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU())\n",
    "        self.out_layer = nn.Sequential(nn.Linear(64, action_dim))\n",
    "        \n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(input_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 32),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "        \n",
    "        # Set initial weights and biases\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, ob):\n",
    "        mid_logits = self.fcs(ob)\n",
    "        logits = self.out_layer(mid_logits)\n",
    "        if self.final_activation is not None:\n",
    "            logits = self.final_activation(logits, dim=-1)\n",
    "        if self.use_critic:\n",
    "            return logits, self.critic(ob)\n",
    "        else:\n",
    "            return logits, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define game parameters and NN initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED_TEAM_ID = 1\n",
    "BLUE_TEAM_ID = 0\n",
    "NUM_PLAYERS = 5\n",
    "RED_PLAYERS = 2\n",
    "BLUE_PLAYERS = 3\n",
    "HIST_SHAPE = 2 * 25 * (3 * NUM_PLAYERS + 5)\n",
    "SELF_SHAPE = torch.tensor([5])\n",
    "COMM_SHAPE = 32  # Change freely\n",
    "WHO_SHAPE = torch.tensor([NUM_PLAYERS])\n",
    "VOTE_SHAPE =torch.tensor([NUM_PLAYERS])\n",
    "\n",
    "assert RED_PLAYERS + BLUE_PLAYERS == NUM_PLAYERS\n",
    "\n",
    "def get_who():\n",
    "    return NNetwork(SELF_SHAPE + HIST_SHAPE + NUM_PLAYERS*COMM_SHAPE, WHO_SHAPE, nn.functional.softmax, use_critic=False)\n",
    "\n",
    "def get_comm(use_critic=False):\n",
    "    return DiagGaussian.DiagGaussianPolicy(NNetwork(SELF_SHAPE + HIST_SHAPE + WHO_SHAPE, torch.tensor([64]), use_critic=use_critic), COMM_SHAPE, in_features=torch.tensor([64]))\n",
    "\n",
    "def get_miss(mission_shapes = (10,10), use_critic=False):\n",
    "    model = NNetwork(SELF_SHAPE + WHO_SHAPE + HIST_SHAPE, 64, use_critic=use_critic)\n",
    "    return [CategoricalPolicy(model, mish, 64) for mish in mission_shapes]\n",
    "\n",
    "def get_vote(use_critic=False):\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + SELF_SHAPE + WHO_SHAPE, 128, use_critic=use_critic), 2, 128)\n",
    "\n",
    "def get_succ(use_critic=False):\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + SELF_SHAPE + WHO_SHAPE, 128,use_critic=use_critic), 2, 128)\n",
    "\n",
    "@torch.no_grad()\n",
    "def miss_players(miss):\n",
    "    # 0: 0,1\n",
    "    # 1: 0,2\n",
    "    # 2: 0,3\n",
    "    # 3: 0,4\n",
    "    # 4: 1,2\n",
    "    # 5: 1,3\n",
    "    # 6: 1,4\n",
    "    # 7: 2,3\n",
    "    # 8: 2,4\n",
    "    # 9: 3,4\n",
    "    miss_cat = torch.zeros(NUM_PLAYERS)\n",
    "    if miss<4:\n",
    "        miss_cat[0] = 1\n",
    "        miss_cat[miss+1] = 1\n",
    "    elif miss<7:\n",
    "        miss_cat[1] = 1\n",
    "        miss_cat[miss-2] = 1\n",
    "    elif miss<9:\n",
    "        miss_cat[2] = 1\n",
    "        miss_cat[miss-4] = 1\n",
    "    else:\n",
    "        miss_cat[3] = 1\n",
    "        miss_cat[miss-5] = 1\n",
    "    return miss_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def comm():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def who():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def miss():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def vote():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def succ():\n",
    "        pass\n",
    "\n",
    "class RedAgent():\n",
    "\n",
    "    def __init__(self, lr, use_critic):\n",
    "        self.lr = lr\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        self.COMM = get_comm(use_critic = use_critic)\n",
    "#         self.who = (lambda *args : every)\n",
    "        mission_models = get_miss(use_critic = use_critic)\n",
    "        self.MISS = (lambda args : [mission_models[i](args) for i in range(2)])\n",
    "        self.VOTE = get_vote(use_critic = use_critic)\n",
    "        self.SUCC = get_succ(use_critic = use_critic)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.COMM_opt = torch.optim.Adam(self.COMM.parameters(), lr=lr)\n",
    "        self.MISS_opt2 = torch.optim.Adam(mission_models[0].parameters(), lr=lr)\n",
    "        self.MISS_opt3 = torch.optim.Adam(mission_models[1].parameters(), lr=lr)\n",
    "        self.VOTE_opt = torch.optim.Adam(self.VOTE.parameters(), lr=lr)\n",
    "        self.SUCC_opt = torch.optim.Adam(self.SUCC.parameters(), lr=lr)\n",
    "\n",
    "class BlueAgent():\n",
    "    \n",
    "    def __init__(self, lr, use_critic):\n",
    "        self.lr = lr\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        self.COMM = get_comm(use_critic=use_critic)\n",
    "        self.WHO = get_who()\n",
    "        mission_models = get_miss(use_critic=use_critic)\n",
    "        self.MISS = (lambda args : [mission_models[i](args) for i in range(2)])\n",
    "        self.VOTE = get_vote(use_critic=use_critic)\n",
    "#         self.succ = (lambda *args : torch.distributions.bernoulli.Bernoulli(1))\n",
    "        \n",
    "        # Optimizers\n",
    "        self.COMM_opt = torch.optim.Adam(self.COMM.parameters(), lr=lr)\n",
    "        self.WHO_opt = torch.optim.Adam(self.WHO.parameters(), lr=lr)\n",
    "        self.MISS_opt2 = torch.optim.Adam(mission_models[0].parameters(), lr=lr)\n",
    "        self.MISS_opt3 = torch.optim.Adam(mission_models[1].parameters(), lr=lr)\n",
    "        self.VOTE_opt = torch.optim.Adam(self.VOTE.parameters(), lr=lr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline agents\n",
    "class ConstantDistribution():\n",
    "#   A \"distribution\" that always returns the same result when sampled\n",
    "\n",
    "    def __init__(self, values):\n",
    "#       Values must support element-wise comparison with ==\n",
    "        self.values = values\n",
    "    \n",
    "    def sample(self):\n",
    "        return self.values\n",
    "    \n",
    "    def log_prob(self, actions):\n",
    "        return self.values == actions\n",
    "\n",
    "class RedAgentBaseline():\n",
    "    # this red agent takes actions according to the following rules:\n",
    "    # COMM - outputs a random vector\n",
    "    # MISS - always randomly choose 1 red player and 1~2 blue players (according to how many players go on mission)\n",
    "    # VOTE - always vote yes if there are one or more red players on the mission, else no\n",
    "    # SUCC - always fail the mission\n",
    "\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.COMM = (lambda args: (ConstantDistribution(torch.rand((COMM_SHAPE))), None))\n",
    "        \n",
    "        # Optimizers\n",
    "        self.COMM_opt = None\n",
    "        self.MISS_opt2 = None\n",
    "        self.MISS_opt3 = None\n",
    "        self.VOTE_opt = None\n",
    "        self.SUCC_opt = None\n",
    "\n",
    "    def SUCC(self, obs):\n",
    "        return Categorical(probs=torch.Tensor([1])), None\n",
    "    \n",
    "    def VOTE(self, obs):\n",
    "        # obs = SELF_SHAPE + EVERY_SHAPE + (HIST_SHAPE) <- who goes on mission is here\n",
    "        # env.hist = torch.zeros((2, 25, 20))\n",
    "        every = obs[SELF_SHAPE:SELF_SHAPE+WHO_SHAPE]\n",
    "        hist = obs[SELF_SHAPE+WHO_SHAPE:]\n",
    "        hist = hist.reshape((2, 25, len(hist) // 50))\n",
    "        # find the LAST VALID mission combination\n",
    "        # -- hist[1, si, 5:10]\n",
    "        miss = hist[0, hist[1, :, 5].sum().long()-1, 5:10]\n",
    "        probs = torch.Tensor([0, 1]) if (miss == RED_TEAM_ID).sum() > 0 else torch.Tensor([1, 0])\n",
    "        return Categorical(probs=probs), None\n",
    "\n",
    "    def MISS(self, obs):\n",
    "        # obs = SELF_SHAPE + EVERY_SHAPE + HIST_SHAPE\n",
    "        every = obs[SELF_SHAPE:SELF_SHAPE+WHO_SHAPE]\n",
    "        # miss 2\n",
    "        misses = []\n",
    "        for num_pl_miss in [2, 3]:\n",
    "            seq = np.arange(math.comb(NUM_PLAYERS, num_pl_miss))\n",
    "            np.random.shuffle(seq)\n",
    "            for comb in seq:\n",
    "                miss = miss_players(comb) if num_pl_miss == 2 else 1 - miss_players(comb)\n",
    "                if (every[miss.numpy().astype(bool)] == RED_TEAM_ID).sum() == 1:\n",
    "                    break\n",
    "            assert (every[miss.numpy().astype(bool)] == RED_TEAM_ID).sum() == 1\n",
    "            misses.append(torch.clone(miss))\n",
    "        return (ConstantDistribution(torch.Tensor(miss[0])), None), (ConstantDistribution(torch.Tensor(miss[1])), None)\n",
    "\n",
    "class BlueAgentBaseline():\n",
    "    # this blue agent takes actions according to the following rules:\n",
    "    # COMM - outputs a random vector\n",
    "    # MISS - always choose itself and 1~2 other players (according to how many players go on mission)\n",
    "    # VOTE - always vote yes if itself is chosen, else no\n",
    "    # (For WHO, we can assign a BADNESS factor to every other agent according to the number of failures of the missions that the other agents deployed)\n",
    "\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.COMM = lambda args: (ConstantDistribution(torch.rand((COMM_SHAPE))), None)\n",
    "        \n",
    "        self.COMM_opt = None\n",
    "        self.WHO_opt = None\n",
    "        self.MISS_opt2 = None\n",
    "        self.MISS_opt3 = None\n",
    "        self.VOTE_opt = None\n",
    "    \n",
    "    def VOTE(self, obs):\n",
    "        # obs = SELF_SHAPE + WHO_SHAPE + (HIST_SHAPE) <- who goes on mission is here\n",
    "        # env.hist = torch.zeros((2, 25, 20))\n",
    "        self_v = obs[:SELF_SHAPE]\n",
    "        hist = obs[SELF_SHAPE+WHO_SHAPE:]\n",
    "        hist = hist.reshape((2, 25, len(hist) // 50))\n",
    "        # find the LAST VALID mission combination\n",
    "        # -- hist[1, si, 5:10]\n",
    "        miss = hist[0, hist[1, :, 5].sum().long()-1, 5:10]\n",
    "        # 1 if cond else 0\n",
    "        probs = torch.Tensor([0, 1]) if (miss * self_v).sum() == 0 else torch.Tensor([1, 0])\n",
    "        return Categorical(probs=probs), None\n",
    "\n",
    "    def MISS(self, obs):\n",
    "        # obs = SELF_SHAPE + WHO_SHAPE + HIST_SHAPE\n",
    "        self_v = obs[:SELF_SHAPE]\n",
    "        # miss 2\n",
    "        misses = []\n",
    "        for num_pl_miss in [2, 3]:\n",
    "            seq = np.arange(math.comb(NUM_PLAYERS, num_pl_miss))\n",
    "            np.random.shuffle(seq)\n",
    "            for comb in seq:\n",
    "                miss = miss_players(comb) if num_pl_miss == 2 else 1 - miss_players(comb)\n",
    "                if (self_v * miss).sum() == 1:\n",
    "                    break\n",
    "            assert (self_v * miss).sum() == 1\n",
    "            misses.append(torch.clone(miss))\n",
    "        return (ConstantDistribution(torch.Tensor(miss[0]).long()), None), (ConstantDistribution(torch.Tensor(miss[1]).long()), None)\n",
    "    \n",
    "    def WHO(self, obs):\n",
    "#         obs = obs.numpy()\n",
    "        self_v = obs[0:SELF_SHAPE]\n",
    "        _comm_v = obs[SELF_SHAPE:COMM_SHAPE*NUM_PLAYERS]\n",
    "        hist = obs[SELF_SHAPE+COMM_SHAPE*NUM_PLAYERS:]\n",
    "        hist = hist.reshape((2, 25, len(hist) // 50))\n",
    "        \n",
    "        # Create badness score\n",
    "        previous_mission_players = hist[1, :, 5:10]\n",
    "#         previous_mission_successes = np.argmax(hist[1, :, 15:18], axis=-1)\n",
    "        # 0 -> no mission\n",
    "        # 1 -> success\n",
    "        # 2 -> failure\n",
    "        mission_no_result = hist[1, :, 15] == 1\n",
    "        mission_failures = hist[1, :, 17] == 1\n",
    "        \n",
    "        # Number of missions each player was on minus the missions with no result\n",
    "        total_previous_mission_players = torch.sum(previous_mission_players, dim=0) - torch.sum(previous_mission_players * mission_no_result.reshape(25,1), dim=0)\n",
    "        # Number of mission each player was on that failed\n",
    "        previous_mission_failures_players = torch.sum(previous_mission_players * mission_failures.reshape(25,1), dim=0)\n",
    "        \n",
    "        # Badness = Number_of_failed_missions_you_were_on / number_of_missions_you_were_on\n",
    "        badness_score = previous_mission_failures_players / total_previous_mission_players\n",
    "        # In the case where someone has never been on a mission, this will result in a nan\n",
    "        # We will fill nans with 0.5.\n",
    "        badness_score = torch.nan_to_num(badness_score, nan=0.5)\n",
    "        \n",
    "        # We know that I am blue\n",
    "        badness_score[np.argmax(self_v)] = 0\n",
    "        \n",
    "        return badness_score, None\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AvalonEnv():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nm = [2, 3, 2, 3, 3, 0]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.mi = 0\n",
    "        self.ri = 0\n",
    "        self.si = 0\n",
    "        self.li = 0\n",
    "        self.hist = torch.zeros((2, 25, 20))\n",
    "        every = [0, 0, 0, 1, 1]\n",
    "        random.shuffle(every)\n",
    "        self.every = torch.Tensor(every.copy())        \n",
    "        self.who = torch.normal(0.5, 0.1, (NUM_PLAYERS, NUM_PLAYERS))\n",
    "        self.done = False\n",
    "        self.winning_team = None\n",
    "        \n",
    "        # the initial observation of \"hist\" is all zeros\n",
    "        # EXCEPT a one at the leader id (self.li) location of the zero-th step\n",
    "        self.hist[0, 0, self.li] = 1\n",
    "        self.hist[1, 0, :5] = 1\n",
    "        \n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self):         \n",
    "        return self.hist, self.every, self.who, self.li, self.si, self.mi, self.nm[self.mi], self.done\n",
    "    \n",
    "    def update_who(self, who_m):\n",
    "        self.who = who_m\n",
    "        \n",
    "    def update_miss(self, miss):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 5:10] = miss.detach()\n",
    "        self.hist[1, self.si, 5:10] = 1\n",
    "        return self.hist\n",
    "    \n",
    "    def update_vote(self, vote):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 10:15] = vote\n",
    "        self.hist[1, self.si, 10:15] = 1\n",
    "        \n",
    "        # check if there are more yeses than noes\n",
    "        if (vote >= 0.5).sum() > 2:\n",
    "            # set relevance of only the no mission flag\n",
    "            self.hist[1, self.si, 15] = 1\n",
    "        else:\n",
    "            # set the no mission flag\n",
    "            self.hist[0, self.si, 15] = 1\n",
    "\n",
    "            # set the current round\n",
    "            self.hist[0, self.si, 19] = self.ri\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] if self.si else 0\n",
    "            \n",
    "            # set relevance\n",
    "            self.hist[1, self.si, 15:] = 1\n",
    "            \n",
    "            if self.ri == 4:\n",
    "                # game is over, red team wins\n",
    "                self.winning_team = RED_TEAM_ID\n",
    "                self.done = True\n",
    "                \n",
    "            # update leader\n",
    "            self.li = (self.li + 1) % 5\n",
    "            self.hist[0, self.si, self.li] = 1\n",
    "            self.hist[1, self.si, :5] = 1\n",
    "            \n",
    "            # update round id\n",
    "            self.ri = (self.ri+1) % 5\n",
    "            \n",
    "            # update step id\n",
    "            self.si += 1\n",
    "        return self.hist\n",
    "        \n",
    "    def update_succ(self, succ):\n",
    "        # set the current round\n",
    "        self.hist[0, self.si, 19] = self.ri\n",
    "        \n",
    "        # set relevance\n",
    "        self.hist[1, self.si, 16:] = 1\n",
    "        \n",
    "        if (succ < 0.5).sum():\n",
    "            # set the mission failure flag\n",
    "            self.hist[0, self.si, 17] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] + 1 if self.si else 1\n",
    "        else:\n",
    "            # set the mission success flag\n",
    "            self.hist[0, self.si, 16] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] if self.si else 0\n",
    "        \n",
    "        # check if game is over\n",
    "        if self.hist[0, self.si, 18]  == 3:\n",
    "            # game is over, red team wins\n",
    "            self.winning_team = RED_TEAM_ID\n",
    "            self.done = True\n",
    "        elif self.mi == 2 + self.hist[0, self.si, 18]:\n",
    "            # game is over, blue team wins\n",
    "            self.winning_team = BLUE_TEAM_ID\n",
    "            self.done = True\n",
    "            \n",
    "        # update mission id\n",
    "        self.mi += 1\n",
    "\n",
    "        # update round id\n",
    "        self.ri = 0\n",
    "\n",
    "        # update step id\n",
    "        self.si += 1\n",
    "\n",
    "        # update leader\n",
    "        self.li = (self.li + 1) % 5\n",
    "        self.hist[0, self.si, self.li] = 1\n",
    "        self.hist[1, self.si, :5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game engine and runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = getDefaultParams()\n",
    "@dataclass\n",
    "class AvalonEngine:\n",
    "    env: AvalonEnv\n",
    "    train_episodes: int\n",
    "    max_epoch: int\n",
    "    blue: BlueAgent\n",
    "    red: RedAgent\n",
    "    trainable_models: list\n",
    "    gamma: float\n",
    "    gae_lambda: float\n",
    "    \n",
    "    def run(self, is_training=True):\n",
    "        # Useful for constructing the self vector (self_v)\n",
    "        self_m = torch.eye(5)\n",
    "        history_log = []\n",
    "        winning_log = []\n",
    "        every_log = []\n",
    "        \n",
    "        for epoch in tqdm(range(self.max_epoch), desc='epoch'):\n",
    "            for train_model in tqdm(self.trainable_models, desc='train_model'):\n",
    "                # print(f'train model = {train_model}')\n",
    "\n",
    "                winning_team = []\n",
    "                every_replay = []\n",
    "                # trajectory buffer\n",
    "                if is_training:\n",
    "                    obs_replay_epoch = []\n",
    "                    actions_replay_epoch = []\n",
    "                    log_probs_epoch = []\n",
    "                    stepid_replay_epoch = []\n",
    "                    \n",
    "\n",
    "                for episode in range(self.train_episodes):\n",
    "                    hist, every, who, li, si, mi, nm, done = self.env.reset()\n",
    "                    \n",
    "                    every_replay.append(every)\n",
    "                    if is_training:\n",
    "                        obs_replay = []\n",
    "                        actions_replay = []\n",
    "                        log_probs = []\n",
    "                        stepid_replay = []\n",
    "                        \n",
    "\n",
    "                    while not done:\n",
    "                        # print(f'li, si, mi, ri = {li}, {si}, {mi}, {self.env.ri}')\n",
    "                        # Flow: communication -> predict who -> decide miss -> voting -> succ/fail -> next round\n",
    "                        #                                                             -> next round\n",
    "\n",
    "#                         print(f'Mission {mi} Round {self.env.ri} Step {si}')\n",
    "                        \n",
    "                        ''' Communication '''\n",
    "                        # Initialize communication matrix\n",
    "                        comm_m = []\n",
    "                        # Loop over every agent\n",
    "                        for i in range(5):\n",
    "                            # create input vector for network                            \n",
    "                            comm_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "                            \n",
    "#                             if agents == []:\n",
    "#                                 comm_v = torch.rand(COMM_SHAPE)\n",
    "#                             else:\n",
    "                            \n",
    "                            # Call the COMM network of the current agent and return the communication vector (comm_v)\n",
    "                            comm_dist, _ = self.blue.COMM(comm_in) if every[i]==BLUE_TEAM_ID else self.red.COMM(comm_in)\n",
    "                            comm_v = comm_dist.sample()\n",
    "                \n",
    "                            # Append it to the communication matrix (comm)\n",
    "                            comm_m.append(comm_v)\n",
    "                            # If we are currently training on the COMM network, save it to experience buffer\n",
    "                            if is_training and ((train_model == 'comm_red' and every[i] == RED_TEAM_ID) or \\\n",
    "                                    (train_model == 'comm_blue' and every[i] == BLUE_TEAM_ID)):\n",
    "                                obs_replay.append(comm_in)\n",
    "                                actions_replay.append(comm_v)\n",
    "                                log_probs.append(comm_dist.log_prob(comm_v))\n",
    "                                stepid_replay.append(si)\n",
    "                        # Make the torch.Tensor communication matrix\n",
    "                        comm_m = torch.cat(comm_m)\n",
    "                        \n",
    "#                         print(f'  All the players communicated')\n",
    "\n",
    "                        ''' Predicting Who '''\n",
    "                        # Loop over every agent\n",
    "                        for i in range(NUM_PLAYERS):\n",
    "                            # continue if the agent is on the red team\n",
    "                            if every[i] == RED_TEAM_ID:\n",
    "                                continue\n",
    "                            # create input vector for network\n",
    "                            who_in = torch.cat((self_m[i], comm_m, hist.flatten()))\n",
    "\n",
    "                            # Call the WHO network of the current agent (on the blue team)\n",
    "                            # and return the who vector (who)\n",
    "                            who_v, _ = self.blue.WHO(who_in)\n",
    "                            \n",
    "                            # Update \"who_v\" into \"who\"\n",
    "                            who[i] = who_v\n",
    "                            # If we are currently training on the WHO network, save it to experience buffer\n",
    "                            if is_training and train_model == 'who_blue':\n",
    "                                obs_replay.append(who_in)\n",
    "                                actions_replay.append(who_v)\n",
    "                                stepid_replay.append(si)\n",
    "                        # update the who matrix to the environment\n",
    "                        self.env.update_who(who)\n",
    "                        \n",
    "#                         print(f'  All blue players predicted who')\n",
    "#                         for i in range(NUM_PLAYERS):\n",
    "#                             if every[i] == RED_TEAM_ID:\n",
    "#                                 continue\n",
    "#                             print(f'    {i} -> {who[i]}')\n",
    "\n",
    "                        ''' Deciding candidates to go on mission '''\n",
    "                        # create input vector for \"miss\" network\n",
    "                        miss_in = torch.cat((self_m[li], every if every[li] else who[li], hist.flatten()))\n",
    "\n",
    "                        # Only call the leader\n",
    "                        miss_dist = self.blue.MISS(miss_in) if every[li]==BLUE_TEAM_ID else self.red.MISS(miss_in)\n",
    "                        miss = miss_dist[nm - 2][0].sample()\n",
    "            \n",
    "                        # If we are currently training on the MISS network, save it to the experience buffer\n",
    "                        if is_training and (('miss_red' in train_model and every[i] == RED_TEAM_ID) or ('miss_blue' in train_model and every[i] == BLUE_TEAM_ID)):\n",
    "                            if str(nm) in train_model:\n",
    "                                obs_replay.append(miss_in)\n",
    "                                actions_replay.append(miss)\n",
    "                                log_probs.append(miss_dist)\n",
    "                                stepid_replay.append(si)\n",
    "                        # Update the \"miss\" vector to the environment\n",
    "                        miss = miss_players(miss) if nm==2 else 1-miss_players(miss)\n",
    "                        hist = self.env.update_miss(miss)\n",
    "                        \n",
    "#                         print(f'  Leader {li} decides {miss} go on mission')\n",
    "                            \n",
    "                        ''' Voting YES/NO for the mission candidates '''\n",
    "                        # Initialize vote vector\n",
    "                        vote = []\n",
    "                        # Loop over every agent\n",
    "                        for i in range(5):\n",
    "                            # create input vector for network\n",
    "                            vote_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "\n",
    "                            # Call the VOTE network of the current agent and return vote_pi\n",
    "                            vote_dist, _ = self.blue.VOTE(vote_in) if every[i]==BLUE_TEAM_ID else self.red.VOTE(vote_in)\n",
    "                            vote_pi = vote_dist.sample()\n",
    "            \n",
    "                            # Append the voting results to \"vote\"\n",
    "                            vote.append(vote_pi)\n",
    "                            # If we are currently training on the VOTE network, save it to experience buffer\n",
    "                            if is_training and ((train_model == 'vote_red' and every[i] == RED_TEAM_ID) or \\\n",
    "                                    (train_model == 'vote_blue' and every[i] == BLUE_TEAM_ID)):\n",
    "                                obs_replay.append(vote_in)\n",
    "                                actions_replay.append(vote_pi)\n",
    "                                log_probs.append(vote_dist)\n",
    "                                stepid_replay.append(si)\n",
    "                        # Make the torch.Tensor vote vector\n",
    "                        vote = torch.Tensor(vote)\n",
    "                        \n",
    "                        # Update the \"vote\" vector to the environment\n",
    "                        hist = self.env.update_vote(vote)\n",
    "                        \n",
    "#                         print(f'  Voting results {vote} -> {(vote >= 0.5).sum() > 2}')\n",
    "                        \n",
    "    \n",
    "                        ''' Success/Failure for the mission '''\n",
    "                        # check if there are more yeses than noes\n",
    "                        if (vote >= 0.5).sum() > 2:\n",
    "                            # Initialize succ vector\n",
    "                            succ = []\n",
    "                            # Loop over every agent   \n",
    "                            for i in range(5):\n",
    "                                if not miss[i]:\n",
    "                                    continue\n",
    "                                # create input vector for network\n",
    "                                succ_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "\n",
    "                                # Call the SUCCESS network of the current agent and return succ_i\n",
    "                                if every[i] == BLUE_TEAM_ID:\n",
    "                                    succ_i = torch.Tensor([1.0])\n",
    "                                else:\n",
    "                                    succ_dist, _ = self.red.SUCC(succ_in)\n",
    "                                    succ_i = succ_dist.sample()\n",
    "            \n",
    "                                # Append the voting results to \"vote\"\n",
    "                                succ.append(succ_i)\n",
    "                                \n",
    "                                # If we are currently training on the SUCCESS network, save it to experience buffer\n",
    "                                if is_training and train_model == 'succ_red' and every[i] == RED_TEAM_ID:\n",
    "                                    obs_replay.append(succ_in)\n",
    "                                    actions_replay.append(succ_i)\n",
    "                                    log_probs.append(succ_dist)\n",
    "                                    stepid_replay.append(si)\n",
    "                            \n",
    "                            # Make the torch.Tensor succ vector\n",
    "                            succ = torch.Tensor(succ)\n",
    "\n",
    "                            # Update the \"succ\" vector to the environment\n",
    "                            self.env.update_succ(succ)\n",
    "                            \n",
    "#                         print(f'  Mission: {hist[0,self.env.si-1,15:18]} - {hist[0,self.env.si-1,18]} Fails {hist[0,self.env.si-1,19]} Round')\n",
    "                        \n",
    "                        hist, every, who, li, si, mi, nm, done = self.env.get_observation()\n",
    "                        \n",
    "                    # append the result of each episode into the epoch buffer\n",
    "                    winning_team.append(self.env.winning_team)\n",
    "                    history_log.append(hist)\n",
    "                    if is_training:\n",
    "                        obs_replay_epoch.append(obs_replay)\n",
    "                        actions_replay_epoch.append(actions_replay)\n",
    "                        log_probs_epoch.append(log_probs)\n",
    "                        stepid_replay_epoch.append(stepid_replay)\n",
    "                        last_step = si\n",
    "                \n",
    "                if is_training:\n",
    "                    obs_all = []\n",
    "                    actions_all = []\n",
    "                    log_probs_all = []\n",
    "                    stepid_all = []\n",
    "                    discounted_rewards = []\n",
    "                    for epi in range(self.train_episodes):\n",
    "                        if len(stepid_replay_epoch[epi]) > 0:\n",
    "                            obs_all += obs_replay_epoch[epi]\n",
    "                            actions_all += actions_replay_epoch[epi]\n",
    "                            log_probs_all += log_probs_epoch[epi]\n",
    "                            stepid_all += stepid_replay_epoch[epi]\n",
    "                            discounted_rewards += compute_discounted_rewards(stepid_replay_epoch[epi], last_step, winning_team[epi], train_model, self.gamma)   \n",
    "                    value_function = (lambda x : 0)\n",
    "                    sb = preprocess(obs_all, actions_all, stepid_all, discounted_rewards, value_function)\n",
    "\n",
    "                    model_lookup = {\"comm_red\": (red.COMM, red.COMM_opt),\n",
    "                        \"miss_red_2\" : (red.MISS, red.MISS_opt2),\n",
    "                        \"miss_red_3\" : (red.MISS, red.MISS_opt3),\n",
    "                        \"vote_red\" : (red.VOTE, red.VOTE_opt), \n",
    "                        \"succ_red\" : (red.SUCC, red.SUCC_opt),\n",
    "                        \"who_blue\" : (blue.WHO, blue.WHO_opt),\n",
    "                        \"comm_blue\" : (blue.COMM, blue.COMM_opt),\n",
    "                        \"miss_blue_2\" : (blue.MISS, blue.MISS_opt2),\n",
    "                        \"miss_blue_3\" : (blue.MISS, blue.MISS_opt3),\n",
    "                        \"vote_blue\" : (blue.VOTE, blue.VOTE_opt)}\n",
    "\n",
    "                    acmodel, optimizer = model_lookup[train_model]\n",
    "\n",
    "                    if train_model != \"who_blue\":\n",
    "                        update_parameters_ppo(optimizer, acmodel, sb, args, train_model)\n",
    "                    else:\n",
    "                        optimizer.zero_grad()\n",
    "                        loss = 0\n",
    "                        for epi in range(self.train_episodes):\n",
    "                            loss += ((acmodel(obs_all[epi])[0] - every_replay[epi]) ** 2).sum()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # end of self.train_episodes episodes\n",
    "                # # obs_replay, actions_replay, stepid_replay, winning_team\n",
    "                # if len(stepid_replay) > 0:\n",
    "                #     discounted_rewards = compute_discounted_rewards(stepid_replay, winning_team, train_model, self.gamma)\n",
    "                #     advantage_gae = compute_advantage_gae([0.0] * len(stepid_replay), discounted_rewards, stepid_replay, self.gae_lambda, self.gamma, train_model)\n",
    "                # sb['obs', 'action', 'advantage_gae' or 'advantage', 'discounted_reward']\n",
    "                \n",
    "                # end of epoch\n",
    "                winning_log.extend(winning_team)\n",
    "                every_log.extend(every_replay)\n",
    "        \n",
    "        return history_log, winning_log, every_log\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = getDefaultParams()\n",
    "def compute_discounted_rewards(stepid_replay, last_step, winning_team, train_model, gamma):\n",
    "\n",
    "    # last_step = 10\n",
    "    # stepid_replay = [0, 0, 1, 3, 5, 5, 6, 6, 6, 8]\n",
    "\n",
    "    # Calculate the discounted rewards\n",
    "    win_team_color = 'red' if winning_team == RED_TEAM_ID else 'blue'\n",
    "    R = 1 if win_team_color in train_model else -1\n",
    "    discounted_rewards = [R * (gamma ** (last_step - 1 - stepid_replay[-1]))]\n",
    "    curr_step = stepid_replay[-1]\n",
    "\n",
    "    for si in range(len(stepid_replay) - 2, -1, -1):\n",
    "        discounted_rewards += [discounted_rewards[-1] * (gamma ** (curr_step - stepid_replay[si]))]\n",
    "        curr_step = stepid_replay[si]\n",
    "\n",
    "    # stepN = len(stepid_replay)\n",
    "    # win_team_color = 'red' if winning_team[-1] == RED_TEAM_ID else 'blue'\n",
    "    # discounted_rewards = [1 if win_team_color in train_model else -1]\n",
    "\n",
    "    # for i in range(stepN - 2, -1, -1):\n",
    "    #     if stepid_replay[i] == stepid_replay[i+1]:\n",
    "    #         # the current step of the i-th index is the same as the step of the i+1-th index\n",
    "    #         discounted_rewards.append(discounted_rewards[-1])\n",
    "    #     elif stepid_replay[i] != 0 and stepid_replay[i+1] == 0:\n",
    "    #         # the current step of the i-th index is the last step of a new episode\n",
    "    #         win_team_color = 'red' if winning_team[i] == RED_TEAM_ID else 'blue'\n",
    "    #         discounted_rewards.append(1 if win_team_color in train_model else -1)\n",
    "    #     else:\n",
    "    #         # the current step of the i-th index is the step of the i+1-th index minus 1\n",
    "    #         discounted_rewards.append(discounted_rewards[-1] * gamma)\n",
    "\n",
    "    discounted_rewards.reverse()\n",
    "    return discounted_rewards\n",
    "\n",
    "\n",
    "def preprocess(obs_replay, actions_replay, stepid_replay, discounted_rewards, value_function):\n",
    "    # Example:\n",
    "    # preprocess(*full_replay_buffer, range(10), (lambda x: 0))\n",
    "\n",
    "    # print(len(obs_replay))\n",
    "    # print(len(stepid_replay))\n",
    "\n",
    "    assert len(obs_replay) == len(actions_replay)\n",
    "    assert len(obs_replay) == len(stepid_replay)\n",
    "    assert len(discounted_rewards) == len(obs_replay)\n",
    "    discounted_rewards = torch.Tensor(discounted_rewards)\n",
    "    # print(obs_replay)\n",
    "    obs_replay = torch.stack(obs_replay)\n",
    "    actions_replay = torch.stack(actions_replay)\n",
    "    # obs_replay = torch.Tensor(obs_replay)\n",
    "    sb = {\n",
    "        \"obs\" : obs_replay,\n",
    "        \"action\" : actions_replay,\n",
    "        \"advantage\" : discounted_rewards - value_function(obs_replay),\n",
    "        \"discounted_reward\" : discounted_rewards,\n",
    "    }\n",
    "    return sb\n",
    "\n",
    "\n",
    "\n",
    "def update_parameters_ppo(optimizer, acmodel, sb, args, train_model):\n",
    "    def _compute_policy_loss_ppo(logp, old_logp, entropy, advantages):\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        ### TODO: implement PPO policy loss computation (30 pts).  #######\n",
    "        logr = logp - old_logp\n",
    "        ratios = torch.exp(logr)\n",
    "        \n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1-args['clip_ratio'], 1+args['clip_ratio']) * advantages\n",
    "        \n",
    "        policy_loss = (-torch.min(surr1, surr2) -args['entropy_coef']*entropy).mean()\n",
    "        \n",
    "        # approx_kl = torch.sum(torch.exp(logp) * logr)\n",
    "        #approx_kl = torch.nn.functional.kl_div(logp, old_logp)\n",
    "\n",
    "        # approx_kl = ((logr.exp() - 1) - logr).sum()\n",
    "        approx_kl = ((logr ** 2) / 2).sum()\n",
    "        \n",
    "        ##################################################################\n",
    "        \n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(values, returns):\n",
    "        ### TODO: implement PPO value loss computation (10 pts) ##########\n",
    "        value_loss = F.mse_loss(values.squeeze(-1), returns).mean() #(values - returns).pow(2).mean()\n",
    "        ##################################################################\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    result = acmodel(sb['obs'])\n",
    "    if 'miss' in train_model:\n",
    "        if \"2\" in train_model:\n",
    "            result = result[0]\n",
    "        else:\n",
    "            result = result[1]\n",
    "\n",
    "    dist, (_, values) = result\n",
    "    \n",
    "    old_logp = dist.log_prob(sb['action']).detach()\n",
    "    logp = dist.log_prob(sb['action'])\n",
    "    dist_entropy = dist.entropy()\n",
    "    \n",
    "    advantage = sb['advantage_gae'] if args['use_gae'] else sb['advantage']\n",
    "    \n",
    "    policy_loss, _ = _compute_policy_loss_ppo(logp, old_logp, dist_entropy, advantage)\n",
    "\n",
    "    if not args[\"use_critic\"]:\n",
    "        values = torch.zeros_like(sb['discounted_reward'])\n",
    "\n",
    "    value_loss = _compute_value_loss(values, sb['discounted_reward'])\n",
    "\n",
    "    for i in range(args['train_ac_iters']):\n",
    "        result = acmodel(sb['obs'])\n",
    "        if 'miss' in train_model:\n",
    "            if \"2\" in train_model:\n",
    "                result = result[0]\n",
    "            else:\n",
    "                result = result[1]\n",
    "        dists, (_, values) = result\n",
    "            \n",
    "        logp = dists.log_prob(sb['action'])\n",
    "        dist_entropy = dists.entropy()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_pi, approx_kl = _compute_policy_loss_ppo(logp, old_logp, dist_entropy, advantage)\n",
    "\n",
    "        if not args[\"use_critic\"]:\n",
    "            values = torch.zeros_like(sb['discounted_reward'])\n",
    "            \n",
    "        loss_v = _compute_value_loss(values, sb['discounted_reward'])\n",
    "\n",
    "        loss = loss_v + loss_pi\n",
    "        if approx_kl > 1.5 * args['target_kl']:\n",
    "            break\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    \n",
    "    update_policy_loss = policy_loss.item()\n",
    "    update_value_loss = value_loss.item()\n",
    "\n",
    "    logs = {\n",
    "        \"policy_loss\": update_policy_loss,\n",
    "        \"value_loss\": update_value_loss,\n",
    "    }\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7a5c2d512647fa8a2ad36e2fdc67be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa706e23bf34605b1474a478f254f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m my_env \u001b[38;5;241m=\u001b[39m AvalonEnv()\n\u001b[1;32m     15\u001b[0m engine \u001b[38;5;241m=\u001b[39m AvalonEngine(env\u001b[38;5;241m=\u001b[39mmy_env, blue\u001b[38;5;241m=\u001b[39mblue, red\u001b[38;5;241m=\u001b[39mred, train_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, max_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m     16\u001b[0m                       trainable_models\u001b[38;5;241m=\u001b[39mtrainable_models, gamma\u001b[38;5;241m=\u001b[39mgamma, gae_lambda\u001b[38;5;241m=\u001b[39mgae_lambda)\n\u001b[0;32m---> 17\u001b[0m hist_log, winning_log, every_log \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m evaluate(hist_log, winning_log, every_log)\n",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36mAvalonEngine.run\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    125\u001b[0m                                 stepid_replay\u001b[38;5;241m.\u001b[39mappend(si)\n\u001b[1;32m    126\u001b[0m                         \u001b[38;5;66;03m# Update the \"miss\" vector to the environment\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m                         miss \u001b[38;5;241m=\u001b[39m \u001b[43mmiss_players\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmiss\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m nm\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mmiss_players(miss)\n\u001b[1;32m    128\u001b[0m                         hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mupdate_miss(miss)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#                         print(f'  Leader {li} decides {miss} go on mission')\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Classes/6.484/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mmiss_players\u001b[0;34m(miss)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m miss\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     44\u001b[0m     miss_cat[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     miss_cat[miss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m miss\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m7\u001b[39m:\n\u001b[1;32m     47\u001b[0m     miss_cat[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "# trainable_models=['comm_red', 'comm_blue', 'who_blue', 'miss_red_2', 'miss_red_3', 'miss_blue_2', 'miss_blue_3', 'vote_red', 'vote_blue', 'succ_red']\n",
    "# trainable_models=['comm_red', 'miss_red_2', 'miss_red_3', 'vote_red', 'succ_red']\n",
    "trainable_models=['comm_blue', 'who_blue', 'miss_blue_2', 'miss_blue_3', 'vote_blue',]\n",
    "\n",
    "\n",
    "set_random_seed(1)\n",
    "\n",
    "lr = 0.00025\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "blue = BlueAgent(lr=lr, use_critic=args[\"use_critic\"]) #BlueAgent(lr=lr, use_critic=args[\"use_critic\"]) #BlueAgentBaseline()\n",
    "red = RedAgentBaseline()#RedAgent(lr=lr, use_critic=args[\"use_critic\"]) #RedAgentBaseline(\n",
    "\n",
    "my_env = AvalonEnv()\n",
    "engine = AvalonEngine(env=my_env, blue=blue, red=red, train_episodes=20, max_epoch=40,\n",
    "                      trainable_models=trainable_models, gamma=gamma, gae_lambda=gae_lambda)\n",
    "hist_log, winning_log, every_log = engine.run()\n",
    "evaluate(hist_log, winning_log, every_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(hist_log, winning_log, every_log):\n",
    "    plt.plot(winning_log)\n",
    "    plt.title(\"Success Rate over time\")\n",
    "    plt.show()\n",
    "    plt.plot(scipy.ndimage.gaussian_filter1d(np.array(winning_log)*2-1, sigma=0.25))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "evaluate(hist_log, winning_log, every_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title\n",
    "# def compute_advantage_gae(values, discounted_rewards, stepid_replay, gae_lambda, gamma, train_model):\n",
    "#     rewards = [x if abs(x) == 1 else 0 for x in discounted_rewards]\n",
    "#     # print(rewards)\n",
    "#     # print(discounted_rewards)\n",
    "\n",
    "#     reps = 0\n",
    "#     if stepid_replay[0] != 0:\n",
    "#         print(train_model)\n",
    "#         print(stepid_replay)\n",
    "#     while stepid_replay[reps] == 0:\n",
    "#         reps += 1\n",
    "\n",
    "#     rewards_single = torch.Tensor(rewards[::reps])\n",
    "#     values_single = torch.Tensor(values[::reps])\n",
    "\n",
    "#     end_of_epi = np.arange(len(rewards_single))[rewards_single != 0].astype(int)\n",
    "    \n",
    "#     # print(end_of_epi)\n",
    "#     # print(values_single)\n",
    "\n",
    "#     last_end = -1\n",
    "#     total_advs = []\n",
    "#     for end in end_of_epi:\n",
    "#         # calculate advantage_gae from index last_end+1 ~ end\\\n",
    "#         theta = rewards_single[last_end+1:end+1] - values_single[last_end+1:end+1]\n",
    "#         theta[:-1] += gamma * values_single[last_end+2:end+1]\n",
    "#         gl = gamma * gae_lambda\n",
    "#         advantages = torch.tensor([\n",
    "#             (theta[i:] * (gl ** torch.arange(end - last_end -i))).sum() for i in range(end - last_end)\n",
    "#         ])\n",
    "\n",
    "#         # print(f'last_end: {last_end} ~ end: {end} => {len(advantages)}')\n",
    "#         # print(advantages)\n",
    "#         total_advs.append(advantages)\n",
    "#         last_end = end\n",
    "#     ############################################################################\n",
    "    \n",
    "#     total_advs = torch.concat(total_advs)\n",
    "#     torch.repeat_interleave(total_advs, 2)\n",
    "\n",
    "#     # print(total_advs)\n",
    "#     # print(torch.repeat_interleave(total_advs, 2))\n",
    "#     # assert 0\n",
    "\n",
    "#     return total_advs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
