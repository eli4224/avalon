{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import easyrl.models.diag_gaussian_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, final_activation=None):\n",
    "        self.final_activation = final_activation\n",
    "        super().__init__()\n",
    "        #### A simple network that takes\n",
    "        #### as input the history, and outputs the \n",
    "        #### distribution parameters.\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU())\n",
    "        self.out_layer = nn.Sequential(nn.Linear(64, action_dim))\n",
    "\n",
    "    def forward(self, ob):\n",
    "        mid_logits = self.fcs(ob)\n",
    "        logits = self.out_layer(mid_logits)\n",
    "        if self.final_activation is not None:\n",
    "            logits = self.final_activation(logits)\n",
    "        return logits\n",
    "\n",
    "# class MishNet(NNetwork):\n",
    "#     def __init__(*args, mission_shapes, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.final_layers = [nn.Sequential(nn.Linear(64, s)) for s in mission_shapes]\n",
    "        \n",
    "#     def forward(self, ob):\n",
    "#         mid_logits = self.fcs(ob)\n",
    "#         logits = [fl(mid_logits) for fl in self.final_layers]\n",
    "#         if self.final_activation is not None:\n",
    "#             logits = [self.final_activation(logit) for logit in logits]\n",
    "#         return logits\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PLAYERS = 5\n",
    "RED_PLAYERS = 2\n",
    "BLUE_PLAYERS = 3\n",
    "HIST_SHAPE = 2 * 25 * (3 * NUM_PLAYERS + 5)\n",
    "SELF_SHAPE = 5\n",
    "COMM_SHAPE = 32  # Change freely\n",
    "WHO_SHAPE = NUM_PLAYERS\n",
    "VOTE_SHAPE = NUM_PLAYERS\n",
    "\n",
    "assert RED_PLAYERS + BLUE_PLAYERS == NUM_PLAYERS\n",
    "\n",
    "def get_who():\n",
    "    return NNetwork(SELF_SHAPE + HIST_SHAPE + COMM_SHAPE, WHO_SHAPE, nn.softmax)\n",
    "\n",
    "def get_comm():\n",
    "    return easyrl.models.diag_gauss_dist(NNetwork(SELF_SHAPE + HIST_SHAPE + WHO_SHAPE, 64), COMM_SHAPE)\n",
    "\n",
    "def get_miss(mission_shapes = (10,10)):\n",
    "    model = NNetwork(SELF_SHAPE + WHO_SHAPE + HIST_SHAPE, 64, nn.softmax)\n",
    "    return [torch.distributions.Categorical(model, mish) for mish in mission_shapes]\n",
    "\n",
    "def get_vote():\n",
    "    return torch.distributions.bernoulli.Bernoulli(NNetwork(HIST_SHAPE + COMM_SHAPE + WHO_SHAPE, 1, nn.softmax))\n",
    "\n",
    "def get_succ():\n",
    "    return torch.distributions.bernoulli.Bernoulli(NNetwork(HIST_SHAPE + COMM_SHAPE + WHO_SHAPE, 1, nn.softmax))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def comm():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def who():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def miss():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def vote():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def succ():\n",
    "        pass\n",
    "\n",
    "class RedAgent(Agent):\n",
    "    \n",
    "    def __init__(every):\n",
    "        self.comm = get_comm()\n",
    "        self.who = (lambda *args : every)\n",
    "        mission_models = get_miss()\n",
    "        self.miss = (lambda *args : [f(x) for f, x in zip(mission_models, args)])\n",
    "        self.vote = get_vote()\n",
    "        self.succ = get_succ()\n",
    "\n",
    "class BlueAgent(Agent):\n",
    "    \n",
    "    def __init__():\n",
    "        self.comm = get_comm()\n",
    "        self.who = get_who()\n",
    "        mission_models = get_miss()\n",
    "        self.miss = (lambda *args : [f(x) for f, x in zip(mission_models, args)])\n",
    "        self.vote = get_vote()\n",
    "        self.succ = (lambda *args : torch.distributions.bernoulli.Bernoulli(1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1177925979.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [11]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def get_who:\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NUM_PLAYERS = 5\n",
    "RED_PLAYERS = 2\n",
    "BLUE_PLAYERS = 3\n",
    "HIST_SHAPE = 2 * 25 * (3 * NUM_PLAYERS + 5)\n",
    "COMM_SHAPE = 32  # Change freely\n",
    "WHO_SHAPE = NUM_PLAYERS\n",
    "VOTE_SHAPE = NUM_PLAYERS\n",
    "\n",
    "assert RED_PLAYERS + BLUE_PLAYERS == NUM_PLAYERS\n",
    "\n",
    "def get_who:\n",
    "    return NNetwork(HIST_SHAPE + COMM_SHAPE, WHO_SHAPE, nn.softmax)\n",
    "\n",
    "def get_comm:\n",
    "    return easyrl.models.diag_gauss_dist(nn.Module(HIST_SHAPE, 64), COMM_SHAPE)\n",
    "\n",
    "def get_miss -> nn.Module\n",
    "\n",
    "def get_vote:\n",
    "    return easyrl.models.DiscretePolicy(NNetwork(HIST_SHAPE + COMM_SHAPE + WHO_SHAPE, 64, nn.softmax), 1)\n",
    "\n",
    "def get_succ:\n",
    "    return easyrl.models.DiscretePolicy(NNetwork(HIST_SHAPE + COMM_SHAPE + WHO_SHAPE + VOTE_SHAPE, 1, nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim,final_activation=None):\n",
    "        self.final_activation = final_activation\n",
    "        super().__init__()\n",
    "        #### A simple network that takes\n",
    "        #### as input the history, and outputs the \n",
    "        #### distribution parameters.\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, ob):\n",
    "        logits = self.fcs(ob)\n",
    "        if self.final_activation is not None:\n",
    "            logits = self.final_activation(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AvalonEnv():\n",
    "    def __init__(self):\n",
    "        self.nm = [2, 3, 2, 3, 3]\n",
    "        self.tasks = ['comm', 'who', 'miss', 'vote', 'succ']\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.mi = 0\n",
    "        self.ri = 0\n",
    "        self.si = 0\n",
    "        self.li = 0\n",
    "        self.hist = torch.zeros((2, 25, 20))\n",
    "        self.every = torch.Tensor(np.random.shuffle([0, 0, 0, 1, 1]))\n",
    "        self.who = torch.rand((5, 5))\n",
    "        self.comm = torch.zeros((5, COMM_SHAPE))\n",
    "        self.miss = torch.zeros(5)\n",
    "        self.task = 'comm'\n",
    "        self.done = False\n",
    "        self.winning_team = None\n",
    "        \n",
    "        # the initial observation of \"hist\" is all zeros\n",
    "        # EXCEPT a one at the leader id (self.li) location of the zero-th step\n",
    "        self.hist[0, 0, self.li] = 1\n",
    "        self.hist[1, 0, :5] = 1\n",
    "        \n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self): \n",
    "        return self.task, self.hist, self.every, self.who, self.mi, self.ri, self.li, self.done\n",
    "    \n",
    "    def get_comm(self):\n",
    "        return self.comm\n",
    "    \n",
    "    def get_miss(self):\n",
    "        return self.miss\n",
    "    \n",
    "    def step_comm(self, comm):\n",
    "        self.comm = torch.Tensor(comm)\n",
    "        # save the trajectory\n",
    "        \n",
    "        # assign the next task\n",
    "        self.task = 'who'\n",
    "    \n",
    "    def step_who(self, pi, who_v):\n",
    "        self.who[pi] = who_v\n",
    "        # save the trajectory\n",
    "        \n",
    "        # assign the next task\n",
    "        self.task = 'miss'\n",
    "    \n",
    "    def step_miss(self, miss);\n",
    "        self.miss = miss\n",
    "        # save the trajectory\n",
    "        \n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 5:10] = miss\n",
    "        self.hist[1, self.si, 5:10] = 1\n",
    "        # assign the next task\n",
    "        self.task = 'vote'\n",
    "    \n",
    "    def step_vote(self, vote);\n",
    "        self.vote = vote\n",
    "        # save the trajectory\n",
    "        \n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 10:15] = vote\n",
    "        self.hist[1, self.si, 10:15] = 1\n",
    "        # check if there are more yeses than noes\n",
    "        if (vote == 1).sum() > 2:\n",
    "            # set relevance of only the no mission flag\n",
    "            self.hist[1, self.si, 15] = 1\n",
    "            \n",
    "            # assign the next task\n",
    "            self.task = 'succ'\n",
    "        else:\n",
    "            # set the no mission flag\n",
    "            self.hist[0, self.si, 15] = 1\n",
    "\n",
    "            # set the current round\n",
    "            self.hist[0, self.si, 18] = self.ri\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 19] = self.hist[0, self.si-1, 19] if self.si else 0\n",
    "            \n",
    "            # set relevance\n",
    "            self.hist[1, self.si, 15:] = 1\n",
    "            \n",
    "            if self.ri == 4:\n",
    "                # game is over, red team wins\n",
    "                self.done = True\n",
    "            else:\n",
    "                # update step id\n",
    "                self.si += 1\n",
    "                \n",
    "                # update round id\n",
    "                self.ri += 1\n",
    "                \n",
    "                # update leader\n",
    "                self.li = (self.li + 1) % 5\n",
    "                \n",
    "                # assign the next task\n",
    "                self.task = 'comm'\n",
    "        \n",
    "    def step_succ(self, succ):\n",
    "        # set the current round\n",
    "        self.hist[0, self.si, 18] = self.ri\n",
    "        \n",
    "        # set relevance\n",
    "        self.hist[1, self.si, 16:] = 1\n",
    "        \n",
    "        if 0 in succ:\n",
    "            # set the mission failure flag\n",
    "            self.hist[0, self.si, 17] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 19] = self.hist[0, self.si-1, 19] + 1 if self.si else 1\n",
    "        else:\n",
    "            # set the mission success flag\n",
    "            self.hist[0, self.si, 16] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 19] = self.hist[0, self.si-1, 19] if self.si else 0\n",
    "        \n",
    "        # check if game is over\n",
    "        if self.hist[0, self.si, 18]  == 3:\n",
    "            # game is over, red team wins\n",
    "            self.winning_team = 1\n",
    "            self.done = True\n",
    "        elif self.mi == 2 + self.hist[0, self.si, 18]:\n",
    "            # game is over, blue team wins\n",
    "            self.winning_team = 0\n",
    "            self.done = True\n",
    "        else:\n",
    "            # assign the next task\n",
    "            self.task = 'comm'\n",
    "            \n",
    "            # update mission id\n",
    "            self.mi += 1\n",
    "\n",
    "            # update round id\n",
    "            self.ri = 0\n",
    "\n",
    "            # update step id\n",
    "            self.si += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AvalonEngine:\n",
    "    max_steps: int\n",
    "    env: AvalonEnv\n",
    "    train_episodes: int\n",
    "    max_epoch: int\n",
    "    agents: Any\n",
    "    \n",
    "    def run():\n",
    "        for epoch in tqdm.tqdm(range(self.max_epoch)):\n",
    "            for model in self.TRAINABLE_MODELS:\n",
    "                for episode in range(self.train_episodes):\n",
    "                    task, hist, every, who, mi, ri, li, done = env.reset()\n",
    "                    \n",
    "                    while not done:\n",
    "                        # Useful for constructing the self vector (self_v)\n",
    "                        self_m = torch.eye(5)\n",
    "                        \n",
    "                        if task == 'comm': # Tell every agent to say something (build the \"comm\" matrix)     \n",
    "                            # Initialize communication matrix\n",
    "                            comm = []\n",
    "                            # Loop over every agent\n",
    "                            for i in range(5):\n",
    "                                # create the self vector (self_v)\n",
    "                                self_v = self_m[i]\n",
    "                                # Call the COMM network of the current agent\n",
    "                                # and return the communication vector (comm_v)\n",
    "                                comm_v = agents[i].COMM((\n",
    "                                    self_v,\n",
    "                                    every if every[i] else who[i],\n",
    "                                    hist\n",
    "                                ))\n",
    "                                # Append it to the communication matrix (comm)\n",
    "                                comm.append(comm_v)\n",
    "                            # take a steps by updating communication matrix to the environment\n",
    "                            env.step_comm(comm)\n",
    "                        elif task == 'who':\n",
    "                            # get the communication matrix from env\n",
    "                            comm = env.get_comm()\n",
    "                            # Loop over every agent\n",
    "                            for i in range(5):\n",
    "                                # continue if the agent is on the red team\n",
    "                                if every[i]:\n",
    "                                    continue\n",
    "                                # create the self vector (self_v)\n",
    "                                self_v = self_m[i]\n",
    "                                # Call the WHO network of the current agent (on the blue team)\n",
    "                                # and return the who vector (who)\n",
    "                                who = agents[i].WHO((\n",
    "                                    self_v,\n",
    "                                    comm,\n",
    "                                    hist\n",
    "                                ))\n",
    "                                # take a step by updating the \"who\" vector to the environment\n",
    "                                env.step_who(i, who)\n",
    "                        elif task == 'miss':\n",
    "                            # create the self vector (self_v) ACCORDING to the leader (li)\n",
    "                            self_v = self_m[li]\n",
    "                            # Only call the leader\n",
    "                            miss = agents[li].MISSION((\n",
    "                                self_v,\n",
    "                                every if every[li] else who[li],\n",
    "                                hist\n",
    "                            ))\n",
    "                            # take a step by updating the \"miss\" vector to the environment\n",
    "                            env.step_miss(miss)\n",
    "                        elif task == 'vote':\n",
    "                            # Initialize vote vector\n",
    "                            vote = []\n",
    "                            # Loop over every agent   \n",
    "                            for i in range(5):\n",
    "                                # create the self vector (self_v)\n",
    "                                self_v = self_m[i]\n",
    "                                # Call the VOTE network of the current agent\n",
    "                                # and return vote_pi\n",
    "                                vote_i = agents[i].VOTE((\n",
    "                                    self_v,\n",
    "                                    every if every[i] else who[i],\n",
    "                                    hist\n",
    "                                ))\n",
    "                                # Append the voting results to \"vote\"\n",
    "                                vote.append(vote_i)\n",
    "                            # take a step by updating the \"vote\" vector to the environment\n",
    "                            env.step_vote(vote)\n",
    "                        else:\n",
    "                            # get the miss vector\n",
    "                            miss = env.get_miss()\n",
    "                            # Initialize succ vector\n",
    "                            succ = []\n",
    "                            # Loop over every agent   \n",
    "                            for i in range(5):\n",
    "                                if not miss[i]:\n",
    "                                    continue\n",
    "                                # create the self vector (self_v)\n",
    "                                self_v = self_m[i]\n",
    "                                # Call the SUCCESS network of the current agent\n",
    "                                # and return succ_i\n",
    "                                succ_i = agents[i].SUCCESS((\n",
    "                                    self_v,\n",
    "                                    every if every[i] else who[i],\n",
    "                                    hist\n",
    "                                ))\n",
    "                                # Append the voting results to \"vote\"\n",
    "                                succ.append(succ_i)\n",
    "                            # take a step by updating the \"vote\" vector to the environment\n",
    "                            env.step_succ(succ)\n",
    "                        \n",
    "                        task, hist, every, who, mi, ri, li, done = env.get_observation()\n",
    "                    \n",
    "                    # check who won\n",
    "                    \n",
    "    \n",
    "    def main_loop():\n",
    "    \n",
    "        for t in tqdm(range(self.max_steps), desc='Step'):\n",
    "            \n",
    "            mi, ri, 'miss' = env.query()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
