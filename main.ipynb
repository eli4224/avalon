{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import easyrl.models.diag_gaussian_policy as DiagGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stolen from easyrl Categorical policy policy with some modifications\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self,\n",
    "                 body_net,\n",
    "                 action_dim,\n",
    "                 in_features=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.body = body_net\n",
    "        if in_features is None:\n",
    "            for i in reversed(range(len(self.body.fcs))):\n",
    "                layer = self.body.fcs[i]\n",
    "                if hasattr(layer, 'out_features'):\n",
    "                    in_features = layer.out_features\n",
    "                    break\n",
    "\n",
    "        self.head = nn.Sequential(nn.Linear(in_features, action_dim), nn.Softmax())\n",
    "\n",
    "    def forward(self, x=None, body_x=None, **kwargs):\n",
    "        if x is None and body_x is None:\n",
    "            raise ValueError('One of [x, body_x] should be provided!')\n",
    "        if body_x is None:\n",
    "            body_x = self.body(x, **kwargs)\n",
    "        if isinstance(body_x, tuple):\n",
    "            pi = self.head(body_x[0])\n",
    "        else:\n",
    "            pi = self.head(body_x)\n",
    "        action_dist = Categorical(probs=pi)\n",
    "        return action_dist, body_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# set random seed\n",
    "seed = 0\n",
    "set_random_seed(seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, final_activation=None):\n",
    "        self.final_activation = final_activation\n",
    "        super().__init__()\n",
    "        #### A simple network that takes\n",
    "        #### as input the history, and outputs the \n",
    "        #### distribution parameters.\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU())\n",
    "        self.out_layer = nn.Sequential(nn.Linear(64, action_dim))\n",
    "\n",
    "    def forward(self, ob):\n",
    "        mid_logits = self.fcs(ob)\n",
    "        logits = self.out_layer(mid_logits)\n",
    "        if self.final_activation is not None:\n",
    "            logits = self.final_activation(logits, dim=-1)\n",
    "        return logits\n",
    "\n",
    "# class MishNet(NNetwork):\n",
    "#     def __init__(*args, mission_shapes, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.final_layers = [nn.Sequential(nn.Linear(64, s)) for s in mission_shapes]\n",
    "        \n",
    "#     def forward(self, ob):\n",
    "#         mid_logits = self.fcs(ob)\n",
    "#         logits = [fl(mid_logits) for fl in self.final_layers]\n",
    "#         if self.final_activation is not None:\n",
    "#             logits = [self.final_activation(logit) for logit in logits]\n",
    "#         return logits\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED_TEAM_ID = 1\n",
    "BLUE_TEAM_ID = 0\n",
    "NUM_PLAYERS = 5\n",
    "RED_PLAYERS = 2\n",
    "BLUE_PLAYERS = 3\n",
    "HIST_SHAPE = 2 * 25 * (3 * NUM_PLAYERS + 5)\n",
    "SELF_SHAPE = torch.tensor([5])\n",
    "COMM_SHAPE = 32  # Change freely\n",
    "WHO_SHAPE = torch.tensor([NUM_PLAYERS])\n",
    "VOTE_SHAPE =torch.tensor([NUM_PLAYERS])\n",
    "\n",
    "assert RED_PLAYERS + BLUE_PLAYERS == NUM_PLAYERS\n",
    "\n",
    "def get_who():\n",
    "    return NNetwork(SELF_SHAPE + HIST_SHAPE + NUM_PLAYERS*COMM_SHAPE, WHO_SHAPE, nn.functional.softmax)\n",
    "\n",
    "def get_comm():\n",
    "    return DiagGaussian.DiagGaussianPolicy(NNetwork(SELF_SHAPE + HIST_SHAPE + WHO_SHAPE, torch.tensor([64])), COMM_SHAPE, in_features=torch.tensor([64]))\n",
    "\n",
    "def get_miss(mission_shapes = (10,10)):\n",
    "    model = NNetwork(SELF_SHAPE + WHO_SHAPE + HIST_SHAPE, 64)\n",
    "    return [CategoricalPolicy(model, mish, 64) for mish in mission_shapes]\n",
    "\n",
    "def get_vote():\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + SELF_SHAPE + WHO_SHAPE, 128), 2, 128)\n",
    "\n",
    "def get_succ():\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + SELF_SHAPE + WHO_SHAPE, 128), 2, 128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def comm():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def who():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def miss():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def vote():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def succ():\n",
    "        pass\n",
    "\n",
    "class RedAgent():\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "        self.COMM = get_comm()\n",
    "#         self.who = (lambda *args : every)\n",
    "        mission_models = get_miss()\n",
    "        self.MISS = (lambda args : [mission_models[i](args) for i in range(2)])\n",
    "        self.VOTE = get_vote()\n",
    "        self.SUCC = get_succ()\n",
    "        \n",
    "        # Optimizers\n",
    "        self.COMM_opt = torch.optim.Adam(self.COMM.parameters(), lr=lr)\n",
    "        self.MISS_opt2 = torch.optim.Adam(mission_models[0].parameters(), lr=lr)\n",
    "        self.MISS_opt3 = torch.optim.Adam(mission_models[1].parameters(), lr=lr)\n",
    "        self.VOTE_opt = torch.optim.Adam(self.VOTE.parameters(), lr=lr)\n",
    "        self.SUCC_opt = torch.optim.Adam(self.SUCC.parameters(), lr=lr)\n",
    "\n",
    "class BlueAgent():\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "        self.COMM = get_comm()\n",
    "        self.WHO = get_who()\n",
    "        mission_models = get_miss()\n",
    "        self.MISS = (lambda args : [mission_models[i](args) for i in range(2)])\n",
    "        self.VOTE = get_vote()\n",
    "#         self.succ = (lambda *args : torch.distributions.bernoulli.Bernoulli(1))\n",
    "        \n",
    "        # Optimizers\n",
    "        self.COMM_opt = torch.optim.Adam(self.COMM.parameters(), lr=lr)\n",
    "        self.WHO_opt = torch.optim.Adam(self.WHO.parameters(), lr=lr)\n",
    "        self.MISS_opt2 = torch.optim.Adam(mission_models[0].parameters(), lr=lr)\n",
    "        self.MISS_opt3 = torch.optim.Adam(mission_models[1].parameters(), lr=lr)\n",
    "        self.VOTE_opt = torch.optim.Adam(self.VOTE.parameters(), lr=lr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AvalonEnv():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nm = [2, 3, 2, 3, 3, 0]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.mi = 0\n",
    "        self.ri = 0\n",
    "        self.si = 0\n",
    "        self.li = 0\n",
    "        self.hist = torch.zeros((2, 25, 20))\n",
    "        every = [0, 0, 0, 1, 1]\n",
    "        random.shuffle(every)\n",
    "        self.every = torch.Tensor(every.copy())        \n",
    "        self.who = torch.normal(0.5, 0.1, (NUM_PLAYERS, NUM_PLAYERS))\n",
    "        self.done = False\n",
    "        self.winning_team = None\n",
    "        \n",
    "        # the initial observation of \"hist\" is all zeros\n",
    "        # EXCEPT a one at the leader id (self.li) location of the zero-th step\n",
    "        self.hist[0, 0, self.li] = 1\n",
    "        self.hist[1, 0, :5] = 1\n",
    "        \n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self):         \n",
    "        return self.hist, self.every, self.who, self.li, self.si, self.mi, self.nm[self.mi], self.done\n",
    "    \n",
    "    def update_who(self, who_m):\n",
    "        self.who = who_m\n",
    "        \n",
    "    def update_miss(self, miss):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 5:10] = miss.detach()\n",
    "        self.hist[1, self.si, 5:10] = 1\n",
    "        return self.hist\n",
    "    \n",
    "    def update_vote(self, vote):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 10:15] = vote\n",
    "        self.hist[1, self.si, 10:15] = 1\n",
    "        \n",
    "        # check if there are more yeses than noes\n",
    "        if (vote >= 0.5).sum() > 2:\n",
    "            # set relevance of only the no mission flag\n",
    "            self.hist[1, self.si, 15] = 1\n",
    "        else:\n",
    "            # set the no mission flag\n",
    "            self.hist[0, self.si, 15] = 1\n",
    "\n",
    "            # set the current round\n",
    "            self.hist[0, self.si, 19] = self.ri\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] if self.si else 0\n",
    "            \n",
    "            # set relevance\n",
    "            self.hist[1, self.si, 15:] = 1\n",
    "            \n",
    "            if self.ri == 4:\n",
    "                # game is over, red team wins\n",
    "                self.winning_team = RED_TEAM_ID\n",
    "                self.done = True\n",
    "                \n",
    "            # update leader\n",
    "            self.li = (self.li + 1) % 5\n",
    "            self.hist[0, self.si, self.li] = 1\n",
    "            self.hist[1, self.si, :5] = 1\n",
    "            \n",
    "            # update round id\n",
    "            self.ri = (self.ri+1) % 5\n",
    "            \n",
    "            # update step id\n",
    "            self.si += 1\n",
    "        return self.hist\n",
    "        \n",
    "    def update_succ(self, succ):\n",
    "        # set the current round\n",
    "        self.hist[0, self.si, 19] = self.ri\n",
    "        \n",
    "        # set relevance\n",
    "        self.hist[1, self.si, 16:] = 1\n",
    "        \n",
    "        if (succ < 0.5).sum():\n",
    "            # set the mission failure flag\n",
    "            self.hist[0, self.si, 17] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] + 1 if self.si else 1\n",
    "        else:\n",
    "            # set the mission success flag\n",
    "            self.hist[0, self.si, 16] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] if self.si else 0\n",
    "        \n",
    "        # check if game is over\n",
    "        if self.hist[0, self.si, 18]  == 3:\n",
    "            # game is over, red team wins\n",
    "            self.winning_team = RED_TEAM_ID\n",
    "            self.done = True\n",
    "        elif self.mi == 2 + self.hist[0, self.si, 18]:\n",
    "            # game is over, blue team wins\n",
    "            self.winning_team = BLUE_TEAM_ID\n",
    "            self.done = True\n",
    "            \n",
    "        # update mission id\n",
    "        self.mi += 1\n",
    "\n",
    "        # update round id\n",
    "        self.ri = 0\n",
    "\n",
    "        # update step id\n",
    "        self.si += 1\n",
    "\n",
    "        # update leader\n",
    "        self.li = (self.li + 1) % 5\n",
    "        self.hist[0, self.si, self.li] = 1\n",
    "        self.hist[1, self.si, :5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def miss_players(miss):\n",
    "    # 0: 0,1\n",
    "    # 1: 0,2\n",
    "    # 2: 0,3\n",
    "    # 3: 0,4\n",
    "    # 4: 1,2\n",
    "    # 5: 1,3\n",
    "    # 6: 1,4\n",
    "    # 7: 2,3\n",
    "    # 8: 2,4\n",
    "    # 9: 3,4\n",
    "    miss_cat = torch.zeros(NUM_PLAYERS)\n",
    "    if miss<4:\n",
    "        miss_cat[0] = 1\n",
    "        miss_cat[miss+1] = 1\n",
    "    elif miss<7:\n",
    "        miss_cat[1] = 1\n",
    "        miss_cat[miss-2] = 1\n",
    "    elif miss<9:\n",
    "        miss_cat[2] = 1\n",
    "        miss_cat[miss-4] = 1\n",
    "    else:\n",
    "        miss_cat[3] = 1\n",
    "        miss_cat[miss-5] = 1\n",
    "    return miss_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = getDefaultParams()\n",
    "@dataclass\n",
    "class AvalonEngine:\n",
    "    env: AvalonEnv\n",
    "    train_episodes: int\n",
    "    max_epoch: int\n",
    "    blue: BlueAgent\n",
    "    red: RedAgent\n",
    "    trainable_models: list\n",
    "    gamma: float\n",
    "    gae_lambda: float\n",
    "    \n",
    "    def run(self):\n",
    "        # Useful for constructing the self vector (self_v)\n",
    "        self_m = torch.eye(5)\n",
    "        \n",
    "        for epoch in tqdm(range(self.max_epoch), desc='epoch'):\n",
    "            for train_model in tqdm(self.trainable_models, desc='train_model'):\n",
    "                # print(f'train model = {train_model}')\n",
    "\n",
    "                # trajectory buffer\n",
    "                obs_replay_epoch = []\n",
    "                actions_replay_epoch = []\n",
    "                log_probs_epoch = []\n",
    "                stepid_replay_epoch = []\n",
    "                winning_team = []\n",
    "                every_replay = []\n",
    "\n",
    "                for episode in range(self.train_episodes):\n",
    "                    obs_replay = []\n",
    "                    actions_replay = []\n",
    "                    log_probs = []\n",
    "                    stepid_replay = []\n",
    "\n",
    "                    hist, every, who, li, si, mi, nm, done = self.env.reset()\n",
    "                    every_replay.append(every)\n",
    "\n",
    "                    while not done:\n",
    "                        # print(f'li, si, mi, ri = {li}, {si}, {mi}, {self.env.ri}')\n",
    "                        # Flow: communication -> predict who -> decide miss -> voting -> succ/fail -> next round\n",
    "                        #                                                             -> next round\n",
    "\n",
    "#                         print(f'Mission {mi} Round {self.env.ri} Step {si}')\n",
    "                        \n",
    "                        ''' Communication '''\n",
    "                        # Initialize communication matrix\n",
    "                        comm_m = []\n",
    "                        # Loop over every agent\n",
    "                        for i in range(5):\n",
    "                            # create input vector for network                            \n",
    "                            comm_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "                            \n",
    "#                             if agents == []:\n",
    "#                                 comm_v = torch.rand(COMM_SHAPE)\n",
    "#                             else:\n",
    "                            \n",
    "                            # Call the COMM network of the current agent and return the communication vector (comm_v)\n",
    "                            comm_dist, _ = self.blue.COMM(comm_in) if every[i]==BLUE_TEAM_ID else self.red.COMM(comm_in)\n",
    "                            comm_v = comm_dist.sample()\n",
    "                \n",
    "                            # Append it to the communication matrix (comm)\n",
    "                            comm_m.append(comm_v)\n",
    "                            # If we are currently training on the COMM network, save it to experience buffer\n",
    "                            if (train_model == 'comm_red' and every[i] == RED_TEAM_ID) or \\\n",
    "                                    (train_model == 'comm_blue' and every[i] == BLUE_TEAM_ID):\n",
    "                                obs_replay.append(comm_in)\n",
    "                                actions_replay.append(comm_v)\n",
    "                                log_probs.append(comm_dist.log_prob(comm_v))\n",
    "                                stepid_replay.append(si)\n",
    "                        # Make the torch.Tensor communication matrix\n",
    "                        comm_m = torch.cat(comm_m)\n",
    "                        \n",
    "#                         print(f'  All the players communicated')\n",
    "\n",
    "                        ''' Predicting Who '''\n",
    "                        # Loop over every agent\n",
    "                        for i in range(NUM_PLAYERS):\n",
    "                            # continue if the agent is on the red team\n",
    "                            if every[i] == RED_TEAM_ID:\n",
    "                                continue\n",
    "                            # create input vector for network\n",
    "                            who_in = torch.cat((self_m[i], comm_m, hist.flatten()))\n",
    "\n",
    "                            # Call the WHO network of the current agent (on the blue team)\n",
    "                            # and return the who vector (who)\n",
    "                            who_v = self.blue.WHO(who_in)\n",
    "                            \n",
    "                            # Update \"who_v\" into \"who\"\n",
    "                            who[i] = who_v\n",
    "                            # If we are currently training on the WHO network, save it to experience buffer\n",
    "                            if train_model == 'who_blue':\n",
    "                                obs_replay.append(who_in)\n",
    "                                actions_replay.append(who_v)\n",
    "                                stepid_replay.append(si)\n",
    "                        # update the who matrix to the environment\n",
    "                        self.env.update_who(who)\n",
    "                        \n",
    "#                         print(f'  All blue players predicted who')\n",
    "#                         for i in range(NUM_PLAYERS):\n",
    "#                             if every[i] == RED_TEAM_ID:\n",
    "#                                 continue\n",
    "#                             print(f'    {i} -> {who[i]}')\n",
    "\n",
    "                        ''' Deciding candidates to go on mission '''\n",
    "                        # create input vector for \"miss\" network\n",
    "                        miss_in = torch.cat((self_m[li], every if every[li] else who[li], hist.flatten()))\n",
    "\n",
    "                        # Only call the leader\n",
    "                        miss_dist = self.blue.MISS(miss_in) if every[li]==BLUE_TEAM_ID else self.red.MISS(miss_in)\n",
    "                        miss = miss_dist[nm - 2][0].sample()\n",
    "            \n",
    "                        # If we are currently training on the MISS network, save it tp experience buffer\n",
    "                        if ('miss_red' in train_model and every[i] == RED_TEAM_ID) or ('miss_blue' in train_model and every[i] == BLUE_TEAM_ID):\n",
    "                            if str(nm) in train_model:\n",
    "                                obs_replay.append(miss_in)\n",
    "                                actions_replay.append(miss)\n",
    "                                log_probs.append(miss_dist)\n",
    "                                stepid_replay.append(si)\n",
    "                        # Update the \"miss\" vector to the environment\n",
    "                        miss = miss_players(miss) if nm==2 else 1-miss_players(miss)\n",
    "                        hist = self.env.update_miss(miss)\n",
    "                        \n",
    "#                         print(f'  Leader {li} decides {miss} go on mission')\n",
    "                            \n",
    "                        ''' Voting YES/NO for the mission candidates '''\n",
    "                        # Initialize vote vector\n",
    "                        vote = []\n",
    "                        # Loop over every agent   \n",
    "                        for i in range(5):\n",
    "                            # create input vector for network\n",
    "                            vote_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "\n",
    "                            # Call the VOTE network of the current agent and return vote_pi\n",
    "                            vote_dist, _ = self.blue.VOTE(vote_in) if every[i]==BLUE_TEAM_ID else self.red.VOTE(vote_in)\n",
    "                            vote_pi = vote_dist.sample()\n",
    "            \n",
    "                            # Append the voting results to \"vote\"\n",
    "                            vote.append(vote_pi)\n",
    "                            # If we are currently training on the VOTE network, save it to experience buffer\n",
    "                            if (train_model == 'vote_red' and every[i] == RED_TEAM_ID) or \\\n",
    "                                    (train_model == 'vote_blue' and every[i] == BLUE_TEAM_ID):\n",
    "                                obs_replay.append(vote_in)\n",
    "                                actions_replay.append(vote_pi)\n",
    "                                log_probs.append(vote_dist)\n",
    "                                stepid_replay.append(si)\n",
    "                        # Make the torch.Tensor vote vector\n",
    "                        vote = torch.Tensor(vote)\n",
    "                        \n",
    "                        # Update the \"vote\" vector to the environment\n",
    "                        hist = self.env.update_vote(vote)\n",
    "                        \n",
    "#                         print(f'  Voting results {vote} -> {(vote >= 0.5).sum() > 2}')\n",
    "                        \n",
    "    \n",
    "                        ''' Success/Failure for the mission '''\n",
    "                        # check if there are more yeses than noes\n",
    "                        if (vote >= 0.5).sum() > 2:\n",
    "                            # Initialize succ vector\n",
    "                            succ = []\n",
    "                            # Loop over every agent   \n",
    "                            for i in range(5):\n",
    "                                if not miss[i]:\n",
    "                                    continue\n",
    "                                # create input vector for network\n",
    "                                succ_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "\n",
    "                                # Call the SUCCESS network of the current agent and return succ_i\n",
    "                                if every[i] == BLUE_TEAM_ID:\n",
    "                                    succ_i = torch.Tensor([1.0])\n",
    "                                else:\n",
    "                                    succ_dist, _ = self.red.SUCC(succ_in)\n",
    "                                    succ_i = succ_dist.sample()\n",
    "            \n",
    "                                # Append the voting results to \"vote\"\n",
    "                                succ.append(succ_i)\n",
    "                                \n",
    "                                # If we are currently training on the SUCCESS network, save it to experience buffer\n",
    "                                if train_model == 'succ_red' and every[i] == RED_TEAM_ID:\n",
    "                                    obs_replay.append(succ_in)\n",
    "                                    actions_replay.append(succ_i)\n",
    "                                    log_probs.append(succ_dist)\n",
    "                                    stepid_replay.append(si)\n",
    "                            \n",
    "                        # Make the torch.Tensor succ vector\n",
    "                        succ = torch.Tensor(succ)\n",
    "                        \n",
    "                        # Update the \"succ\" vector to the environment\n",
    "                        self.env.update_succ(succ)\n",
    "                            \n",
    "#                         print(f'  Mission: {hist[0,self.env.si-1,15:18]} - {hist[0,self.env.si-1,18]} Fails {hist[0,self.env.si-1,19]} Round')\n",
    "                        \n",
    "                        hist, every, who, li, si, mi, nm, done = self.env.get_observation()\n",
    "                        \n",
    "                    # append the result of each episode into the epoch buffer\n",
    "                    obs_replay_epoch.append(obs_replay)\n",
    "                    actions_replay_epoch.append(actions_replay)\n",
    "                    log_probs_epoch.append(log_probs)\n",
    "                    stepid_replay_epoch.append(stepid_replay)\n",
    "                    winning_team.append(self.env.winning_team)\n",
    "                    last_step = si\n",
    "                \n",
    "                obs_all = []\n",
    "                actions_all = []\n",
    "                log_probs_all = []\n",
    "                stepid_all = []\n",
    "                discounted_rewards = []\n",
    "                for epi in range(self.train_episodes):\n",
    "                    if len(stepid_replay_epoch[epi]) > 0:\n",
    "                        obs_all += obs_replay_epoch[epi]\n",
    "                        actions_all += actions_replay_epoch[epi]\n",
    "                        log_probs_all += log_probs_epoch[epi]\n",
    "                        stepid_all += stepid_replay_epoch[epi]\n",
    "                        discounted_rewards += compute_discounted_rewards(stepid_replay_epoch[epi], last_step, winning_team[epi], train_model, self.gamma)\n",
    "                    #####\n",
    "                \n",
    "                \n",
    "                value_function = (lambda x : 0)\n",
    "                sb = preprocess(obs_all, actions_all, stepid_all, discounted_rewards, value_function)\n",
    "\n",
    "                model_lookup = {\"comm_red\": (red.COMM, red.COMM_opt),\n",
    "                    \"miss_red_2\" : (red.MISS, red.MISS_opt2),\n",
    "                    \"miss_red_3\" : (red.MISS, red.MISS_opt3),\n",
    "                    \"vote_red\" : (red.VOTE, red.VOTE_opt), \n",
    "                    \"succ_red\" : (red.SUCC, red.SUCC_opt),\n",
    "                    \"who_blue\" : (blue.WHO, blue.WHO_opt),\n",
    "                    \"comm_blue\" : (blue.COMM, blue.COMM_opt),\n",
    "                    \"miss_blue_2\" : (blue.MISS, blue.MISS_opt2),\n",
    "                    \"miss_blue_3\" : (blue.MISS, blue.MISS_opt3),\n",
    "                    \"vote_blue\" : (blue.VOTE, blue.VOTE_opt)}\n",
    "\n",
    "                acmodel, optimizer = model_lookup[train_model]\n",
    "\n",
    "                if train_model != \"who_blue\":\n",
    "                    update_parameters_ppo(optimizer, acmodel, sb, args, train_model)\n",
    "                else:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = 0\n",
    "                    for epi in range(self.train_episodes):\n",
    "                        loss += ((acmodel(obs_all[epi]) - every_replay[epi]) ** 2).sum()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # end of self.train_episodes episodes\n",
    "                # # obs_replay, actions_replay, stepid_replay, winning_team\n",
    "                # if len(stepid_replay) > 0:\n",
    "                #     discounted_rewards = compute_discounted_rewards(stepid_replay, winning_team, train_model, self.gamma)\n",
    "                #     advantage_gae = compute_advantage_gae([0.0] * len(stepid_replay), discounted_rewards, stepid_replay, self.gae_lambda, self.gamma, train_model)\n",
    "                # sb['obs', 'action', 'advantage_gae' or 'advantage', 'discounted_reward']\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title\n",
    "# def compute_advantage_gae(values, discounted_rewards, stepid_replay, gae_lambda, gamma, train_model):\n",
    "#     rewards = [x if abs(x) == 1 else 0 for x in discounted_rewards]\n",
    "#     # print(rewards)\n",
    "#     # print(discounted_rewards)\n",
    "\n",
    "#     reps = 0\n",
    "#     if stepid_replay[0] != 0:\n",
    "#         print(train_model)\n",
    "#         print(stepid_replay)\n",
    "#     while stepid_replay[reps] == 0:\n",
    "#         reps += 1\n",
    "\n",
    "#     rewards_single = torch.Tensor(rewards[::reps])\n",
    "#     values_single = torch.Tensor(values[::reps])\n",
    "\n",
    "#     end_of_epi = np.arange(len(rewards_single))[rewards_single != 0].astype(int)\n",
    "    \n",
    "#     # print(end_of_epi)\n",
    "#     # print(values_single)\n",
    "\n",
    "#     last_end = -1\n",
    "#     total_advs = []\n",
    "#     for end in end_of_epi:\n",
    "#         # calculate advantage_gae from index last_end+1 ~ end\\\n",
    "#         theta = rewards_single[last_end+1:end+1] - values_single[last_end+1:end+1]\n",
    "#         theta[:-1] += gamma * values_single[last_end+2:end+1]\n",
    "#         gl = gamma * gae_lambda\n",
    "#         advantages = torch.tensor([\n",
    "#             (theta[i:] * (gl ** torch.arange(end - last_end -i))).sum() for i in range(end - last_end)\n",
    "#         ])\n",
    "\n",
    "#         # print(f'last_end: {last_end} ~ end: {end} => {len(advantages)}')\n",
    "#         # print(advantages)\n",
    "#         total_advs.append(advantages)\n",
    "#         last_end = end\n",
    "#     ############################################################################\n",
    "    \n",
    "#     total_advs = torch.concat(total_advs)\n",
    "#     torch.repeat_interleave(total_advs, 2)\n",
    "\n",
    "#     # print(total_advs)\n",
    "#     # print(torch.repeat_interleave(total_advs, 2))\n",
    "#     # assert 0\n",
    "\n",
    "#     return total_advs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = getDefaultParams()\n",
    "def compute_discounted_rewards(stepid_replay, last_step, winning_team, train_model, gamma):\n",
    "\n",
    "    # last_step = 10\n",
    "    # stepid_replay = [0, 0, 1, 3, 5, 5, 6, 6, 6, 8]\n",
    "\n",
    "    # Calculate the discounted rewards\n",
    "    win_team_color = 'red' if winning_team == RED_TEAM_ID else 'blue'\n",
    "    R = 1 if win_team_color in train_model else -1\n",
    "    discounted_rewards = [R * (gamma ** (last_step - 1 - stepid_replay[-1]))]\n",
    "    curr_step = stepid_replay[-1]\n",
    "\n",
    "    for si in range(len(stepid_replay) - 2, -1, -1):\n",
    "        discounted_rewards += [discounted_rewards[-1] * (gamma ** (curr_step - stepid_replay[si]))]\n",
    "        curr_step = stepid_replay[si]\n",
    "\n",
    "    # stepN = len(stepid_replay)\n",
    "    # win_team_color = 'red' if winning_team[-1] == RED_TEAM_ID else 'blue'\n",
    "    # discounted_rewards = [1 if win_team_color in train_model else -1]\n",
    "\n",
    "    # for i in range(stepN - 2, -1, -1):\n",
    "    #     if stepid_replay[i] == stepid_replay[i+1]:\n",
    "    #         # the current step of the i-th index is the same as the step of the i+1-th index\n",
    "    #         discounted_rewards.append(discounted_rewards[-1])\n",
    "    #     elif stepid_replay[i] != 0 and stepid_replay[i+1] == 0:\n",
    "    #         # the current step of the i-th index is the last step of a new episode\n",
    "    #         win_team_color = 'red' if winning_team[i] == RED_TEAM_ID else 'blue'\n",
    "    #         discounted_rewards.append(1 if win_team_color in train_model else -1)\n",
    "    #     else:\n",
    "    #         # the current step of the i-th index is the step of the i+1-th index minus 1\n",
    "    #         discounted_rewards.append(discounted_rewards[-1] * gamma)\n",
    "\n",
    "    discounted_rewards.reverse()\n",
    "    return discounted_rewards\n",
    "\n",
    "\n",
    "def preprocess(obs_replay, actions_replay, stepid_replay, discounted_rewards, value_function):\n",
    "    # Example:\n",
    "    # preprocess(*full_replay_buffer, range(10), (lambda x: 0))\n",
    "\n",
    "    # print(len(obs_replay))\n",
    "    # print(len(stepid_replay))\n",
    "\n",
    "    assert len(obs_replay) == len(actions_replay)\n",
    "    assert len(obs_replay) == len(stepid_replay)\n",
    "    assert len(discounted_rewards) == len(obs_replay)\n",
    "    discounted_rewards = torch.Tensor(discounted_rewards)\n",
    "    # print(obs_replay)\n",
    "    obs_replay = torch.stack(obs_replay)\n",
    "    actions_replay = torch.stack(actions_replay)\n",
    "    # obs_replay = torch.Tensor(obs_replay)\n",
    "    sb = {\n",
    "        \"obs\" : obs_replay,\n",
    "        \"action\" : actions_replay,\n",
    "        \"advantage\" : discounted_rewards - value_function(obs_replay),\n",
    "        \"discounted_reward\" : discounted_rewards,\n",
    "    }\n",
    "    return sb\n",
    "\n",
    "\n",
    "\n",
    "def update_parameters_ppo(optimizer, acmodel, sb, args, train_model):\n",
    "    def _compute_policy_loss_ppo(logp, old_logp, entropy, advantages):\n",
    "        policy_loss, approx_kl = 0, 0\n",
    "\n",
    "        ### TODO: implement PPO policy loss computation (30 pts).  #######\n",
    "        logr = logp - old_logp\n",
    "        ratios = torch.exp(logr)\n",
    "        \n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1-args['clip_ratio'], 1+args['clip_ratio']) * advantages\n",
    "        \n",
    "        policy_loss = (-torch.min(surr1, surr2) -args['entropy_coef']*entropy).mean()\n",
    "        \n",
    "        # approx_kl = torch.sum(torch.exp(logp) * logr)\n",
    "        #approx_kl = torch.nn.functional.kl_div(logp, old_logp)\n",
    "\n",
    "        # approx_kl = ((logr.exp() - 1) - logr).sum()\n",
    "        approx_kl = ((logr ** 2) / 2).sum()\n",
    "        \n",
    "        ##################################################################\n",
    "        \n",
    "        return policy_loss, approx_kl\n",
    "    \n",
    "    def _compute_value_loss(values, returns):\n",
    "        ### TODO: implement PPO value loss computation (10 pts) ##########\n",
    "        value_loss = F.mse_loss(values.squeeze(-1), returns).mean() #(values - returns).pow(2).mean()\n",
    "        ##################################################################\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "    if 'miss' in train_model:\n",
    "        if \"2\" in train_model:\n",
    "            ((dist, _), _) = acmodel(sb['obs'])\n",
    "        else:\n",
    "            (_, (dist, _)) = acmodel(sb['obs'])\n",
    "    else:\n",
    "        dist, values = acmodel(sb['obs'])\n",
    "    \n",
    "    # print(dist)\n",
    "    \n",
    "    old_logp = dist.log_prob(sb['action']).detach()\n",
    "    logp = dist.log_prob(sb['action'])\n",
    "    dist_entropy = dist.entropy()\n",
    "    \n",
    "    advantage = sb['advantage_gae'] if args['use_gae'] else sb['advantage']\n",
    "    \n",
    "    policy_loss, _ = _compute_policy_loss_ppo(logp, old_logp, dist_entropy, advantage)\n",
    "\n",
    "    values = torch.zeros_like(sb['discounted_reward'])\n",
    "\n",
    "    value_loss = _compute_value_loss(values, sb['discounted_reward'])\n",
    "\n",
    "    for i in range(args['train_ac_iters']):\n",
    "        if 'miss' in train_model:\n",
    "            if \"2\" in train_model:\n",
    "                ((dists, _), _) = acmodel(sb['obs'])\n",
    "            else:\n",
    "                (_, (dists, _)) = acmodel(sb['obs'])\n",
    "        else:\n",
    "            dists, values = acmodel(sb['obs'])\n",
    "            \n",
    "        logp = dists.log_prob(sb['action'])\n",
    "        dist_entropy = dists.entropy()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_pi, approx_kl = _compute_policy_loss_ppo(logp, old_logp, dist_entropy, advantage)\n",
    "\n",
    "\n",
    "        values = torch.zeros_like(sb['discounted_reward'])\n",
    "        loss_v = _compute_value_loss(values, sb['discounted_reward'])\n",
    "\n",
    "        loss = loss_v + loss_pi\n",
    "        if approx_kl > 1.5 * args['target_kl']:\n",
    "            break\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    \n",
    "    update_policy_loss = policy_loss.item()\n",
    "    update_value_loss = value_loss.item()\n",
    "\n",
    "    logs = {\n",
    "        \"policy_loss\": update_policy_loss,\n",
    "        \"value_loss\": update_value_loss,\n",
    "    }\n",
    "\n",
    "    return logs\n",
    "\n",
    "# PROBLEMS TO FIX\n",
    "# How to deal with the optimizer for MISS? Do we need to train each model separetely?\n",
    "# Need supervised learning for WHO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9fee7afb7c49cc85303085d52dacd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abeac6148364546920e83417d10337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0aa2c6df56245cdb0a3eb082840d591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02afc58e7ba741929417a5680f91c0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74577a88a86948b7801d2c249173ecbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edce548dfbc4726a11a19ccf0b4f0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cbbe9409cb4d93abe71639e1e54867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468c80d4a30a493abb9af8837d0c6610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb4a7a0e6b64d7cad7a16f7672490b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29eb7dd1258d43f9aa184d751c60d2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e342cf656c44a03818653b048a6e1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainable_models=['comm_red', 'comm_blue', 'who_blue', 'miss_red_2', 'miss_red_3', 'miss_blue_2', 'miss_blue_3', 'vote_red', 'vote_blue', 'succ_red']\n",
    "\n",
    "set_random_seed(0)\n",
    "\n",
    "lr = 0.00025\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "blue = BlueAgent(lr=lr)\n",
    "red = RedAgent(lr=lr)\n",
    "\n",
    "my_env = AvalonEnv()\n",
    "engine = AvalonEngine(env=my_env, blue=blue, red=red, train_episodes=100, max_epoch=10,\n",
    "                      trainable_models=trainable_models, gamma=gamma, gae_lambda=gae_lambda)\n",
    "engine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDefaultParams():\n",
    "    # here goes the default parameters for the agent\n",
    "    config = dict(\n",
    "        # env=env, the agent does not need to ahve access to the env because there is an engine\n",
    "        learning_rate=0.00025,\n",
    "        gamma=0.99,\n",
    "        memory_size=200000,\n",
    "        initial_epsilon=1.0,\n",
    "        min_epsilon=0.1,\n",
    "        max_epsilon_decay_steps=150000,\n",
    "        warmup_steps=500,\n",
    "        target_update_freq=2000,\n",
    "        batch_size=32,\n",
    "        device=None,\n",
    "        disable_target_net=False,\n",
    "        enable_double_q=False,\n",
    "        use_gae=False,\n",
    "        train_ac_iters=5,\n",
    "        target_kl=0.01,\n",
    "        clip_ratio=0.2,\n",
    "        entropy_coef=0.01\n",
    "    )\n",
    "    return config\n",
    "\n",
    "        # def __init__(self,\n",
    "        #         score_threshold=0.93,\n",
    "        #         discount=0.995,\n",
    "        #         lr=1e-3,\n",
    "        #         max_grad_norm=0.5,\n",
    "        #         log_interval=10,\n",
    "        #         max_episodes=2000,\n",
    "        #         gae_lambda=0.95,\n",
    "        #         use_critic=False,\n",
    "        #         clip_ratio=0.2,\n",
    "        #         target_kl=0.01,\n",
    "        #         train_ac_iters=5,\n",
    "        #         use_discounted_reward=False,\n",
    "        #         entropy_coef=0.01,\n",
    "        #         use_gae=False):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
