{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import easyrl.models.diag_gaussian_policy as DiagGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stolen from easyrl Categorical policy policy with some modifications\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self,\n",
    "                 body_net,\n",
    "                 action_dim,\n",
    "                 in_features=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.body = body_net\n",
    "        if in_features is None:\n",
    "            for i in reversed(range(len(self.body.fcs))):\n",
    "                layer = self.body.fcs[i]\n",
    "                if hasattr(layer, 'out_features'):\n",
    "                    in_features = layer.out_features\n",
    "                    break\n",
    "\n",
    "        self.head = nn.Sequential(nn.Linear(in_features, action_dim), nn.Softmax())\n",
    "\n",
    "    def forward(self, x=None, body_x=None, **kwargs):\n",
    "        if x is None and body_x is None:\n",
    "            raise ValueError('One of [x, body_x] should be provided!')\n",
    "        if body_x is None:\n",
    "            body_x = self.body(x, **kwargs)\n",
    "        if isinstance(body_x, tuple):\n",
    "            pi = self.head(body_x[0])\n",
    "        else:\n",
    "            pi = self.head(body_x)\n",
    "        action_dist = Categorical(probs=pi)\n",
    "        return action_dist, body_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# set random seed\n",
    "seed = 0\n",
    "set_random_seed(seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, final_activation=None):\n",
    "        self.final_activation = final_activation\n",
    "        super().__init__()\n",
    "        #### A simple network that takes\n",
    "        #### as input the history, and outputs the \n",
    "        #### distribution parameters.\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU())\n",
    "        self.out_layer = nn.Sequential(nn.Linear(64, action_dim))\n",
    "\n",
    "    def forward(self, ob):\n",
    "        mid_logits = self.fcs(ob)\n",
    "        logits = self.out_layer(mid_logits)\n",
    "        if self.final_activation is not None:\n",
    "            logits = self.final_activation(logits)\n",
    "        return logits\n",
    "\n",
    "# class MishNet(NNetwork):\n",
    "#     def __init__(*args, mission_shapes, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.final_layers = [nn.Sequential(nn.Linear(64, s)) for s in mission_shapes]\n",
    "        \n",
    "#     def forward(self, ob):\n",
    "#         mid_logits = self.fcs(ob)\n",
    "#         logits = [fl(mid_logits) for fl in self.final_layers]\n",
    "#         if self.final_activation is not None:\n",
    "#             logits = [self.final_activation(logit) for logit in logits]\n",
    "#         return logits\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PLAYERS = 5\n",
    "RED_PLAYERS = 2\n",
    "BLUE_PLAYERS = 3\n",
    "HIST_SHAPE = 2 * 25 * (3 * NUM_PLAYERS + 5)\n",
    "SELF_SHAPE = 5\n",
    "COMM_SHAPE = 32  # Change freely\n",
    "WHO_SHAPE = NUM_PLAYERS\n",
    "VOTE_SHAPE = NUM_PLAYERS\n",
    "\n",
    "assert RED_PLAYERS + BLUE_PLAYERS == NUM_PLAYERS\n",
    "\n",
    "def get_who():\n",
    "    return NNetwork(SELF_SHAPE + HIST_SHAPE + COMM_SHAPE, WHO_SHAPE, nn.softmax)\n",
    "\n",
    "def get_comm():\n",
    "    return DiagGaussian(NNetwork(SELF_SHAPE + HIST_SHAPE + WHO_SHAPE, 64), COMM_SHAPE, 64)\n",
    "\n",
    "def get_miss(mission_shapes = (10,10)):\n",
    "    model = NNetwork(SELF_SHAPE + WHO_SHAPE + HIST_SHAPE, 64)\n",
    "    return [CategoricalPolicy(model, mish, 64) for mish in mission_shapes]\n",
    "\n",
    "def get_vote():\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + COMM_SHAPE + WHO_SHAPE, 128), 1, 128)\n",
    "\n",
    "def get_succ():\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + COMM_SHAPE + WHO_SHAPE, 128), 1, 128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def comm():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def who():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def miss():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def vote():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def succ():\n",
    "        pass\n",
    "\n",
    "class RedAgent(Agent):\n",
    "    \n",
    "    def __init__(every):\n",
    "        self.comm = get_comm()\n",
    "        self.who = (lambda *args : every)\n",
    "        mission_models = get_miss()\n",
    "        self.miss = (lambda *args : [f(x) for f, x in zip(mission_models, args)])\n",
    "        self.vote = get_vote()\n",
    "        self.succ = get_succ()\n",
    "\n",
    "class BlueAgent(Agent):\n",
    "    \n",
    "    def __init__():\n",
    "        self.comm = get_comm()\n",
    "        self.who = get_who()\n",
    "        mission_models = get_miss()\n",
    "        self.miss = (lambda *args : [f(x) for f, x in zip(mission_models, args)])\n",
    "        self.vote = get_vote()\n",
    "        self.succ = (lambda *args : torch.distributions.bernoulli.Bernoulli(1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AvalonEnv():\n",
    "    def __init__(self):\n",
    "        self.nm = [2, 3, 2, 3, 3]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.red_team_id = 1\n",
    "        self.blue_team_id = 0\n",
    "        self.mi = 0\n",
    "        self.ri = 0\n",
    "        self.si = 0\n",
    "        self.li = 0\n",
    "        self.hist = torch.zeros((2, 25, 20))\n",
    "        every = [0, 0, 0, 1, 1]\n",
    "        random.shuffle(every)\n",
    "        self.every = torch.Tensor(every.copy())        \n",
    "        self.who = torch.normal(0.5, 0.1, NUM_PLAYERS)\n",
    "        self.done = False\n",
    "        self.winning_team = None\n",
    "        \n",
    "        # the initial observation of \"hist\" is all zeros\n",
    "        # EXCEPT a one at the leader id (self.li) location of the zero-th step\n",
    "        self.hist[0, 0, self.li] = 1\n",
    "        self.hist[1, 0, :5] = 1\n",
    "        \n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self): \n",
    "        return self.hist, self.every, self.who, self.li, self.done\n",
    "    \n",
    "    def update_who(self, who_m):\n",
    "        self.who = who_m\n",
    "        \n",
    "    def update_miss(self, miss):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 5:10] = miss\n",
    "        self.hist[1, self.si, 5:10] = 1\n",
    "    \n",
    "    def update_vote(self, vote):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 10:15] = vote\n",
    "        self.hist[1, self.si, 10:15] = 1\n",
    "        \n",
    "        # check if there are more yeses than noes\n",
    "        if (vote == 1).sum() > 2:\n",
    "            # set relevance of only the no mission flag\n",
    "            self.hist[1, self.si, 15] = 1\n",
    "        else:\n",
    "            # set the no mission flag\n",
    "            self.hist[0, self.si, 15] = 1\n",
    "\n",
    "            # set the current round\n",
    "            self.hist[0, self.si, 18] = self.ri\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 19] = self.hist[0, self.si-1, 19] if self.si else 0\n",
    "            \n",
    "            # set relevance\n",
    "            self.hist[1, self.si, 15:] = 1\n",
    "            \n",
    "            if self.ri == 4:\n",
    "                # game is over, red team wins\n",
    "                self.done = True\n",
    "            else:\n",
    "                # update step id\n",
    "                self.si += 1\n",
    "                \n",
    "                # update round id\n",
    "                self.ri += 1\n",
    "                \n",
    "                # update leader\n",
    "                self.li = (self.li + 1) % 5\n",
    "                self.hist[0, self.si, self.li] = 1\n",
    "                self.hist[1, self.si, :5] = 1\n",
    "        \n",
    "    def update_succ(self, succ):\n",
    "        # set the current round\n",
    "        self.hist[0, self.si, 18] = self.ri\n",
    "        \n",
    "        # set relevance\n",
    "        self.hist[1, self.si, 16:] = 1\n",
    "        \n",
    "        if 0 in succ:\n",
    "            # set the mission failure flag\n",
    "            self.hist[0, self.si, 17] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 19] = self.hist[0, self.si-1, 19] + 1 if self.si else 1\n",
    "        else:\n",
    "            # set the mission success flag\n",
    "            self.hist[0, self.si, 16] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 19] = self.hist[0, self.si-1, 19] if self.si else 0\n",
    "        \n",
    "        # check if game is over\n",
    "        if self.hist[0, self.si, 18]  == 3:\n",
    "            # game is over, red team wins\n",
    "            self.winning_team = 1\n",
    "            self.done = True\n",
    "        elif self.mi == 2 + self.hist[0, self.si, 18]:\n",
    "            # game is over, blue team wins\n",
    "            self.winning_team = 0\n",
    "            self.done = True\n",
    "        else:            \n",
    "            # update mission id\n",
    "            self.mi += 1\n",
    "\n",
    "            # update round id\n",
    "            self.ri = 0\n",
    "\n",
    "            # update step id\n",
    "            self.si += 1\n",
    "                \n",
    "            # update leader\n",
    "            self.li = (self.li + 1) % 5\n",
    "            self.hist[0, self.si, self.li] = 1\n",
    "            self.hist[1, self.si, :5] = 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AvalonEngine:\n",
    "    env: AvalonEnv\n",
    "    train_episodes: int\n",
    "    max_epoch: int\n",
    "    agents: list\n",
    "    trainable_models: list\n",
    "    \n",
    "    def run():\n",
    "        # Useful for constructing the self vector (self_v)\n",
    "        self_m = torch.eye(5)\n",
    "        \n",
    "        for epoch in tqdm.tqdm(range(self.max_epoch)):\n",
    "            for train_model in self.trainable_models:\n",
    "                # trajectory buffer\n",
    "                obs_replay = []\n",
    "                actions_replay = []\n",
    "                \n",
    "                for episode in range(self.train_episodes):\n",
    "                    hist, every, who, li, done = env.reset()\n",
    "                    \n",
    "                    while not done:\n",
    "                        # Flow: communication -> predict who -> decide miss -> voting -> succ/fail -> next round\n",
    "                        #                                                             -> next round\n",
    "\n",
    "                        ''' Communication '''\n",
    "                        # Initialize communication matrix\n",
    "                        comm_m = []\n",
    "                        # Loop over every agent\n",
    "                        for i in range(5):\n",
    "                            # create input vector for network\n",
    "                            comm_in = tf.concat(0, [self_m[i], every if every[i] else who[i], hist])\n",
    "                            # Call the COMM network of the current agent and return the communication vector (comm_v)\n",
    "                            comm_v = agents[i].COMM(comm_in)\n",
    "                            # Append it to the communication matrix (comm)\n",
    "                            comm_m.append(comm_v)\n",
    "                            # If we are currently training on the COMM network, save it to experience buffer\n",
    "                            if (train_model == 'comm_red' and every[i] == env.red_team_id) or \\\n",
    "                                    (train_model == 'comm_blue' and every[i] == env.blue_team_id):\n",
    "                                obs_replay.append(comm_in)\n",
    "                                actions_replay.append(comm_v)\n",
    "                        # Make the torch.Tensor communication matrix\n",
    "                        comm_m = torch.Tensor(comm_m)\n",
    "\n",
    "                        ''' Predicting Who '''\n",
    "                        # Initialize who matrix (this matrix cannot be directly accessed by any agent)\n",
    "                        who_m = []\n",
    "                        # Loop over every agent\n",
    "                        for i in range(5):\n",
    "                            # continue if the agent is on the red team\n",
    "                            if every[i] == env.red_team_id:\n",
    "                                continue\n",
    "                            # create input vector for network\n",
    "                            who_in = tf.concat(0, [self_m[i], comm_m, hist])\n",
    "                            # Call the WHO network of the current agent (on the blue team)\n",
    "                            # and return the who vector (who)\n",
    "                            who_v = agents[i].WHO((who_in))\n",
    "                            # Append it to the who matrix (who_m)\n",
    "                            who_m.append(who_v)\n",
    "                            # If we are currently training on the WHO network, save it to experience buffer\n",
    "                            if train_model == 'who_blue':\n",
    "                                obs_replay.append(who_in)\n",
    "                                actions_replay.append(who_v)\n",
    "                        # update the who matrix to the environment\n",
    "                        env.update_who(who_m)\n",
    "\n",
    "                        ''' Deciding candidates to go on mission '''\n",
    "                        # create input vector for \"miss\" network\n",
    "                        miss_in = tf.concat(0, [self_m[li], every if every[li] else who_m[li], hist])\n",
    "                        # Only call the leader\n",
    "                        miss = agents[li].MISSION((miss_in))\n",
    "                        # If we are currently training on the MISS network, save it tp experience buffer\n",
    "                        if (train_model == 'miss_red' and every[i] == env.red_team_id) or \\\n",
    "                                (train_model == 'miss_blue' and every[i] == env.blue_team_id):\n",
    "                            obs_replay.append(miss_in)\n",
    "                            actions_replay.append(miss)\n",
    "                        # Update the \"miss\" vector to the environment\n",
    "                        env.update_miss(miss)\n",
    "                            \n",
    "                        ''' Voting YES/NO for the mission candidates '''\n",
    "                        # Initialize vote vector\n",
    "                        vote = []\n",
    "                        # Loop over every agent   \n",
    "                        for i in range(5):\n",
    "                            # create input vector for network\n",
    "                            vote_in = tf.concat(0, [self_m[i], every if every[i] else who_m[i], hist])\n",
    "                            # Call the VOTE network of the current agent and return vote_pi\n",
    "                            vote_pi = agents[i].VOTE((vote_in))\n",
    "                            # Append the voting results to \"vote\"\n",
    "                            vote.append(vote_pi)\n",
    "                            # If we are currently training on the VOTE network, save it to experience buffer\n",
    "                            if (train_model == 'vote_red' and every[i] == env.red_team_id) or \\\n",
    "                                    (train_model == 'vote_blue' and every[i] == env.blue_team_id):\n",
    "                                obs_replay.append(vote_in)\n",
    "                                actions_replay.append(vote_pi)\n",
    "                                \n",
    "                        # Make the torch.Tensor communication matrix\n",
    "                        vote = torch.Tensor(vote)\n",
    "                        # Update the \"vote\" vector to the environment\n",
    "                        env.update_vote(vote)\n",
    "                        \n",
    "                        ''' Success/Failure for the mission '''\n",
    "                        # check if there are more yeses than noes\n",
    "                        if (vote == 1).sum() > 2:\n",
    "                            # Initialize succ vector\n",
    "                            succ = []\n",
    "                            # Loop over every agent   \n",
    "                            for i in range(5):\n",
    "                                if not miss[i]:\n",
    "                                    continue                                \n",
    "                                # create input vector for network\n",
    "                                succ_in = tf.concat(0, [self_m[i], every if every[i] else who_m[i], hist])\n",
    "                                # Call the SUCCESS network of the current agent and return succ_i\n",
    "                                succ_i = agents[i].SUCCESS((succ_in))\n",
    "                                # Append the voting results to \"vote\"\n",
    "                                succ.append(succ_i)\n",
    "                                # If we are currently training on the SUCCESS network, save it to experience buffer\n",
    "                                if train_model == 'succ_red' and every[i] == env.red_team_id:\n",
    "                                    obs_replay.append(succ_in)\n",
    "                                    actions_replay.append(succ_i)\n",
    "                            # Update the \"succ\" vector to the environment\n",
    "                            env.update_succ(succ)\n",
    "                        \n",
    "                        hist, every, who, li, done = env.get_observation()\n",
    "                    \n",
    "                    # check who won\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_config():\n",
    "    env = AvalonEnv()\n",
    "    set_random_seed(5)\n",
    "    config = dict(\n",
    "        env=env,\n",
    "        train_episodes=10,\n",
    "        max_epoch=10,\n",
    "        trainable_models=['comm_red', 'comm_blue', 'who_blue', 'miss_red', 'miss_blue', 'vote_red', 'vote_blue', 'succ_red']\n",
    "    )\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = config['env']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.]],\n",
       " \n",
       "         [[1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.]]]),\n",
       " tensor([0., 0., 1., 0., 1.]),\n",
       " [tensor([0.8303, 0.1261, 0.9075, 0.8199, 0.9201]),\n",
       "  tensor([0.1166, 0.1644, 0.7379, 0.0333, 0.9942]),\n",
       "  tensor([0.6064, 0.5646, 0.0724, 0.6593, 0.7150]),\n",
       "  tensor([0.5793, 0.9809, 0.6502, 0.0566, 0.9201]),\n",
       "  tensor([0.6698, 0.2615, 0.0407, 0.7850, 0.9752])],\n",
       " 0,\n",
       " False)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "normal() received an invalid combination of arguments - got (float, float, int), but expected one of:\n * (Tensor mean, Tensor std, *, torch.Generator generator, Tensor out)\n * (Tensor mean, float std, *, torch.Generator generator, Tensor out)\n * (float mean, Tensor std, *, torch.Generator generator, Tensor out)\n * (float mean, float std, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [233]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mget_default_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# agents = [BlueAgent(), BlueAgent(), BlueAgent(), RedAgent(), RedAgent()]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m agents \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [230]\u001b[0m, in \u001b[0;36mget_default_config\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_default_config\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mAvalonEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     set_random_seed(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      4\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m      5\u001b[0m         env\u001b[38;5;241m=\u001b[39menv,\n\u001b[1;32m      6\u001b[0m         train_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      7\u001b[0m         max_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      8\u001b[0m         trainable_models\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomm_red\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomm_blue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwho_blue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmiss_red\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmiss_blue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvote_red\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvote_blue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msucc_red\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m     )\n",
      "Input \u001b[0;32mIn [228]\u001b[0m, in \u001b[0;36mAvalonEnv.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [228]\u001b[0m, in \u001b[0;36mAvalonEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(every)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevery \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(every\u001b[38;5;241m.\u001b[39mcopy())        \n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwho \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_PLAYERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwinning_team \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: normal() received an invalid combination of arguments - got (float, float, int), but expected one of:\n * (Tensor mean, Tensor std, *, torch.Generator generator, Tensor out)\n * (Tensor mean, float std, *, torch.Generator generator, Tensor out)\n * (float mean, Tensor std, *, torch.Generator generator, Tensor out)\n * (float mean, float std, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "config = get_default_config()  \n",
    "# agents = [BlueAgent(), BlueAgent(), BlueAgent(), RedAgent(), RedAgent()]\n",
    "agents = []\n",
    "engine = AvalonEngine(agents, **config)\n",
    "engine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
