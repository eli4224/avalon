{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import easyrl.models.diag_gaussian_policy as DiagGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stolen from easyrl Categorical policy policy with some modifications\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self,\n",
    "                 body_net,\n",
    "                 action_dim,\n",
    "                 in_features=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.body = body_net\n",
    "        if in_features is None:\n",
    "            for i in reversed(range(len(self.body.fcs))):\n",
    "                layer = self.body.fcs[i]\n",
    "                if hasattr(layer, 'out_features'):\n",
    "                    in_features = layer.out_features\n",
    "                    break\n",
    "\n",
    "        self.head = nn.Sequential(nn.Linear(in_features, action_dim), nn.Softmax())\n",
    "\n",
    "    def forward(self, x=None, body_x=None, **kwargs):\n",
    "        if x is None and body_x is None:\n",
    "            raise ValueError('One of [x, body_x] should be provided!')\n",
    "        if body_x is None:\n",
    "            body_x = self.body(x, **kwargs)\n",
    "        if isinstance(body_x, tuple):\n",
    "            pi = self.head(body_x[0])\n",
    "        else:\n",
    "            pi = self.head(body_x)\n",
    "        action_dist = Categorical(probs=pi)\n",
    "        return action_dist, body_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# set random seed\n",
    "seed = 0\n",
    "set_random_seed(seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, final_activation=None):\n",
    "        self.final_activation = final_activation\n",
    "        super().__init__()\n",
    "        #### A simple network that takes\n",
    "        #### as input the history, and outputs the \n",
    "        #### distribution parameters.\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU())\n",
    "        self.out_layer = nn.Sequential(nn.Linear(64, action_dim))\n",
    "\n",
    "    def forward(self, ob):\n",
    "        mid_logits = self.fcs(ob)\n",
    "        logits = self.out_layer(mid_logits)\n",
    "        if self.final_activation is not None:\n",
    "            logits = self.final_activation(logits)\n",
    "        return logits\n",
    "\n",
    "# class MishNet(NNetwork):\n",
    "#     def __init__(*args, mission_shapes, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.final_layers = [nn.Sequential(nn.Linear(64, s)) for s in mission_shapes]\n",
    "        \n",
    "#     def forward(self, ob):\n",
    "#         mid_logits = self.fcs(ob)\n",
    "#         logits = [fl(mid_logits) for fl in self.final_layers]\n",
    "#         if self.final_activation is not None:\n",
    "#             logits = [self.final_activation(logit) for logit in logits]\n",
    "#         return logits\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED_TEAM_ID = 1\n",
    "BLUE_TEAM_ID = 0\n",
    "NUM_PLAYERS = 5\n",
    "RED_PLAYERS = 2\n",
    "BLUE_PLAYERS = 3\n",
    "HIST_SHAPE = 2 * 25 * (3 * NUM_PLAYERS + 5)\n",
    "SELF_SHAPE = torch.tensor([5])\n",
    "COMM_SHAPE = 32  # Change freely\n",
    "WHO_SHAPE = torch.tensor([NUM_PLAYERS])\n",
    "VOTE_SHAPE =torch.tensor([NUM_PLAYERS])\n",
    "\n",
    "assert RED_PLAYERS + BLUE_PLAYERS == NUM_PLAYERS\n",
    "\n",
    "def get_who():\n",
    "    return NNetwork(SELF_SHAPE + HIST_SHAPE + COMM_SHAPE, WHO_SHAPE, nn.functional.softmax)\n",
    "\n",
    "def get_comm():\n",
    "    return DiagGaussian.DiagGaussianPolicy(NNetwork(SELF_SHAPE + HIST_SHAPE + WHO_SHAPE, torch.tensor([64])), COMM_SHAPE, in_features=torch.tensor([64]))\n",
    "\n",
    "def get_miss(mission_shapes = (10,10)):\n",
    "    model = NNetwork(SELF_SHAPE + WHO_SHAPE + HIST_SHAPE, 64)\n",
    "    return [CategoricalPolicy(model, mish, 64) for mish in mission_shapes]\n",
    "\n",
    "def get_vote():\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + COMM_SHAPE + WHO_SHAPE, 128), 1, 128)\n",
    "\n",
    "def get_succ():\n",
    "    return CategoricalPolicy(NNetwork(HIST_SHAPE + COMM_SHAPE + WHO_SHAPE, 128), 1, 128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def comm():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def who():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def miss():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def vote():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def succ():\n",
    "        pass\n",
    "\n",
    "class RedAgent():\n",
    "    \n",
    "    def __init__(self,every):\n",
    "        self.comm = get_comm()\n",
    "        self.who = (lambda *args : every)\n",
    "        mission_models = get_miss()\n",
    "        self.miss = (lambda *args : [f(x) for f, x in zip(mission_models, args)])\n",
    "        self.vote = get_vote()\n",
    "        self.succ = get_succ()\n",
    "\n",
    "class BlueAgent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.comm = get_comm()\n",
    "        self.who = get_who()\n",
    "        mission_models = get_miss()\n",
    "        self.miss = (lambda *args : [f(x) for f, x in zip(mission_models, args)])\n",
    "        self.vote = get_vote()\n",
    "        self.succ = (lambda *args : torch.distributions.bernoulli.Bernoulli(1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RedAgent at 0x7f99c840e490>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RedAgent(torch.eye(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AvalonEnv():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nm = [2, 3, 2, 3, 3]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.mi = 0\n",
    "        self.ri = 0\n",
    "        self.si = 0\n",
    "        self.li = 0\n",
    "        self.hist = torch.zeros((2, 25, 20))\n",
    "        every = [0, 0, 0, 1, 1]\n",
    "        random.shuffle(every)\n",
    "        self.every = torch.Tensor(every.copy())        \n",
    "        self.who = torch.normal(0.5, 0.1, (NUM_PLAYERS, NUM_PLAYERS))\n",
    "        self.done = False\n",
    "        self.winning_team = None\n",
    "        \n",
    "        # the initial observation of \"hist\" is all zeros\n",
    "        # EXCEPT a one at the leader id (self.li) location of the zero-th step\n",
    "        self.hist[0, 0, self.li] = 1\n",
    "        self.hist[1, 0, :5] = 1\n",
    "        \n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self):         \n",
    "        return self.hist, self.every, self.who, self.li, self.done\n",
    "    \n",
    "    def update_who(self, who_m):\n",
    "        self.who = who_m\n",
    "        \n",
    "    def update_miss(self, miss):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 5:10] = miss\n",
    "        self.hist[1, self.si, 5:10] = 1\n",
    "        return self.hist\n",
    "    \n",
    "    def update_vote(self, vote):\n",
    "        # save to self.hist\n",
    "        self.hist[0, self.si, 10:15] = vote\n",
    "        self.hist[1, self.si, 10:15] = 1\n",
    "        \n",
    "        # check if there are more yeses than noes\n",
    "        if (vote >= 0.5).sum() > 2:\n",
    "            # set relevance of only the no mission flag\n",
    "            self.hist[1, self.si, 15] = 1\n",
    "        else:\n",
    "            # set the no mission flag\n",
    "            self.hist[0, self.si, 15] = 1\n",
    "\n",
    "            # set the current round\n",
    "            self.hist[0, self.si, 19] = self.ri\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] if self.si else 0\n",
    "            \n",
    "            # set relevance\n",
    "            self.hist[1, self.si, 15:] = 1\n",
    "            \n",
    "            if self.ri == 4:\n",
    "                # game is over, red team wins\n",
    "                self.done = True\n",
    "            else:\n",
    "                # update step id\n",
    "                self.si += 1\n",
    "                \n",
    "                # update round id\n",
    "                self.ri += 1\n",
    "                \n",
    "                # update leader\n",
    "                self.li = (self.li + 1) % 5\n",
    "                self.hist[0, self.si, self.li] = 1\n",
    "                self.hist[1, self.si, :5] = 1\n",
    "        return self.hist\n",
    "        \n",
    "    def update_succ(self, succ):\n",
    "        # set the current round\n",
    "        self.hist[0, self.si, 19] = self.ri\n",
    "        \n",
    "        # set relevance\n",
    "        self.hist[1, self.si, 16:] = 1\n",
    "        \n",
    "        if (succ < 0.5).sum():\n",
    "            # set the mission failure flag\n",
    "            self.hist[0, self.si, 17] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] + 1 if self.si else 1\n",
    "        else:\n",
    "            # set the mission success flag\n",
    "            self.hist[0, self.si, 16] = 1\n",
    "            \n",
    "            # set the number of failures\n",
    "            self.hist[0, self.si, 18] = self.hist[0, self.si-1, 18] if self.si else 0\n",
    "        \n",
    "        # check if game is over\n",
    "        if self.hist[0, self.si, 18]  == 3:\n",
    "            # game is over, red team wins\n",
    "            self.winning_team = 1\n",
    "            self.done = True\n",
    "        elif self.mi == 2 + self.hist[0, self.si, 18]:\n",
    "            # game is over, blue team wins\n",
    "            self.winning_team = 0\n",
    "            self.done = True\n",
    "            \n",
    "        # update mission id\n",
    "        self.mi += 1\n",
    "\n",
    "        # update round id\n",
    "        self.ri = 0\n",
    "\n",
    "        # update step id\n",
    "        self.si += 1\n",
    "\n",
    "        # update leader\n",
    "        self.li = (self.li + 1) % 5\n",
    "        self.hist[0, self.si, self.li] = 1\n",
    "        self.hist[1, self.si, :5] = 1\n",
    "            \n",
    "        return self.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AvalonEngine:\n",
    "    env: AvalonEnv\n",
    "    train_episodes: int\n",
    "    max_epoch: int\n",
    "    agents: list\n",
    "    trainable_models: list\n",
    "    \n",
    "    def run(self):\n",
    "        # Useful for constructing the self vector (self_v)\n",
    "        self_m = torch.eye(5)\n",
    "        \n",
    "        for epoch in tqdm(range(self.max_epoch), desc='epoch'):\n",
    "            for train_model in tqdm(self.trainable_models, desc='train_model'):\n",
    "                # trajectory buffer\n",
    "                obs_replay = []\n",
    "                actions_replay = []\n",
    "                \n",
    "                for episode in range(self.train_episodes):\n",
    "                    \n",
    "                    hist, every, who, li, done = self.env.reset()\n",
    "                    \n",
    "                    while not done:\n",
    "                        # Flow: communication -> predict who -> decide miss -> voting -> succ/fail -> next round\n",
    "                        #                                                             -> next round\n",
    "\n",
    "#                         print(f'Mission {self.env.mi} Round {self.env.ri} Step {self.env.si}')\n",
    "                        \n",
    "                        ''' Communication '''\n",
    "                        # Initialize communication matrix\n",
    "                        comm_m = []\n",
    "                        # Loop over every agent\n",
    "                        for i in range(5):\n",
    "                            # create input vector for network                            \n",
    "                            comm_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "                            \n",
    "                            if agents == []:\n",
    "                                comm_v = torch.rand(COMM_SHAPE)\n",
    "                            else:\n",
    "                                # Call the COMM network of the current agent and return the communication vector (comm_v)\n",
    "                                comm_v = agents[i].COMM(comm_in)\n",
    "                            \n",
    "                            # Append it to the communication matrix (comm)\n",
    "                            comm_m.append(comm_v)\n",
    "                            # If we are currently training on the COMM network, save it to experience buffer\n",
    "                            if (train_model == 'comm_red' and every[i] == RED_TEAM_ID) or \\\n",
    "                                    (train_model == 'comm_blue' and every[i] == BLUE_TEAM_ID):\n",
    "                                obs_replay.append(comm_in)\n",
    "                                actions_replay.append(comm_v)                        \n",
    "                        # Make the torch.Tensor communication matrix\n",
    "                        comm_m = torch.cat(comm_m)\n",
    "                        \n",
    "#                         print(f'  All the players communicated')\n",
    "\n",
    "                        ''' Predicting Who '''\n",
    "                        # Loop over every agent\n",
    "                        for i in range(NUM_PLAYERS):\n",
    "                            # continue if the agent is on the red team\n",
    "                            if every[i] == RED_TEAM_ID:\n",
    "                                continue\n",
    "                            # create input vector for network\n",
    "                            who_in = torch.cat((self_m[i], comm_m, hist.flatten()))\n",
    "                            \n",
    "                            if agents == []:\n",
    "                                who_v = torch.rand(NUM_PLAYERS)\n",
    "                            else:\n",
    "                                # Call the WHO network of the current agent (on the blue team)\n",
    "                                # and return the who vector (who)\n",
    "                                who_v = agents[i].WHO((who_in))\n",
    "                            \n",
    "                            # Update \"who_v\" into \"who\"\n",
    "                            who[i] = who_v\n",
    "                            # If we are currently training on the WHO network, save it to experience buffer\n",
    "                            if train_model == 'who_blue':\n",
    "                                obs_replay.append(who_in)\n",
    "                                actions_replay.append(who_v)\n",
    "                        # update the who matrix to the environment\n",
    "                        self.env.update_who(who)\n",
    "                        \n",
    "#                         print(f'  All blue players predicted who')\n",
    "#                         for i in range(NUM_PLAYERS):\n",
    "#                             if every[i] == RED_TEAM_ID:\n",
    "#                                 continue\n",
    "#                             print(f'    {i} -> {who[i]}')\n",
    "\n",
    "                        ''' Deciding candidates to go on mission '''\n",
    "                        # create input vector for \"miss\" network\n",
    "                        miss_in = torch.cat((self_m[li], every if every[li] else who[li], hist.flatten()))\n",
    "                        \n",
    "                        if agents == []:\n",
    "                            miss_args = torch.argsort(torch.rand(NUM_PLAYERS))[:self.env.nm[self.env.mi]]\n",
    "                            miss = torch.zeros(NUM_PLAYERS)\n",
    "                            for i in miss_args:\n",
    "                                miss[i] = 1\n",
    "                        else:\n",
    "                            # Only call the leader\n",
    "                            miss = agents[li].MISSION((miss_in))\n",
    "                        \n",
    "                        # If we are currently training on the MISS network, save it tp experience buffer\n",
    "                        if (train_model == 'miss_red' and every[i] == RED_TEAM_ID) or \\\n",
    "                                (train_model == 'miss_blue' and every[i] == BLUE_TEAM_ID):\n",
    "                            obs_replay.append(miss_in)\n",
    "                            actions_replay.append(miss)\n",
    "                        # Update the \"miss\" vector to the environment\n",
    "                        hist = self.env.update_miss(miss)\n",
    "                        \n",
    "#                         print(f'  Leader {li} decides {miss} go on mission')\n",
    "                            \n",
    "                        ''' Voting YES/NO for the mission candidates '''\n",
    "                        # Initialize vote vector\n",
    "                        vote = []\n",
    "                        # Loop over every agent   \n",
    "                        for i in range(5):\n",
    "                            # create input vector for network\n",
    "                            vote_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "                            \n",
    "                            if agents == []:\n",
    "                                vote_pi = torch.rand(1)\n",
    "                            else:\n",
    "                                # Call the VOTE network of the current agent and return vote_pi\n",
    "                                vote_pi = agents[i].VOTE((vote_in))\n",
    "                            \n",
    "                            # Append the voting results to \"vote\"\n",
    "                            vote.append(vote_pi)\n",
    "                            # If we are currently training on the VOTE network, save it to experience buffer\n",
    "                            if (train_model == 'vote_red' and every[i] == RED_TEAM_ID) or \\\n",
    "                                    (train_model == 'vote_blue' and every[i] == BLUE_TEAM_ID):\n",
    "                                obs_replay.append(vote_in)\n",
    "                                actions_replay.append(vote_pi)\n",
    "                        # Make the torch.Tensor vote vector\n",
    "                        vote = torch.Tensor(vote)\n",
    "                        # Update the \"vote\" vector to the environment\n",
    "                        hist = self.env.update_vote(vote)\n",
    "                        \n",
    "#                         print(f'  Voting results {vote} -> {(vote >= 0.5).sum() > 2}')\n",
    "                        \n",
    "                        ''' Success/Failure for the mission '''\n",
    "                        # check if there are more yeses than noes\n",
    "                        if (vote >= 0.5).sum() > 2:\n",
    "                            # Initialize succ vector\n",
    "                            succ = []\n",
    "                            # Loop over every agent   \n",
    "                            for i in range(5):\n",
    "                                if not miss[i]:\n",
    "                                    continue                                \n",
    "                                # create input vector for network\n",
    "                                succ_in = torch.cat((self_m[i], every if every[i] else who[i], hist.flatten()))\n",
    "                                \n",
    "                                if agents == []:\n",
    "                                    succ_i = torch.rand(1)\n",
    "                                else:\n",
    "                                    # Call the SUCCESS network of the current agent and return succ_i\n",
    "                                    succ_i = agents[i].SUCCESS((succ_in))\n",
    "                                \n",
    "                                # Append the voting results to \"vote\"\n",
    "                                succ.append(succ_i)\n",
    "                                # If we are currently training on the SUCCESS network, save it to experience buffer\n",
    "                                if train_model == 'succ_red' and every[i] == RED_TEAM_ID:\n",
    "                                    obs_replay.append(succ_in)\n",
    "                                    actions_replay.append(succ_i)\n",
    "                            # Make the torch.Tensor succ vector\n",
    "                            succ = torch.Tensor(succ)\n",
    "                            \n",
    "#                             print(f'  Succ = {succ}')\n",
    "                            \n",
    "                            # Update the \"succ\" vector to the environment\n",
    "                            hist = self.env.update_succ(succ)\n",
    "                            \n",
    "#                         print(f'  Mission: {hist[0,self.env.si-1,15:18]} - {hist[0,self.env.si-1,18]} Fails {hist[0,self.env.si-1,19]} Round')\n",
    "                        \n",
    "                        hist, every, who, li, done = self.env.get_observation()\n",
    "                    \n",
    "                    # check who won\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bddc237584435e94adc9f4626cc92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4054c5629a8347e6b1bae8d3f8ce0d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad62e7b20eb94cccb51f5898f369633a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add35568d42240baa5877c2de15f4f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a9fcc6ceb2470da712052974538fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18599575b8e9445f96a293ab52cba340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409a0a7b4def41b19b69f939fb71df2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2254d42e4384a5898ef129e629bb1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ea51f5efa249279fac852195d3a110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21158e5f85e54638a9aab2da691ea286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84df04611fbe4f7195775ad87a7f2aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_model:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainable_models=['comm_red', 'comm_blue', 'who_blue', 'miss_red', 'miss_blue', 'vote_red', 'vote_blue', 'succ_red']\n",
    "\n",
    "set_random_seed(0)\n",
    "\n",
    "# agents = [BlueAgent(), BlueAgent(), BlueAgent(), RedAgent(), RedAgent()]\n",
    "agents = []\n",
    "\n",
    "my_env = AvalonEnv()\n",
    "engine = AvalonEngine(env=my_env, agents=agents, train_episodes=10, max_epoch=10,\n",
    "                      trainable_models=trainable_models)\n",
    "engine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_config():\n",
    "    # here goes the default parameters for the agent\n",
    "    config = dict(\n",
    "        # env=env, the agent does not need to ahve access to the env because there is an engine\n",
    "        learning_rate=0.00025,\n",
    "        gamma=0.99,\n",
    "        memory_size=200000,\n",
    "        initial_epsilon=1.0,\n",
    "        min_epsilon=0.1,\n",
    "        max_epsilon_decay_steps=150000,\n",
    "        warmup_steps=500,\n",
    "        target_update_freq=2000,\n",
    "        batch_size=32,\n",
    "        device=None,\n",
    "        disable_target_net=False,\n",
    "        enable_double_q=False\n",
    "    )\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
